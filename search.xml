<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Contrastive Learning Survey (1) â€” InstDisc</title>
    <url>/2022/02/24/Contrastive-Learning-Survey-1-%E2%80%94-InstDisc/</url>
    <content><![CDATA[<p>This article aims to introduce the pioneering work (InstDisc) that leads to contrastive learning.</p>
<p>Paper: Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In CVPR, 2018. Updated version accessed at: https://arxiv.org/abs/1805.01978v1.</p>
<a id="more"></a>
<h1 id="why-this-paper">Why this paper?</h1>
<p>Self-supervised models such as BERT/GPT have been used with great success in the NLP field. However, supervised learning still occupies most of the work in the CV field. Researchers hope to find a kind of pretext task in the image domain to complete self-supervised learning, just like MASK in the NLP domain. This paper creatively proposes a pretext task called <strong>instance discrimination</strong> which performs well and is still the most mainstream pretext task for contrastive learning so far.</p>
<p>Although techniques about the loss function, how negative samples are stored in this paper are obsolete, the pretext task and the framework in this paper has a milestone significance in the history of contrastive learning and has influenced most of the subsequent work.</p>
<p>Hence, I choose this paper as the first one in this series of the survey of contrastive learning.</p>
<h1 id="research-motivation">Research Motivation</h1>
<p><img src="figure1.png" width="50%" height="50%"></p>
<p>Paperâ€™s Figure 1 is very clear to show the motivation. Authors find that:</p>
<blockquote>
<p>For an image from class <em>leopard</em>, the classes that get highest responses from a trained neural net classifier are all visually correlated, e.g., <em>jaguar</em> and <em>cheetah</em>. It is not the semantic labeling, but the apparent similarity in the data themselves that brings some classes closer than others.</p>
</blockquote>
<p>Hence, they want to:</p>
<blockquote>
<p>Our unsupervised approach takes the class-wise supervision to the extreme and learns a feature representation that discriminates among individual instances.</p>
</blockquote>
<p>In other words, contrastive learning is also called representation learning which aims to find an image vector(like word vector in NLP) which represents image semantic in a high dimension by unsupervised learning. And then, you can use these vector in your custom downstream tasks with a small annotated dataset.</p>
<h1 id="model-overview">Model Overview</h1>
<p><img src="figure2.png"></p>
<p>Figure 2 shows a clear pipeline of the model inference. Now I will describe each part of them and try to make you understand easier.</p>
<h2 id="instance-discrimination">Instance Discrimination</h2>
<p>As figure 1 says, we want to get a vertor which similar images are close in a high dimension(128 in this paper) and not similar images are far way from each other without using annotated images. However, although InstDict model is called as a unsupervised model, they actually still uses label. The method they use is really tricky that they take every image as a class and the task is to distinguish every image instance. This task sounds useless, and yes they are. So this kind of task is named as pretext task, which the aim of them is just to let the model can be trained.</p>
<p><img src="InstDisc.png"></p>
<p>Above figure shows that a pipline of instance discrimination. An instance, in this example , is an image of a unqiue cat. it will be transferred by image augmentation into 2 images: cropped one and rotated one. These two represent the same cat and so the image vector of these 2 images should be similar in our model, and vertors of other images in red square should be quite different from these 2images. What you find? Yes, we get positive and negative sample by this kind of easy implementation. Usually, in contrastive learning, we call the â€œcropped catâ€ (one of the augmentation image) Query, and the combination of the other augmentation image and the rest images is called Keys. Hence, our aim is to find a key in keys which is most similar with the query.</p>
<p>To conclusion, instance discrimination is a tricky pretext task which make the images without labels can have similar label by using image augmentation.</p>
<p>Pros: easy to implementation, good explanation, proved to be useful</p>
<p>Cons: the negative samples may contain the positive sample(image another cat image is included but it will be treated as a negative sample in instance discrimination task)</p>
<h2 id="loss-function-nce-loss">Loss Function: NCE loss</h2>
<p><span class="math display">\[Loss = -\log\frac{\exp \left(\mathbf{v}_{i}^{T} \mathbf{v} / \tau\right)}{\sum_{j=1}^{n} \exp \left(\mathbf{v}_{j}^{T} \mathbf{v} / \tau\right)}\]</span></p>
<p><span class="math inline">\(v_i\)</span> is query, <span class="math inline">\(v\)</span> is positive key, <span class="math inline">\(v_j\)</span> are negative keys. The loss function is similar with Cross Entropy Softmax.</p>
<p>Actually, InfoNCE is totally the same as NCE loss, which infoNCE uses n (n &lt; j) samples instead of all negative samples to make calculation faster.</p>
<h2 id="memory-bank">Memory Bank</h2>
<p>Another important work in InstDisc is that it proposed a data structure called memory bank, which stored all imagesâ€™ 128d vectors, also the <span class="math inline">\(v_j\)</span> in the loss function.</p>
<p>Pipeline in Paper:</p>
<blockquote>
<p>Let V = {vj} be the memory bank and fi = fÎ¸(xi) be the feature of xi. During each learning itera- tion, the representation fi as well as the network parameters Î¸ are optimized via stochastic gradient descend. Then fi is updated to V at the corresponding instance entry fi â†’ vi. We initialize all the representations in the memory bank V as unit random vectors.</p>
</blockquote>
<p>A more clear flowchart:</p>
<p><img src="memory%20bank.png"></p>
<p>Pros: The number of negative samples in the previous comparative learning is not enough because it is strongly related to the batch size. Memory bank liberates this limation. In this paper, Each time nce loss is calculated, 4096 negative samples are randomly selected from the memory bank.</p>
<p>Cons: encoder network may update too quickly to make the distribution between image vectors has a large difference.</p>
<h2 id="evaluation">Evaluation</h2>
<p>Since contrastive learning is unsupervised learning and aims to learn good representation feature of images, we cannot directly assess the accuracy of the model.</p>
<p>InstDisc uses <strong>linear SVM</strong> and <strong>KNN</strong> to evaluate model performance. Simply, it is to use the encoder after model training as a feature extractor, and train a svm or knn on the features of the image after the encoder to evaluate model performance.</p>
<p>However, since this is a contrastive learning survey, I want to describe a more popular way in later works: <strong>Linear Classification Protocol</strong>. This method also freezes features from trained encoder, and train a supervised linear classifier(a fully-connected layer followed by softmax) and evualte different models between supervised and unspervised models. And ImageNet is always used as evaluation method.</p>
<p>InstDisc only has 54% accuracy on ImageNet by linear classification, which is much lower than supervised model. But it is a groundbeaking work and the following contrastive models have achieved the same performance with supervised model.</p>
<p>We will have a global description of model performance after we has discussed several classic contrastive learning models in following chapters.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We describe the pretext task: Instance Discrimination, model pipeline, a brief of NCE loss and Memory bank. Actually, this paper also proposed other technique skills, but these techniques have little inspiration for follow-up research.</p>
<p>Next blog we will continue discussing another kinds of contrastive learning framework which are different from InstDisc.</p>
]]></content>
      <categories>
        <category>Contrastive Learning</category>
      </categories>
      <tags>
        <tag>self-supervised learning</tag>
        <tag>computer vision</tag>
      </tags>
  </entry>
  <entry>
    <title>DataMining Notes</title>
    <url>/2019/11/29/Data%20Mining%20%E7%AC%94%E8%AE%B0%20/</url>
    <content><![CDATA[<h1 id="data-mining-ç¬”è®°">Data Mining ç¬”è®°</h1>
<hr>
<p>å…³äºæ•°æ®æŒ–æ˜çš„ä¸€äº›çŸ¥è¯†ï¼Œå†…å®¹åŒ…æ‹¬åˆ†ç±»ï¼Œèšç±»ï¼Œå¼‚å¸¸å€¼æ£€æµ‹ï¼Œå…³è”æ€§åˆ†æç­‰ç­‰ã€‚</p>
<p>å¤§é‡æˆªå›¾å’Œæ€è·¯æ¥è‡ª the University of Queensland çš„Dr Hongzhi Yinæ‰€æ•™è¯¾ç¨‹INFS7203 Data Miningã€‚</p>
<a id="more"></a>
<h2 id="i.-classification-åˆ†ç±»">I. Classification åˆ†ç±»</h2>
<h3 id="æ¨¡å‹è¯„ä¼°model-evluation">1.æ¨¡å‹è¯„ä¼°ï¼ˆmodel evluationï¼‰</h3>
<h4 id="æ··æ·†çŸ©é˜µconfusion-matrix">æ··æ·†çŸ©é˜µï¼ˆconfusion matrixï¼‰</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Prediction/actual</th>
<th>T</th>
<th style="text-align: left;">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">T</td>
<td>True positive(TP)</td>
<td style="text-align: left;">False positive(FP)</td>
</tr>
<tr class="even">
<td style="text-align: left;">F</td>
<td>False negative(FN)</td>
<td style="text-align: left;">True negative(TN)</td>
</tr>
</tbody>
</table>
<p><strong>è¯¾ä»¶ä¸­çš„pptï¼šçœŸå®å€¼æ˜¯è¡Œï¼Œé¢„æµ‹å€¼æ˜¯åˆ—ã€‚</strong></p>
<p>åœ¨æˆ‘ä¸Šç½‘æŸ¥èµ„æ–™æˆ–è€…å¾ˆå¤šæµè¡Œçš„æ··æ·†çŸ©é˜µçš„åŒ…é‡Œé¢éƒ½æ˜¯æˆ‘ä¸Šè¿°çš„çœŸå®å€¼æ˜¯åˆ—ï¼Œ é¢„æµ‹å€¼æ˜¯è¡Œã€‚å½“æ—¶å°±é»˜è®¤æ˜¯è¯¾ä»¶é‡Œçš„æƒ…å†µï¼Œæ‰€ä»¥åœ¨è®¡ç®—ä¸­ä¼šå‡ºç°é—®é¢˜ï¼Œå¹¶ä¸”ç»å¸¸æœ‰æƒ³è¦éªŒç®—çš„æƒ³æ³•ã€‚æ‰€ä»¥è¿™é‡Œç”¨å’Œè¯¾ä»¶ä¸­æƒ³åçš„æƒ…å†µï¼Œæé†’å¤§å®¶è¿™éƒ½æ˜¯å–å†³äºè‡ªå·±çš„ã€‚</p>
<h4 id="è¯„ä¼°æ–¹æ³•">è¯„ä¼°æ–¹æ³•</h4>
<ol type="1">
<li><p>$Accuracy =  $</p>
<p>å¾ˆç›´è§‚ï¼Œé¢„æµ‹æˆåŠŸçš„æ•°é‡é™¤ä»¥é¢„æµ‹çš„æ€»æ•°</p>
<p>ä½†æ˜¯æœ‰<del>ä¸€ä¸ª</del>å‡ ä¸ªä¸å¥½çš„åœ°æ–¹ã€‚æˆ‘ä»¬å¾ˆå¤šæ—¶å€™åªå…³æ³¨Tå’ŒFä¸­çš„ä¸€ä¸ªã€‚</p></li>
<li><p><span class="math inline">\(Precision = \frac {TP}{TP + FP}\)</span></p></li>
<li><p><span class="math inline">\(Recall = \frac{TP}{TP+FN}\)</span></p></li>
<li><p><span class="math inline">\(F1-value = 2\frac{Precision*Recall}{Rrecision+recall}\)</span></p></li>
</ol>
<p>æœ€çˆ±çš„ä¸¾ä¾‹ç¯èŠ‚ï¼š</p>
<p>å°æ˜æœ‰ä¸€å¤©ç»ˆäºè¿›å…¥äº†å¿ƒå¿ƒå¿µå¿µçš„ç›¸äº²è§’ï¼ˆæœ‰100ä¸ªå¥³ç”Ÿç­‰ç€ä»–ï¼‰ã€‚ç”±ç”·äººçš„ä¸‰å¤§é”™è§‰ï¼ˆåˆ†ç±»å™¨ï¼‰ä»–è§‰å¾—<del>è‡ªå·±ç‰æ ‘ä¸´é£ï¼Œé£è¶£å¹½é»˜ï¼Œæ‰åæ¨ªæº¢</del>, é¢„æµ‹ä¼šè¢«99ä¸ªå¥³ç”Ÿæœ‰å¥½æ„Ÿï¼Œ1ä¸€ä¸ªæ²¡æœ‰å¥½æ„Ÿï¼Œç„¶è€Œäº‹å®æ˜¯å¦‚æ­¤çš„æ®‹é…·ï¼Œç›¸äº²æ˜¯å¦‚æ­¤çš„æ¿€çƒˆï¼ŒçœŸå®æƒ…å†µ100ä¸ªå¥³ç”Ÿéƒ½æ²¡æœ‰ç›¸ä¸­å°æ˜ã€‚äºæ˜¯æˆ‘ä»¬äº§ç”Ÿäº†ä¸€ä¸ªæ··æ·†çŸ©é˜µï¼š</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Prediction/actual</th>
<th>T</th>
<th>F</th>
<th>Total prediction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">T</td>
<td>0</td>
<td>99</td>
<td>99</td>
</tr>
<tr class="even">
<td style="text-align: left;">F</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total actual</td>
<td>0</td>
<td>100</td>
<td></td>
</tr>
</tbody>
</table>
<p>æˆ‘ä»¬å¯ä»¥ç®—å‡º<span class="math inline">\(accuracy = \frac{1}{100} = 0.01\)</span>, æœç„¶â€œç”·äººçš„è‡ªæˆ‘æ„Ÿè§‰â€è¿™ä¸ªåˆ†ç±»å™¨éå¸¸çš„ç³Ÿç³•ã€‚</p>
<p>å°æ˜åœ¨è¿™æ¬¡æ— æƒ…çš„ç°å®æ‰“è„¸åï¼Œå¼ºè¿«ç€è¢«çˆ¸å¦ˆå†æ¬¡å¸¦åˆ°äº†ç›¸äº²è§’ã€‚æ ¹æ®ä¸Šæ¬¡çš„ç»éªŒï¼Œå‘å¾®çš„å°æ˜è¿™æ¬¡ä¹Ÿè§‰å¾—æ²¡æœ‰äººä¼šçœ‹ä¸Šä»–ï¼Œé¢„æµ‹æ²¡æœ‰å¥³ç”Ÿç›¸ä¸­è‡ªå·±ã€‚ç„¶è€Œè¿™æ¬¡äº‹å®ä¸Šæœ‰ä¸€ä¸ªå¥³ç”Ÿä¸çŸ¥é“æ˜¯ä¸æ˜¯åœ£æ¯çˆ†å‘ï¼Œè§‰å¾—å°æ˜å¤ªå‘å¾®äº†ï¼Œç»™äº†ä»–ä¸€æ¬¡æœºä¼šã€‚ äºæ˜¯æˆ‘ä»¬äº§ç”Ÿäº†ä¸€ä¸ªæ··æ·†çŸ©é˜µï¼š</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Prediction/actual</th>
<th>T</th>
<th>F</th>
<th>Total prediction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">T</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td style="text-align: left;">F</td>
<td>1</td>
<td>99</td>
<td>100</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total actual</td>
<td>1</td>
<td>99</td>
<td></td>
</tr>
</tbody>
</table>
<p>è¿™æ¬¡<span class="math inline">\(accuracy = \frac {99}{100} = 0.99\)</span>ã€‚ å“‡å–”ï¼Œå°æ˜é¡¿æ—¶å¥½ä½©æœè‡ªå·±ï¼Œå±…ç„¶æœ‰<span class="math inline">\(99%\)</span>çš„é¢„æµ‹æˆåŠŸç‡é¢„æµ‹æˆåŠŸè‡ªå·±ä¸ä¼šè¢«äººçœ‹ä¸Šã€‚ ç­‰ç­‰ï¼Œä¸ºä»€ä¹ˆ<strong>é¢„æµ‹é‡å¿ƒå˜æˆäº†è‡ªå·±ä¸æˆåŠŸçš„æ¦‚ç‡äº†</strong>ï¼Œä¸æ˜¯åº”è¯¥æ›´å¸Œæœ›è‡ªå·±è¢«ç›¸ä¸­çš„å˜›ï¼Œè®©ä¸æˆåŠŸçš„æ¦‚ç‡è¿™ä¹ˆé«˜å¹²å˜›ğŸ¤£ã€‚</p>
<p>3å¹´åï¼Œå°æ˜ç»ˆäºèµ°å‡ºäº†ä¸Šä¸€æ¬¡ç›¸äº²çš„é˜´å½±<del>ï¼ˆç­‰æ–°çš„å¥³ç”Ÿåˆ·æ–°å¤ªä¸å®¹æ˜“äº†ï¼‰</del>ï¼Œå†æ¬¡èµ°è¿›ç›¸äº²è§’ã€‚ç»è¿‡äº†äººç”Ÿçš„æ²‰æ·€ï¼Œå°æ˜æ˜ç™½ä»–ä¸éœ€è¦å…³æ³¨é‚£äº›çœ‹ä¸ä¸Šè‡ªå·±çš„å¥³äºº<del>ï¼ˆæ°”æ€¥è´¥åï¼‰</del>, è€Œè¦æŠŠé‡ç‚¹æ”¾åœ¨é‚£äº›çœ‹ä¸Šè‡ªå·±çš„å¥³ç”Ÿä»¬ã€‚è¿™æ¬¡å°æ˜é¢„æµ‹è‡ªå·±æœ‰20ä¸ªå¥³ç”Ÿå¯¹ä»–æœ‰å¥½æ„Ÿï¼Œ80ä¸ªæ²¡æœ‰ã€‚äº‹å®æƒ…å†µæ˜¯æœ‰10ä¸ªå¥³ç”Ÿçœ‹ä¸­å°æ˜ï¼Œ90ä¸ªæ²¡æœ‰ã€‚æ›´é‡è¦çš„æ˜¯æ’å¤§è¿äº†ï¼Œé‚£10ä¸ªå¥³ç”Ÿåˆšå¥½ä¹Ÿæ˜¯å°æ˜é¢„æµ‹åº”è¯¥å–œæ¬¢ä»–çš„ã€‚äºæ˜¯æˆ‘ä»¬åˆæœ‰äº†æ··æ·†çŸ©é˜µï¼š</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Prediction/actual</th>
<th>T</th>
<th>F</th>
<th>Total prediction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">T</td>
<td>10</td>
<td>10</td>
<td>20</td>
</tr>
<tr class="even">
<td style="text-align: left;">F</td>
<td>0</td>
<td>80</td>
<td>80</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total actual</td>
<td>10</td>
<td>9</td>
<td></td>
</tr>
</tbody>
</table>
<p>æ­¤æ—¶<span class="math inline">\(accuracy = \frac{10+80}{100} = 0.9\)</span>,å½“ç„¶è¿™ä¹Ÿå……å…¶é‡åªèƒ½è¯æ˜å°æ˜å¯¹è‡ªå·±æœ‰æ¯”è¾ƒæ¸…é†’çš„è®¤çŸ¥ï¼ˆ<del>ä¸‘</del>) ,å’Œä»–çš„ç›®æ ‡ï¼ˆè„±å•ï¼‰æ²¡ä»€ä¹ˆå…³ç³»çš„ã€‚ ä½†æ˜¯ä¸‰å¹´ä¸­å°æ˜å­¦ä¼šäº†ç¬¬äºŒç§åº¦é‡æ–¹æ³•<span class="math inline">\(Precesion = \frac{10}{10+10}=0.5\)</span>, è¿™è¯´æ˜å°æ˜ä»¥åé¢„æµ‹è°è°è°å–œæ¬¢è‡ªå·±ï¼Œéƒ½èƒ½æœ‰50%çš„æˆåŠŸç‡ï¼ï¼ˆå§æ§½ï¼Œä¸ºä»€ä¹ˆè¦é«˜å…´ï¼Œæœ¬æ¥ä¸å°±æ˜¯æˆåŠŸå’Œå¤±è´¥2ä¸ªé€‰é¡¹å—ï¼Ÿï¼Ÿï¼‰ã€‚</p>
<p><span class="math inline">\(Recall = \frac {10}{0+10} = 100\%\)</span>, è¿™è¯´æ˜å–œæ¬¢å°æ˜çš„å¦¹å­100%éƒ½è¢«å°æ˜é¢„æµ‹ä¸­äº†ï¼ˆå‘µï¼Œå¥³äººï¼‰ã€‚</p>
<p>ç°åœ¨çš„å°æ˜è¦æ˜¯å›åˆ°äº†ä¸‰å¹´å‰ï¼Œä»–é¢„æµ‹æœ‰99ä¸ªå¦¹å­å–œæ¬¢ä»–ï¼Œå°±ä¼šæœ‰45ä¸ªå¦¹å­çœŸçš„å–œæ¬¢ï¼ˆPrecision=0.5ï¼‰ï¼Œè€Œä¸”å°æ˜å¯¹å¥¹ä»¬ç´¢ç„¶æ— å‘³ï¼Œéƒ½æ²¡æœ‰å‡ºä¹ä»–æ„æ–™å¤–çš„å¦¹å­ï¼ˆrecall = 1ï¼‰ã€‚å°æ˜æƒ³ï¼šæˆ‘æœç„¶è¿˜æ˜¯é‚£ä¸ªæˆ‘ï¼Œç‰æ ‘ä¸´é£ï¼Œé£è¶£å¹½é»˜ï¼Œæ‰åæ¨ªæº¢ã€‚é†’é†’ç°åœ¨çš„ä½ å·²ç»ä¸ä¼šè¯´è¿™æ ·çš„è¯äº†ï¼Œå› ä¸ºä½ å·²ç»æ˜¯ä¸€ä¸ªæˆç†Ÿçš„åˆ†ç±»å™¨ï¼ˆClassification)äº†ã€‚</p>
<h4 id="æ€»ç»“">æ€»ç»“</h4>
<p>å¦‚æœå«Œä¸Šé¢çš„ä¾‹å­ç¯‡å¹…è¿‡å¤§ï¼Œç›´æ¥åˆ°PPTçš„week2-P43 èƒŒå…¬å¼ã€‚</p>
<p>åº”ä»˜å®Œäº†è€ƒè¯•åï¼Œå¦‚æœæƒ³åŠ æ·±ç†è§£ï¼Œä¸ºä»€ä¹ˆè¦æœ‰æ–°çš„åº¦é‡æ–¹å¼è€Œä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£çš„accuracyï¼Œå¯ä»¥googleæˆ–è€…å¤ä¹ å°æ˜çš„æ‚²æƒ¨ç»å†ã€‚</p>
<p>ä¸Šè¿°çš„é­é‡ä¹Ÿæ˜¯è¿™ä¸€ç« <strong>Classification åˆ†ç±»</strong>çš„ä¸€ä¸ªå…·ä½“æ¼”ç¤ºã€‚å°æ˜åŸºäºä»–çš„æ•°æ®ï¼ˆèº«é«˜ä½“é‡é¢œå€¼å…´è¶£èƒŒæ™¯ç­‰ï¼‰æœ‰äº†ä¸€ä¸ªå¯¹è‡ªå·±çš„è®¤çŸ¥ï¼ˆåˆ†ç±»å™¨ï¼‰ã€‚å½“å°æ˜ç»è¿‡äº†ä¸€æ¬¡ç›¸äº²è§’çš„çœŸå®é­é‡åï¼ˆæ··æ·†çŸ©é˜µè¯„ä¼°ï¼‰ï¼Œä»–éªŒè¯äº†è‡ªå·±çš„è®¤çŸ¥å¯¹ä¸å¯¹ï¼ˆè¿™ä¸ªåˆ†ç±»å™¨çš„å¥½åï¼‰ã€‚å¦‚æœä»–å¯¹è‡ªå·±çš„è®¤çŸ¥éå¸¸çš„é«˜ï¼Œä»–å°±çŸ¥é“äº†ä»–æ˜¯å“ªç§å¦¹å­çš„èœï¼Œä»–ä»¥åçœ‹åˆ°è¿™ç§ç±»å‹çš„å¦¹å­å°±çŸ¥é“ä»–ä»¬å¾ˆå¯èƒ½å‘ç”Ÿä»€ä¹ˆæ•…äº‹ã€‚</p>
<p>æ ç²¾æé—®ï¼šé‚£ä½ è¿™ä¸ª<strong>è®¤çŸ¥ï¼ˆåˆ†ç±»å™¨ï¼‰</strong>å“ªé‡Œæ¥çš„å•Šï¼Œä½ ä¸€å¼€å§‹çš„è®¤çŸ¥ï¼ˆç‰æ ‘ä¸´é£ï¼Œé£è¶£å¹½é»˜ï¼Œæ‰åæ¨ªæº¢ï¼‰ä¸æ˜¯é”™çš„å—ï¼Ÿè¿™ä¸‰å¹´å°æ˜æ˜¯æ€ä¹ˆæå‡è‡ªå·±çš„å•Šï¼ˆæ€ä¹ˆæ”¹è¿›åˆ†ç±»å™¨ï¼‰ï¼Ÿ</p>
<p>â€œ<font size="2">æ‰¿è®¤æˆ‘ä¼˜ç§€é‚£ä¹ˆéš¾å—ï¼Œä¸‹ä¸€èŠ‚å°±å‘Šè¯‰ä½ å•¦ï¼Œæ¯å¤©100ä¸ªä¿¯å§æ’‘100ä¸ªä»°å§èµ·åâ€¦â€¦.å˜ç§ƒ</font>â€</p>
<h3 id="åˆ†ç±»æ–¹æ³•">2. åˆ†ç±»æ–¹æ³•</h3>
<h4 id="rote-learner-æœ€åŸå§‹æœ€åœŸå‘³">1. Rote-learner (æœ€åŸå§‹æœ€åœŸå‘³)</h4>
<p>å½“ä½ è¦é¢„æµ‹çš„æ•°æ®å’Œè®­ç»ƒé›†é‡Œæœ‰çš„æ•°æ®ä¸€æ¨¡ä¸€æ ·çš„æ—¶å€™ï¼Œé¢„æµ‹ç»“æœå°±æ˜¯è®­ç»ƒé›†ä¸­æ•°æ®çš„æ ‡ç­¾ã€‚</p>
<h4 id="knnkè¿‘é‚»">2. KNNï¼ˆKè¿‘é‚»ï¼‰</h4>
<ol type="1">
<li><p>ä½ å…ˆé€‰ä¸€ä¸ªKï¼Œæ¯”å¦‚5</p></li>
<li>ä½ æ¯æ¬¡è¦é¢„æµ‹ä¸€ä¸ªæ•°æ®çš„æ—¶å€™ï¼Œå°±æ‰¾ç¦»è¿™ä¸ªæ•°æ®æœ€è¿‘çš„5ä¸ªè®­ç»ƒé›†é‡Œé¢çš„ç‚¹ã€‚</li>
<li><p>æŠ•ç¥¨è¿™5ä¸ªç‚¹çš„åˆ†ç±»ã€‚å“ªç§åˆ†ç±»å çš„å¤šä½ é¢„æµ‹çš„è¿™ä¸ªæ•°æ®å°±å±äºè¿™ç§åˆ†ç±»ã€‚</p></li>
</ol>
<p>æ€ä¹ˆå®šä¹‰è·ç¦»ï¼Ÿ æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆè¯¯å·®çš„å¹³æ–¹å’Œå¼€æ ¹å·ï¼‰æˆ–è€…å…¬å¼ <span class="math inline">\(\boldsymbol{d i s t}=\sqrt{\sum_{\boldsymbol{k}=1}^{\boldsymbol{n}}\left(\boldsymbol{p}_{\boldsymbol{k}}-\boldsymbol{q}_{\boldsymbol{k}}\right)^{2}}\)</span></p>
<p>p å’Œ q æ˜¯æ•°æ®ï¼Œä¸€èˆ¬æˆ‘ä»¬çš„æ•°æ®éƒ½æ˜¯å¤šç»´çš„ï¼Œä¾‹å¦‚æ•°æ®ï¼šåº„ç¬‘é½ï¼ˆç”·ï¼Œé«˜ï¼Œå¯Œï¼Œå¸…ï¼‰ï¼Œæ¯ä¸€ä¸ªç›¸åŒçš„åˆ—ç›¸å‡ç­‰åˆ°è¯¯å·®ï¼Œè¯¯å·®å¹³æ–¹å’Œï¼Œå¼€æ ¹å· â€”â€”&gt; è·ç¦»</p>
<p>KNNçš„ç¼ºç‚¹ï¼š</p>
<ol type="1">
<li><p>å¦‚ä½•å®šä¹‰è¿™ä¸ªkå€¼ï¼Œå¦‚æœkå¾ˆå°ï¼Œæˆ‘ä»¬çš„åˆ¤æ–­å®¹æ˜“è¢«å™ªå£°ï¼ˆnoiseï¼‰å½±å“ï¼Œå¦‚æœkå¤ªå¤§ï¼Œå¯èƒ½åŒ…å«æ›´å¤šå…¶ä»–çš„åˆ†ç±»</p></li>
<li><p>KNNéœ€è¦scaleï¼Œå› ä¸ºæ•°æ®çš„å¤§å°å€¼éå¸¸å½±å“knnçš„åˆ¤æ–­ã€‚</p>
<p>ä¸¾ä¾‹ç¯èŠ‚ï¼šæˆ‘ä»¬ç°åœ¨çš„æ•°æ®é›†æ‹¥æœ‰<strong>ï¼ˆå¹´é¾„ï¼Œæ”¶å…¥ï¼‰</strong> 2ä¸ªå±æ€§ï¼Œå’Œä¸€ä¸ªæ ‡ç­¾<strong>ï¼ˆèŒä¸šï¼‰</strong></p>
<p>ç°åœ¨æˆ‘ä»¬æœ‰2ä¸ªæ ·æœ¬ï¼Œä¸€ä½ç¨‹åºå‘˜ï¼ˆ25ï¼Œ10000ï¼‰å’Œä¸€ä½åŒ»ç”Ÿï¼ˆ50ï¼Œ15000ï¼‰ï¼Œç°åœ¨æˆ‘ä»¬è¦é¢„æµ‹ä¸€ä½30å²æ‹¥æœ‰14000æœˆè–ªçš„äººçš„èŒä¸šã€‚</p>
<p>å–k=1ï¼Œæ ¹æ®å…¬å¼<span class="math inline">\(d(x,ç¨‹åºå‘˜)=\sqrt{(25-30)^2+(10000-14000)^2} = 4000\)</span>,<span class="math inline">\(d(x,åŒ»ç”Ÿ)=\sqrt{(50-30)^2+(15000-14000)^2} = 1000\)</span>, ç”±äºè·ç¦»åŒ»ç”Ÿæ›´è¿‘ï¼Œæˆ‘ä»¬åˆ¤æ–­è¿™äººæ˜¯ä¸€ä½åŒ»ç”Ÿã€‚</p>
<p>ç„¶è€Œäº‹å®ä¸Šæˆ‘ä»¬è§‰å¾—ä»–åº”è¯¥æ˜¯ä¸€ä¸ªç¨‹åºå‘˜ï¼Œå› ä¸ºæ›´åŠ çš„å¹´è½»ã€‚ç„¶è€Œ<strong>å¹´é¾„</strong>åœ¨è¿™ä¸ªè·ç¦»çš„å…¬å¼é‡Œè´¡çŒ®çš„éå¸¸å°ï¼Œå› ä¸ºå¹´é¾„å’Œæ”¶å…¥å®ƒä»¬çš„<strong>çº¬åº¦ï¼ˆå®šä¹‰åŸŸï¼‰</strong>ä¸ä¸€æ ·ï¼Œæ‰€ä»¥åœ¨KNNä¸­ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦é¢„å…ˆè¿›è¡Œscaleæ“ä½œï¼Œä¿è¯æ•°æ®éƒ½åœ¨åŒä¸€ä¸ªåº¦é‡æ ‡å‡†ä¸­ã€‚</p>
<p>ç»è¿‡scaleï¼ˆæ ¹æ®æ¯”ä¾‹å‹ç¼©è¿›0åˆ°1ä¹‹é—´ï¼‰ï¼Œæˆ‘ä»¬ç°åœ¨çš„æ•°æ®æ˜¯ï¼šç¨‹åºå‘˜ï¼ˆ0.5ï¼Œ0.66ï¼‰ï¼ŒåŒ»ç”Ÿï¼ˆ1ï¼Œ1ï¼‰ï¼Œ</p>
<p>xï¼ˆ0.66.0.93ï¼‰ã€‚ï¼ˆè¿™ä¸ªscaleæ–¹æ³•æ˜¯æˆ‘çXXæƒ³å½“ç„¶çš„ï¼‰</p>
<p><span class="math inline">\(d(x,ç¨‹åºå‘˜)=\sqrt{(0.5-0.66)^2+(0.66-0.93)^2} = 0.31\)</span>,</p>
<p><span class="math inline">\(d(x,åŒ»ç”Ÿ)=\sqrt{(1-0.66)^2+(1-0.93)^2} = 0.35\)</span>,</p>
<p>ç°åœ¨xå°±è¢«é¢„æµ‹ä¸ºç¨‹åºå‘˜äº†ã€‚</p></li>
<li><p>KNNä¸éœ€è¦é¢„å…ˆè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼ˆlazyï¼‰ï¼Œæ¯æ¬¡é¢„æµ‹ä¸€ä¸ªæ•°æ®ï¼Œéƒ½è¦é‡æ–°è®¡ç®—ä¸€è¾¹æ‰€æœ‰æ•°æ®ç‚¹çš„è·ç¦»å»æ‰¾æœ€è¿‘çš„ï¼Œæ—¶é—´å¤æ‚åº¦å¾ˆå¤§ã€‚</p></li>
</ol>
<h4 id="naÃ¯ve-bayesæœ´ç´ è´å¶æ–¯">3. NaÃ¯ve Bayesï¼ˆæœ´ç´ è´å¶æ–¯)</h4>
<p>ç›®æ ‡ï¼šæˆ‘ä»¬æœ‰ä¸€ç»„æ•°æ®Aï¼Œå¸Œæœ›ç”¨è¿™ä¸€ç»„æ•°æ®Aåˆ¤æ–­å®ƒå±äºå“ªä¸ªç±»</p>
<p>æ ¸å¿ƒæ€è·¯ï¼šè®¡ç®—åœ¨ç»™å®šè¿™ç»„æ•°æ®çš„æƒ…å†µä¸‹å®ƒå±äºå„ä¸ªç±»çš„æ¦‚ç‡ï¼Œæ¦‚ç‡æœ€é«˜çš„å°±æŠŠå®ƒåˆ¤å®šä¸ºè¿™ä¸ªç±»ã€‚</p>
<p>â€‹ å±äºç±»Cçš„æ¦‚ç‡å°±ç­‰äºï¼š<span class="math inline">\(P(C|A)\)</span></p>
<p>æˆ‘ä»¬çŸ¥é“ <span class="math inline">\(P(C|A)=\frac{P(A|C)P(C)}{P(A)}\)</span></p>
<p>æˆ‘ä»¬ç°åœ¨æœ‰ä»€ä¹ˆï¼š<span class="math inline">\(P(A_i|C)\)</span>:å¯¹äºç±»Cï¼Œæ¯ä¸€ç§å±æ€§çš„æ¦‚ç‡ã€‚P(C):è¿™ä¸ªç±»åœ¨æ‰€æœ‰ç±»é‡Œé¢çš„æ¦‚ç‡ï¼ŒP(A): æ‰€æœ‰çš„<span class="math inline">\(P(C_i|A)\)</span>éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæˆ‘ä»¬åªæ¯”è¾ƒå¤§å°çš„ï¼Œå¯ä»¥å¿½ç•¥æ‰ï¼Œæ¯”å¦‚è¦æ¯”<span class="math inline">\(\frac{5}{10000}\)</span>å’Œ<span class="math inline">\(\frac{5}{10000}\)</span>çš„å¤§å°ä¸éœ€è¦å…³å¿ƒ<span class="math inline">\(10000\)</span>.</p>
<p>ç¼ºç‚¹ï¼šä¸ºä»€ä¹ˆæŠŠè¿™ä¸ªåˆ†ç±»æ–¹æ³•çš„ç¼ºç‚¹æå‰å°±æ˜¯æœ´ç´ è´å¶æ–¯çš„æœ€å¤§bugï¼Œæ²¡æœ‰è¿™ä¸ªç¼ºç‚¹è¿›è¡Œä¸ä¸‹å»è®¡ç®—ã€‚æˆ‘ä»¬æ€ä¹ˆèƒ½ä»<span class="math inline">\(P(A_i|C)\)</span>æ¥å¾—å‡º<span class="math inline">\(P(A|C)\)</span>å‘¢ï¼Ÿ æˆ‘ä»¬çŸ¥é“äº’ç›¸ç‹¬ç«‹çš„äº‹ä»¶çš„è”åˆæ¦‚ç‡å°±ç­‰äºå„è‡ªçš„æ¦‚ç‡çš„ä¹˜ç§¯ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ´ç´ è´å¶æ–¯å°±å‡è®¾æ‰€æœ‰å±æ€§æ˜¯ç‹¬ç«‹çš„ï¼Ÿï¼Ÿï¼Ÿ</p>
<p>æœ‰äº†è¿™ä¸ªå‡è®¾åï¼Œæˆ‘ä»¬çš„ç›®æ ‡å°±å˜æˆäº†æ±‚å‡º</p>
<p><span class="math display">\[\Pi P\left(A_{i} | C_{j}\right) \times P\left(C_{j}\right)\]</span>çš„æœ€å¤§å€¼ã€‚</p>
<p>è¯¾ä»¶ï¼Œtutorialï¼Œè°·æ­Œé‡Œçš„ä¾‹å­éƒ½éå¸¸çš„è¯¦ç»†ï¼Œæˆ‘å°±ä¸è´´äº†ï¼Œç†è§£è¿‡ç¨‹å°±å¥½ã€‚</p>
<p>å…¶ä»–ç¼ºç‚¹ï¼šåœ¨å¯¹è¿ç»­æ€§å˜é‡æ“ä½œçš„æ—¶å€™è¦æå‰æŠŠå®ƒä»¬ç¦»æ•£åŒ–ï¼Œä¸ç„¶é’ˆå¯¹ä¸€ä¸ªæ•°æ®çš„æ¦‚ç‡ä¼šéå¸¸çš„å°‘ï¼Œæ•ˆæœä¹Ÿä¸å¥½ã€‚</p>
<p>æ”¹è¿›ï¼š</p>
<p>å½“æˆ‘ä»¬åš<span class="math inline">\(\Pi P\left(A_{i} | C_{j}\right) \times P\left(C_{j}\right)\)</span>è¿™ä¸€æ­¥çš„æ—¶å€™ï¼Œç»å¸¸æ˜¯å¾ˆå¤šå¾ˆå°å¾ˆå°çš„æ¦‚ç‡ï¼ˆ0.0å‡ ï¼‰ç›¸ç§°ï¼Œå€¼ä¼šè¶Šæ¥è¶Šå°ï¼Œä»è€Œå¯èƒ½å¯¼è‡´å¤±å»ç²¾ç¡®åº¦ï¼ˆå› ä¸ºè®¡ç®—æœºç®—ä¸äº†å°æ•°ä½å¾ˆé•¿çš„å€¼ï¼Œä¼šå››å°„äº”å…¥ï¼‰ã€‚</p>
<p>è¿™æ—¶å€™æˆ‘ä»¬å°±ç»™å®ƒä»¬å–ä¸ª<span class="math inline">\(log\)</span>å˜æˆ<span class="math inline">\(\sum \log P\left(A_{i} | C_{j}\right)+\log P\left(C_{j}\right)\)</span>.</p>
<p><span class="math inline">\(\Pi P\left(A_{i} | C_{j}\right) \times P\left(C_{j}\right)\)</span>å¦‚æœè¿™ä¸ªé¡¹ç›®é‡Œæœ‰ä¸€é¡¹çš„æ¦‚ç‡æ˜¯0ï¼Œé‚£å°±å…¨ä¸º0äº†ï¼Œæ˜¯ä¸å¯¹çš„ã€‚æ‰€ä»¥æˆ‘ä»¬ä¼šåŠ ä¸€ä¸ªç‰¹åˆ«ç‰¹åˆ«å°çš„å€¼ï¼Œè®©æ¦‚ç‡é¿å…æˆä¸º0ã€‚</p>
<p>ä¾‹å¦‚æœ‰ä¸€ä¸ªå¯¹äºæ”¶å…¥çš„åˆ†ç±»ï¼Œæ ‡ç­¾ä¸ºâ€œä½â€ï¼Œâ€œä¸­ç­‰â€ï¼Œâ€œé«˜â€ã€‚ ä½†æ˜¯æˆ‘ä»¬æ•°æ®é›†é‡Œæ²¡æœ‰æ”¶å…¥ä¸ºâ€œä½â€<span class="math inline">\(\Rightarrow P(C=â€œä½â€)=0\)</span>.</p>
<p>æ€»å…±æœ‰1000ä¸ªæ•°æ®ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠ<span class="math inline">\(P(C=â€œä½â€) = 0+1 / 1003\)</span>, 1003æ˜¯å› ä¸ºåœ¨â€œä¸­ç­‰â€å’Œâ€œé«˜â€é‡Œä¹Ÿè¦åŠ 1ã€‚</p>
<h4 id="å†³ç­–æ ‘">4.å†³ç­–æ ‘</h4>
<h5 id="æ¦‚å¿µ">æ¦‚å¿µ</h5>
<p>ä¸‹å›¾æ˜¯ä¸€ä¸ªå†³ç­–æ ‘çš„ä¾‹å­</p>
<p><img src="../images/DataMining/image-20191106221733796.png"></p>
<p>å†³ç­–æ ‘æ˜“äºç†è§£ï¼Œå°±æ˜¯æ ¹æ®ä½ çš„è¾“å…¥å€¼ä¸€å±‚ä¸€å±‚çš„ç­›é€‰å‡ºæœ€åçš„Classã€‚</p>
<p>æ ç²¾æé—®ï¼šé‚£ä½ æ˜¯æ€ä¹ˆé€‰è¿™ä¸ª<strong>å±æ€§</strong>ï¼ˆå³å›¾é»„è‰²éƒ¨åˆ†ï¼‰ä½œä¸ºåˆ†å‰²çš„å‘€ï¼Ÿé‚£<strong>åˆ†å‰²çš„æ¡ä»¶</strong>åˆæ˜¯æ€ä¹ˆç¡®å®šçš„å‘¢ï¼Ÿå¯¹äºè¿ç»­å‹çš„å˜é‡ï¼ˆIncomeï¼‰ï¼Œä½ æ˜¯æ€ä¹ˆæ‰¾åˆ°é€‰å–80Kä½œä¸ºä¸´ç•Œå€¼çš„å‘¢ï¼Ÿåˆæ˜¯ä»€ä¹ˆæ—¶å€™ç»“æŸå†³ç­–æ ‘çš„å‘¢ï¼Ÿï¼ˆä¸€å®šè¦å…¨éƒ¨çš„attributeå…¨éƒ¨åˆ†å‰²è¿‡åå—ï¼Ÿ</p>
<ol type="1">
<li><p>Nominal Attributes:</p>
<p>å¯ä»¥æœ‰å¤šç»´çš„åˆ†å‰²å’ŒäºŒå…ƒçš„åˆ†å‰²</p>
<p><img src="/images/DataMing/image-20191106223823835.png"></p></li>
<li><p>Ordinal Attributes:</p>
<p>å¤šå…ƒçš„å½¢å¼ç›´æ¥æŠŠæ‰€æœ‰æƒ…å†µåˆ†å¼€ï¼Œè€ŒäºŒå…ƒçš„å½¢å¼å¿…é¡»è¦æ³¨æ„å±æ€§é‡Œå­˜åœ¨çš„é¡ºåºã€‚</p>
<p><img src="/images/image-20191106223928930.png"></p></li>
<li><p>Continuous Attributes</p></li>
</ol>
<p><img src="/images/image-20191106224342530.png"></p>
<p>æ€ä¹ˆæ ·çš„åˆ†å‰²æ‰ç®—å¥½å‘¢ï¼Ÿ å¦‚æœæˆ‘ä»¬æ€»å…±æœ‰20æ¡è®°å½•ï¼Œç”¨ä¸€ä¸ªåˆ†å‰²æ¡ä»¶åï¼Œå„å¾—åˆ°10æ¡å±æ€§ã€‚è¿™åæ¡å±æ€§çš„ç±»éƒ½åˆšå¥½å±äºå®ƒä»¬åŸæ¥çš„ç±»ï¼Œå³é¢„æµ‹100%ï¼Œçº¯å‡€åº¦purityä¹Ÿæ˜¯100ã€‚æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæµ‹é‡çš„å°ºåº¦å«åšimpurityï¼ˆä¸çº¯åº¦ï¼‰</p>
<p>ç”±æ­¤æˆ‘ä»¬æœ‰äº†3ç§æ–¹æ³•æ¥è®¡ç®—impurity</p>
<h5 id="gini-index">1. Gini Index</h5>
<p>é€‰å–ä¸€ä¸ªå±æ€§Pï¼Œè®¡ç®—è¿™ä¸ªå±æ€§çš„GINIå€¼ GINI(P)</p>
<p>å°†Påˆ†å‰²æˆå¤šä¸ªå­èŠ‚ç‚¹åï¼Œè®¡ç®—æ¯ä¸ªçš„GINIå€¼ï¼Œåœ¨å„è‡ªä¹˜ä»¥å®ƒä»¬çš„æƒé‡ï¼ˆweights)å¾—åˆ°GINI(M)</p>
<p>è®¡ç®— Gain = GINI(P) -GINI(M)</p>
<p>æœ€é«˜çš„Gainå°±æ˜¯æœ€å¥½çš„åˆ†å‰²</p>
<p>å…·ä½“ä¾‹å­ï¼š</p>
<ol type="1">
<li>å¯¹äºäºŒå…ƒåˆ†ç±»é—®é¢˜çš„è®¡ç®—</li>
</ol>
<p><img src="/images/image-20191106230226860.png"></p>
<ol start="2" type="1">
<li>å¯¹äºå¤šå…ƒå±æ€§çš„è®¡ç®—</li>
</ol>
<p><img src="/images/image-20191106230416524.png"></p>
<ol start="3" type="1">
<li>å¯¹äºè¿ç»­å‹å±æ€§çš„è®¡ç®—</li>
</ol>
<p><img src="/images/image-20191106230444301.png"></p>
<ol type="1">
<li>å°†æ‰€æœ‰çš„valueè¿›è¡Œæ’åºï¼ˆå›¾ä¸­çš„sorted valuesï¼‰</li>
<li>å°†sorted valuesä¸¤ä¸¤ä¹‹é—´è®¡ç®—å¹³å‡å€¼ä½œä¸ºsplit positionï¼ˆå³åˆ†å‰²çš„æ¡ä»¶ï¼‰</li>
<li>å°†é—®é¢˜åŒ–ä¸ºäºŒå…ƒåˆ†ç±»é—®é¢˜è¿›è¡ŒGINI INDEXè®¡ç®—</li>
<li>æ‰¾åˆ°æœ€å°çš„GINIçš„å°±æ˜¯æœ€å°çš„impurity</li>
</ol>
<h5 id="entropy">2. Entropy</h5>
<p><span class="math inline">\(\text { Entropy }(t)=-\sum p(j | t) \log p(j | t)\)</span></p>
<p><span class="math inline">\(\sum p(j | t)\)</span> is the relative frequency of class j at node t.</p>
<p><span class="math inline">\(G A I N_{\text {rut }}=\text { Entropy }(p)-\left(\sum_{i=1}^{k} \frac{n_{i}}{n} \text { Entropy }(i)\right)\)</span></p>
<p>å¯ä»¥çœ‹æˆå’ŒGINI INDEXä¸€æ ·ï¼Œåªæ˜¯è®¡ç®—çš„å…¬å¼å˜äº†ã€‚</p>
<p>ç„¶è€Œä»¥ä¸Š2ç§æ–¹æ³•éƒ½æœ‰ä¸€ä¸ªé—®é¢˜ï¼šNode impurity measures tend to prefer splits that result in large number of partitions, each being small but pure</p>
<p>è®¡ç®—çº¯å‡€åº¦çš„æ–¹æ³•å€¾å‘äºæœ‰å¤§é‡åˆ†å‰²çš„å±æ€§ï¼Œä¾‹å¦‚å¦‚æœå¯¹IDè¿›è¡Œè®¡ç®—ï¼Œå®ƒçš„çº¯å‡€åº¦å°±æ˜¯1å› ä¸ºæ²¡æœ‰ä¸åŒçš„ã€‚</p>
<p><img src="/images/image-20191106232014439.png"></p>
<p>ä½†æ˜¯è¿™å¹¶æ²¡æœ‰å®é™…æ„ä¹‰ï¼Œå¦‚æœæˆ‘ä»¬å°†customer IDåˆ†å‰²çš„è¯ã€‚å› æ­¤æˆ‘ä»¬æœ‰ä¸€ä¸ªæ–°çš„åº¦é‡æ ‡å‡†ï¼š</p>
<ol type="1">
<li>è®¡ç®—<span class="math inline">\(\text { SplitINFO }=-\sum_{i=1}^{k} \frac{n_{i}}{n} \log \frac{n_{i}}{n}\)</span> å¯ä»¥å½“ä½œ<span class="math inline">\(\frac{n_{i}}{n}\)</span>çš„entropy</li>
<li>è®¡ç®—<span class="math inline">\(\operatorname{Gain} R A T I O_{\text {stat }}=\frac{G A I N_{\text {sut}}}{\text {SplitINFO}}\)</span> , é€‰GainRATIOæœ€é«˜çš„</li>
</ol>
<h5 id="error">3. Error</h5>
<p><span class="math inline">\(\text {Error }(t)=1-\max P(i | t)\)</span></p>
<h5 id="ä»€ä¹ˆæ—¶å€™ç»ˆæ­¢åˆ†å‰²">ä»€ä¹ˆæ—¶å€™ç»ˆæ­¢åˆ†å‰²ï¼Ÿ</h5>
<ol type="1">
<li>å½“è¿™ä¸ªèŠ‚ç‚¹åªå‰©ä¸‹äº†ä¸€ç§class</li>
<li>å½“è¿™ä¸ªèŠ‚ç‚¹é‡Œçš„tupleéƒ½æ˜¯ä¸€æ ·çš„ï¼ˆæ— æ³•åˆ†å‰²äº†ï¼‰</li>
<li>early terminationï¼ˆæå‰ç»“æŸé¿å…è¿‡åº¦æ‹Ÿåˆoverfittingï¼‰</li>
</ol>
<h5 id="overfittingçš„åå¤„">Overfittingçš„åå¤„</h5>
<ol type="1">
<li>å¤ªå¤šçš„åˆ†æ”¯å¯¼è‡´noiseå¯èƒ½ä¹Ÿæˆä¸ºäº†åˆ†å‰²çš„æ¡ä»¶å¯¼è‡´é”™è¯¯ã€‚</li>
<li>å¯¹äºæœªçŸ¥çš„æ ·æœ¬ï¼Œå‡†ç¡®æ€§å¾ˆå·®ï¼šå› ä¸ºå®ƒå¯¹äºå·²çŸ¥æ•°æ®é›†çš„æ‹Ÿåˆæ•ˆæœå¤ªé«˜äº†ã€‚</li>
</ol>
<p>é¿å…overfittingçš„æ–¹æ³•ï¼š</p>
<p>early termination</p>
<ol type="1">
<li>ç»“æŸå½“å‰©ä½™çš„æ•°æ®ä¸æ»¡è¶³ç”¨æˆ·è‡ªå·±å®šäºçš„é˜ˆå€¼æ—¶</li>
<li>ç»“æŸå½“åœ¨æ‰©å±•èŠ‚ç‚¹åå´ä¸æå‡impurity</li>
</ol>
<h5 id="ä¼˜ç¼ºç‚¹">ä¼˜ç¼ºç‚¹</h5>
<p>ä¼˜ç‚¹ï¼š</p>
<ol type="1">
<li>æ˜“äºç†è§£ï¼Œæ¨¡å‹çš„å¯è§£é‡Šæ€§</li>
<li>æ˜“äºæ„å»º</li>
<li>å¾—ç›Šäºå·²ç»è®­ç»ƒå‡ºäº†æ¨¡å‹ï¼Œåšåˆ†ç±»ååˆ†è¿…é€Ÿ</li>
<li>å¯¹noiseæœ‰é²æ£’æ€§ï¼ˆæŠ—å™ªå£°ï¼Œå‰ææ˜¯overfittingè¦åšçš„å¥½ï¼‰</li>
<li>å®¹æ˜“å¤„ç†å†—ä½™æˆ–æ— å…³çš„å±æ€§</li>
</ol>
<p>ç¼ºç‚¹ï¼š</p>
<ol type="1">
<li>å†³ç­–æ ‘è¦å æ®çš„spaceè¦æ±‚éå¸¸å¤§ï¼ˆä½¿ç”¨è´ªå©ªç®—æ³•ä¸€èˆ¬ä¸èƒ½å¤Ÿæ‰¾åˆ°æœ€ä¼˜æ ‘ï¼‰</li>
<li>æ²¡æœ‰è€ƒè™‘å±æ€§ä¹‹é—´å¯èƒ½å­˜åœ¨å½±å“</li>
<li>æ¯æ¬¡åªå•ç‹¬ä½¿ç”¨ä¸€ä¸ªattributeåšåˆ†å‰²</li>
</ol>
<h5 id="ä¾‹é¢˜å¦‚ä½•ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªæ•°æ®åº“">ä¾‹é¢˜ï¼šå¦‚ä½•ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªæ•°æ®åº“</h5>
<p><img src="/images/image-20191107094244109.png"></p>
<ol type="1">
<li>è®¡ç®—æ¯ä¸ªattributeså„ç§åˆ†å‰²å¯èƒ½æ€§çš„GINIï¼Œæ‰¾åˆ°æœ€ä½çš„GINIå€¼ï¼ˆå°±æ˜¯æœ€é«˜çš„GAINï¼‰ï¼ˆæ‰¾å‡ºåˆ†å‰²çš„æ¡ä»¶ï¼‰</li>
<li>æ¯”è¾ƒæ¯ä¸ªattributesæœ€ä½çš„GINIå€¼ï¼Œæ‰¾åˆ°æœ€ä½çš„GINIå€¼ï¼Œå³æ˜¯ç¬¬ä¸€æ­¥çš„åˆ†å‰²å±æ€§</li>
<li>æŸ¥çœ‹ç¬¬ä¸€æ­¥åˆ†å‰²åçš„è¡¨æ ¼ï¼Œé‡å¤1ï¼Œ2ï¼Œç›´åˆ°å‰é¢æåˆ°çš„å†³ç­–æ ‘ç»ˆæ­¢æ¡ä»¶</li>
</ol>
<h2 id="ii.similarityç›¸ä¼¼åº¦">II.Similarityï¼ˆç›¸ä¼¼åº¦ï¼‰</h2>
<p>ç›¸ä¼¼åº¦è¯¥æ€ä¹ˆåº¦é‡å‘¢ï¼Ÿ</p>
<p>å¯¹äºnominalçš„ï¼Œéå¸¸ç®€å•ï¼Œé0å³1</p>
<p>å¯¹äºæœ‰é¡ºåºçš„ordinalï¼Œ<span class="math inline">\(Dissimilarity = ï½œx-yï½œ/(n-1)\)</span> (values mapped to 0 to n-1, n is the number of values).</p>
<p>ä¾‹å¦‚å¯¹äºæ”¶å…¥(low, medium, high)ï¼Œ å¯ä»¥mapå˜æˆ(0,1,2), <span class="math inline">\(dissimilarity(low, high) = |0-2|/2 = 1\)</span></p>
<p>å¯¹äºè¿ç»­å‹çš„éšæœºå˜é‡ï¼Œæœ‰3ç§measureçš„æ–¹æ³•ï¼Œ<strong>å½“å«æœ‰å¤šä¸ªå˜é‡æ—¶ï¼Œå¿…é¡»å…ˆåšnormalizationï¼ˆscaleï¼‰</strong></p>
<h3 id="æ›¼å“ˆé¡¿è·ç¦»aka-city-block-taxicab-l_1norm-manhattan-distance">1. æ›¼å“ˆé¡¿è·ç¦»ï¼ˆaka &quot;City blockâ€, taxicab, <span class="math inline">\(L_1\)</span>norm , Manhattan distance)</h3>
<p>$ d(x,y) = _{k=1}^{n}| x_k-y_k|$</p>
<h3 id="æ¬§å‡ é‡Œå¾—è·ç¦»euclidean-distanceaka-l_2-norm">2. æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆEuclidean distanceï¼Œaka <span class="math inline">\(L_2\)</span> normï¼‰</h3>
<p>$ d(x,y) = $</p>
<h3 id="supermumaka-l_maxnorm-l_inftynorm">3. Supermum(aka <span class="math inline">\(L_{max}\)</span>norm, <span class="math inline">\(L_{\infty}\)</span>norm)</h3>
<p><span class="math inline">\(d(x,y) = max(|x_k -y_k|)\)</span></p>
<p>ä¾‹å­ï¼š</p>
<p><img src="/images/image-20191107101011006.png"></p>
<h3 id="äºŒå…ƒå±æ€§çš„ç›¸ä¼¼æ€§">äºŒå…ƒå±æ€§çš„ç›¸ä¼¼æ€§</h3>
<p><img src="/images/image-20191107102524476.png"></p>
<p>ä¾‹å­ï¼š</p>
<p><img src="/images/image-20191107102555880.png"></p>
<p>ä¸ºä»€ä¹ˆè¦ç”¨Jaccardï¼Œæ˜¯å› ä¸ºå¾ˆå¤šæ—¶å€™æˆ‘ä»¬<strong>åªå…³æ³¨éƒ½æ˜¯1çš„æƒ…å†µ</strong>ã€‚</p>
<p>ä¾‹å¦‚åœ¨ä¸€ä¸ªè¶…å¸‚çš„ç”¨æˆ·å’Œç‰©å“è¡¨æ ¼ä¸­ï¼Œ1è¡¨ç¤ºè¯¥ç”¨æˆ·ä¹°äº†è¿™ç±»ç‰©å“ï¼Œ0è¡¨ç¤ºæ²¡ä¹°ï¼š</p>
<p><img src="/images/image-20191107102734670.png"></p>
<p>å¾ˆå®¹æ˜“æƒ³åˆ°ï¼Œ0å°†ä¼šæœ‰éå¸¸å¤šï¼Œ1å¾ˆå°‘ï¼Œæ‰€ä»¥ä½¿ç”¨SMCä¼šç»™æˆ‘ä»¬é”™è¯¯çš„ä¿¡æ¯ã€‚</p>
<h3 id="cosine-similarity">Cosine Similarity</h3>
<p>å°†æ¯ä¸€æ¡tupleå˜æˆä¸€ä¸ªå‘é‡vectorï¼Œè®¡ç®—2ä¸ªå‘é‡é—´çš„å¤¹è§’</p>
<p><span class="math inline">\(\cos \left(p_{\nu}, p_{2}\right)=\left(p_{t} \bullet p_{2}\right) /\left\|p_{t}\right\|\left\|p_{2}\right\|\)</span></p>
<p><img src="/images/image-20191107103215716.png"></p>
<p>åº”ç”¨ï¼š</p>
<p><img src="/images/image-20191107103330975.png"></p>
<p>ä¾‹å¦‚æˆ‘ä»¬æƒ³æŸ¥çœ‹2ä¸ªæ–‡æ¡£æ˜¯å¦ç›¸ä¼¼ã€‚æˆ‘ä»¬æ‰¾å‡ºå‡ ä¸ªå…³é”®å•è¯åœ¨è¿™å‡ ä¸ªæ–‡æ¡£ä¸­å‡ºç°çš„é¢‘æ•°ã€‚åšæˆä¸Šå›¾çš„è¡¨æ ¼ã€‚</p>
<p>æˆ‘ä»¬è®¡ç®—æ¯ä¸ªdocumentçš„cosine similarityå°±èƒ½è®¡ç®—å‡º2ä¸ªæ–‡æ¡£é—´çš„ç›¸ä¼¼åº¦ã€‚</p>
<h2 id="iii.-clustering-èšç±»">III. Clustering èšç±»</h2>
<hr>
<h3 id="æ¦‚å¿µ-1">1.æ¦‚å¿µ</h3>
<p>èšç±»å’Œåˆ†ç±»çš„åŒºåˆ«å°±æ˜¯æ²¡æœ‰æ ‡ç­¾äº†ã€‚</p>
<p>ä¸¾ä¾‹ç¯èŠ‚ï¼š</p>
<p>æœ‰å¾ˆå¤šç§èšç±»çš„å½¢å¼ï¼š</p>
<ol type="1">
<li><p>Partitional Clustering(åˆ†åŒºèšç±»)ï¼šç±»å’Œç±»ä¹‹é—´æ˜¯æ²¡æœ‰äº¤é›†çš„ã€‚ä¾‹å¦‚å¸¸è§çš„k-meansã€‚</p>
<p><img src="/images/image-20191016195923119.png"></p></li>
<li><p>Hierarchical Clustering(åˆ†å±‚èšç±»)ï¼šä¸€ä¸ªç±»æ˜¯å¯ä»¥åŒ…å«å…¶ä»–ç±»çš„ã€‚ä¾‹å¦‚æˆ‘å±äºæ•°æ®ç§‘å­¦ä¸“ä¸šï¼Œæ•°æ®ç§‘å­¦ä¸“ä¸šå±äºå·¥ç¨‹å­¦é™¢ï¼Œå·¥ç¨‹å­¦é™¢å±äºå¤§å­¦ã€‚<del>æˆ‘å±äºå¸…å“¥ï¼Œå¸…å“¥å±äºç”·çš„</del></p>
<p><img src="/images/image-20191016195859318.png"></p></li>
<li><p>Soft Clustering(è½¯èšç±»?)ï¼šä¸€ä¸ªæ•°æ®(object)å¯ä»¥å±äºå¤šä¸ªç±»ï¼Œå’Œç¬¬äºŒç§èšç±»æ–¹å¼çš„åŒºåˆ«åœ¨äºï¼Œç±»ä¹‹é—´æœ‰äº¤é›†ä½†æ˜¯ä¹Ÿæœ‰ä¸å±äºå¯¹æ–¹çš„éƒ¨åˆ†ã€‚ <span class="math inline">\(A \cap B \neq \emptyset\)</span> and $A B A Â or Â B $.</p>
<p><img src="/images/image-20191016195732039.png"></p></li>
</ol>
<h3 id="k-means">2. K-means</h3>
<p>æ ¸å¿ƒæ€æƒ³ï¼š</p>
<p>æ¯ä¸ªç±»éƒ½æœ‰ä¸€ä¸ªç±»ä¸­å¿ƒ(centroid)ï¼Œæ•°æ®é›†é‡Œé¢ç‚¹åˆ°å„ä¸ªç±»ä¸­å¿ƒçš„è·ç¦»æœ€çŸ­çš„é‚£ä¸ªç±»å°±æ˜¯è¿™ä¸ªç‚¹çš„ç±»ã€‚</p>
<p>æ ç²¾æé—®ï¼šå’‹æ‰¾è¿™ä¸ªç±»ä¸­å¿ƒå‘€ï¼Œæ—¢ç„¶èšç±»æœ¬èº«å°±æ˜¯ä¸çŸ¥é“æœ‰ä»€ä¹ˆç±»çš„æƒ…å†µä¸‹æ‰ä½¿ç”¨çš„æ–¹æ³•ï¼Œä½ è¿˜èƒ½æ•´å‡ºæ¥ä¸€ä¸ªç±»çš„ä¸­å¿ƒç‚¹ï¼Ÿ</p>
<p>å…·ä½“æ­¥éª¤ï¼š</p>
<ol type="1">
<li>å…ˆè¦ç¡®å®šä¸€ä¸ªKï¼ŒKä»£è¡¨äº†æˆ‘ä»¬æƒ³è¦æœ‰å¤šå°‘ç±»ï¼ŒK=5é‚£æœ€åæˆ‘ä»¬å°±èƒ½åˆ†å‡ºäº”ç±»æ¥ã€‚</li>
<li>éšæœºé€‰æ‹©Kä¸ªåˆå§‹ç‚¹ï¼ŒæŠŠå®ƒä»¬å½“ä½œå°±æ˜¯5ä¸ªç±»çš„ç±»ä¸­å¿ƒã€‚</li>
<li>è®¡ç®—æ¯ä¸ªç‚¹åˆ°å„ä¸ªç±»ä¸­å¿ƒçš„è·ç¦»ï¼ŒæŠŠç¦»å®ƒä»¬æœ€è¿‘çš„ç‚¹åŠ å…¥å®ƒä»¬ï¼Œè¿™æ ·æˆ‘ä»¬å°±æœ‰äº†æœ‰5ä¸ªå……æ»¡äº†æ•°æ®é›†çš„ç±»ã€‚</li>
<li>è®¡ç®—ç°åœ¨ç±»çš„ä¸­å¿ƒç‚¹ï¼ˆå¹³å‡å€¼ï¼‰</li>
<li>å›åˆ°ç¬¬3æ­¥ã€‚</li>
<li>ç›´åˆ°ç‚¹ä»¬éƒ½ä¸åœ¨ç±»ä¹‹é—´ç§»åŠ¨ï¼Œä¸­å¿ƒç‚¹ä¸å˜ï¼Œæˆ–è€…å¾ˆå°‘è¿›è¡Œç§»åŠ¨äº†å°±åœæ­¢è¿­ä»£ï¼Œç»“æŸã€‚</li>
</ol>
<p>è¯„ä¼°ï¼š</p>
<p>åƒåˆ†ç±»ä¸€æ ·ï¼Œèšç±»ä¹Ÿå¯ä»¥ç”¨æ–¹æ³•è¯„ä¼°ï¼ˆä¸æ˜¯å¾ˆå‡†ï¼Œå› ä¸ºå°±æ ¹æœ¬æ²¡äººçŸ¥é“æ­£ç¡®ç­”æ¡ˆï¼‰ã€‚</p>
<p>æˆ‘ä»¬æƒ³è±¡ä¸­ä¸€ä¸ªç±»åº”è¯¥æ˜¯å®ƒä»¬è¶Šåƒï¼Œè¿™ä¸ªç±»åˆ«è¶Šå‡†ç¡®ã€‚å› ä¸ºk-meansæ˜¯ç”¨è·ç¦»æ¥å®šä¹‰è¿™ä¸ªâ€œåƒâ€çš„ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨è·ç¦»æ¥è¯„ä¼°ã€‚</p>
<p>æˆ‘ä»¬ç§°è¿™ä¸ªæŒ‡æ ‡ä¸ºSSEï¼š<span class="math inline">\(S S E=\hat{\mathbf{a}}^{K} \underset{|c| c}{\hat{\mathbf{a}}} \operatorname{dist}^{2}\left(c_{i}, x\right)\)</span></p>
<p>åˆ«æ…Œï¼Œå…¶å®å°±æ˜¯ç±»é‡Œé¢çš„æ‰€æœ‰ç‚¹åˆ°ç±»ä¸­å¿ƒçš„è·ç¦»çš„å¹³æ–¹å’Œï¼Œè¿™å°±ä»£è¡¨äº†è¿™ä¸ªç±»å†…éƒ¨èšä¸èšæ‹¢ï¼ŒæŠŠæ•°æ®é›†é‡Œé¢æ‰€æœ‰çš„ç±»çš„è¿™ä¸ªå€¼åŠ èµ·æ¥ï¼Œå°±ä»£è¡¨äº†è¿™ä¸ªåˆ†ç±»æ–¹æ³•çš„åˆ†ç±»æ•ˆæœã€‚æ¯ä¸ªç±»å½“ç„¶æ˜¯è¶Šèšæ‹¢è¶Šå¥½ï¼Œæ‰€ä»¥SSEå€¼ä¹Ÿæ˜¯è¶Šä½è¶Šå¥½ã€‚</p>
<p>æœ€ä¼˜Kå€¼ï¼š</p>
<p>å¦‚ä½•é€‰æ‹©Kï¼Ÿå‰é¢æˆ‘ä»¬è¯´äº†éšä¾¿é€‰ï¼Œä½†æ˜¯è®¾æƒ³ä¸€ä¸‹ï¼Œå¯¹äºä¸€ä¸ªåº”è¯¥åªæœ‰2ç±»çš„é—®é¢˜ï¼ˆç”Ÿä¸æ­»ï¼‰ï¼Œæˆ‘ä»¬ç»™å®ƒåˆ†äº”ç±»ï¼Œé‚£è¿™æ˜¯ä¸åˆç†ä¸ç§‘å­¦çš„ã€‚è™½ç„¶æˆ‘ä»¬ä¸çŸ¥é“æ­£ç¡®ç­”æ¡ˆï¼Œä½†æ˜¯è¿˜æ˜¯æœ‰æ–¹æ³•å¯ä»¥åˆ¤æ–­ä¸€ä¸‹çš„ã€‚</p>
<p>æˆ‘ä»¬å°è¯•å¾ˆå¤šå¾ˆå¤šKï¼Œä»K=2è¿è¡Œåˆ°K=10ï¼Ÿ20ï¼Ÿä¸€éï¼Œåˆ†åˆ«è®¡ç®—å„è‡ªKçš„SSEï¼Œä¼šæœ‰ä¸€å¼ å¦‚ä¸‹ç±»ä¼¼çš„å›¾</p>
<p><img src="/images/image-20191016212820651.png"></p>
<p>æœ€ä½³çš„Kå€¼å°±æ˜¯å›¾ä¸­è¿™æ ·çš„æ‹ç‚¹ï¼ˆç»è¿‡å®ƒä¹‹åSSEçš„å˜åŒ–ä¸æ˜æ˜¾äº†ï¼‰</p>
<p>åˆå§‹ç‚¹é€‰æ‹©ï¼š</p>
<p>åˆå§‹ç‚¹çš„é€‰æ‹©æ˜¯ä¼šå½±å“K-meansçš„ç»“æœçš„ï¼æ‰€ä»¥æˆ‘ä»¬å†³å®šç”¨2ç§æ–¹æ³•ä¼˜åŒ–ä¸€ä¸‹åˆå§‹ç‚¹é”™è¯¯ã€‚</p>
<ol type="1">
<li>Multiple runs. å¤šè·‘å‡ æ¬¡ï¼Œå¤šéšæœºé€‰æ‹©å‡ æ¬¡ã€‚</li>
<li>æ¯æ¬¡runä¸€å¼€å§‹é€‰è¶…è¿‡Kä¸ªçš„åˆå§‹ç‚¹ï¼Œç„¶åé€‰æ‹©è¿™äº›é‡Œé¢è·ç¦»æœ€è¿œçš„ï¼ˆmost widely separatedï¼‰</li>
</ol>
<p>åæœŸå¤„ç†ï¼š</p>
<ol type="1">
<li>å¯ä»¥åˆ æ‰å¾ˆå°çš„clusterï¼ˆå¯èƒ½ä»£è¡¨äº†å¼‚å¸¸å€¼ï¼‰</li>
<li>åˆ†æ•£å®½æ¾çš„clusterï¼ˆå†…éƒ¨SSEå¾ˆå¤§çš„ç±»ï¼‰</li>
<li>åˆå¹¶æ¥è¿‘çš„clusterï¼ˆå†…éƒ¨SSEå¾ˆå°çš„ç±»ï¼‰</li>
</ol>
<p>K-meanséš¾ä»¥è§£å†³çš„é—®é¢˜ï¼š</p>
<ol type="1">
<li>æ¯ä¸ªç±»æ‹¥æœ‰ä¸åŒçš„å¤§å°</li>
</ol>
<p><img src="/images/image-20191017145039838.png"></p>
<p>K-meansç®—æ³•åªè¦æ±‚ç‚¹åˆ°ç±»ä¸­å¿ƒç‚¹è·ç¦»æœ€çŸ­ï¼Œä½†ä¼šå¿½ç•¥ç±»æœ¬èº«çš„å¤§å°ä¸åŒã€‚</p>
<ol start="2" type="1">
<li>æ¯ä¸ªç±»çš„å¯†åº¦ä¸ä¸€æ ·</li>
</ol>
<p><img src="/images/image-20191017145213113.png"></p>
<ol start="3" type="1">
<li>æ¯ä¸ªç±»çš„å½¢çŠ¶ä¸ä¸€æ ·</li>
</ol>
<p><img src="/images/image-20191017145248703.png"></p>
<h3 id="hierarchical-clusteringåˆ†å±‚èšç±»">3. Hierarchical Clusteringï¼ˆåˆ†å±‚èšç±»ï¼‰</h3>
<h4 id="æ¦‚å¿µ-2">æ¦‚å¿µ</h4>
<p><img src="/images/image-20191016195859318.png"></p>
<p>æˆ‘ä»¬å‘ç°ç°å®ç”Ÿæ´»ä¸­æœ‰å¾ˆå¤šçš„åˆ†ç±»éƒ½ä¼šåŒ…å«ä¸‹é¢æ›´åŠ ç»†åˆ†çš„ç±»ï¼Œæ‰€ä»¥å°±ç ”ç©¶å‡ºäº†åˆ†å±‚èšç±»ã€‚</p>
<p>æˆ‘ä»¬æœ‰2ç§åˆ†å±‚èšç±»çš„æ–¹æ³•ï¼Œä¸€ç§æ˜¯è‡ªä¸‹è€Œä¸Š(agglomerative)çš„ï¼Œä¸€ç§æ˜¯è‡ªä¸Šè€Œä¸‹çš„(divisive)ã€‚</p>
<p><img src="/images/image-20191017151143939.png"></p>
<p>è¿™å¼ å›¾è¡¨æ˜äº†è¿™2ç§æ–¹æ³•çš„å…·ä½“æ­¥éª¤ã€‚</p>
<h4 id="agglomerative-æ–¹æ³•">agglomerative æ–¹æ³•</h4>
<p>æ€è·¯ï¼š</p>
<ol type="1">
<li>agglomarativeæ˜¯è‡ªä¸‹è€Œä¸Šçš„ï¼Œæˆ‘ä»¬å¾—ä»æ¯ä¸€ä¸ªæ•°æ®ç‚¹å¼€å§‹ï¼Œåˆå¹¶æœ€æ¥è¿‘çš„2ä¸ªæ•°æ®ç‚¹ã€‚</li>
<li>å°†åˆå¹¶æˆåŠŸçš„æ•°æ®ç‚¹å˜æˆä¸€ä¸ªç±»</li>
<li>è®¡ç®—è¿™ä¸ªç±»ä¸å…¶ä»–çš„ç±»çš„è·ç¦»</li>
<li>é‡å¤1ï¼Œ2ï¼Œ3ï¼Œç›´åˆ°æœ€ååªå‰©ä¸‹ä¸€ä¸ªç±»ï¼ˆå³å…¨ä½“éƒ½è¢«åŒ…å«äº†ï¼‰</li>
</ol>
<p>æ ç²¾ç¬‘é½ï¼šæ­¥éª¤1é‡Œé¢ä»€ä¹ˆå«æœ€æ¥è¿‘ï¼Ÿæ­¥éª¤3é‡Œé¢çš„è·ç¦»æ€ä¹ˆç®—ï¼Ÿ</p>
<p>ä¾‹å­ï¼š</p>
<p>æˆ‘ä»¬æœ‰å·¦å›¾ä¸­è¿™æ ·çš„æ•°æ®åˆ†å¸ƒï¼š</p>
<p><img src="/images/image-20191107104418581.png"></p>
<ol type="1">
<li><p>æˆ‘ä»¬å…ˆè®¡ç®—æ¯ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»ï¼ˆè¿™é‡Œæˆ‘ä»¬é‡‡ç”¨[æ›¼å“ˆé¡¿è·ç¦»](#1. æ›¼å“ˆé¡¿è·ç¦»ï¼ˆaka &quot;City block&quot;, taxicab, <span class="math inline">\(L_1\)</span>norm , Manhattan distance)ï¼‰,ç»“æœä¸ºå³ä¸Šå›¾</p></li>
<li><p>åˆå¹¶<strong>è·ç¦»æœ€è¿‘</strong>çš„ä¸¤ä¸ªç‚¹ï¼Œå›¾ä¸­æˆ‘ä»¬å¾—åˆ°<span class="math inline">\(d(42,43) = 1\)</span> æ˜¯æœ€è¿‘çš„2ä¸ªç‚¹ï¼Œåˆå¹¶å®ƒä»¬</p></li>
<li><p>æˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªæ–°çš„è·ç¦»çŸ©é˜µ:</p>
<p><img src="/images/image-20191107104735380.png"></p>
<p>æœ‰äº†ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼Œæˆ‘ä»¬çŸ¥é“42ï¼Œ43åˆ†åˆ«å¯¹å…¶ä»–ç‚¹çš„è·ç¦»ï¼Œä½†æ˜¯æˆ‘ä»¬æ€ä¹ˆ<strong>è®¡ç®—(42,43)è¿™ä¸ªæ–°çš„ç±»åˆ°å…¶ä»–ç‚¹çš„è·ç¦»</strong>å‘¢ï¼Ÿ ä¹Ÿå°±æ˜¯å›¾ä¸­çš„é»„è‰²éƒ¨åˆ†æ˜¯æ€ä¹ˆè®¡ç®—çš„å‘¢ï¼Ÿ</p></li>
</ol>
<p>å½“æˆ‘ä»¬çŸ¥é“è®¡ç®—è¿™ä¸ªè·ç¦»åï¼Œæˆ‘ä»¬åªéœ€è¦ç»§ç»­é‡å¤åˆå¹¶æœ€è¿‘çš„2ä¸ªç‚¹ï¼ˆç±»ï¼‰ï¼Œç›´åˆ°åªæœ‰ä¸€ç±»ç»“æŸã€‚</p>
<p>æˆ‘ä»¬æœ‰å¾ˆå¤šç§è®¡ç®—åˆå¹¶åçš„æ–°ç±»ä¸å…¶ä»–ç±»çš„è·ç¦»è®¡ç®—æ–¹å¼ã€‚</p>
<ol type="1">
<li><p>MIN:</p>
<p><img src="/images/image-20191107105318615.png"></p>
<p>æˆ‘ä»¬é€‰å–æ–°ç±»æ‰€æœ‰ç‚¹åˆ°å…¶ä»–ç±»ä¸­æ‰€æœ‰ç‚¹ä¸­æœ€çŸ­çš„è·ç¦»ï¼Œä½œä¸ºè·ç¦»</p>
<p>ä¾‹å¦‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æˆ‘ä»¬å°±ç”¨äº†MINæ–¹æ³•ï¼š</p>
<p><img src="/images/image-20191107110531148.png"></p>
<p>æˆ‘ä»¬è®¡ç®—(42,43)æ–°ç±»å¯¹ç±»(18)çš„è·ç¦»ï¼š</p>
<p>æˆ‘ä»¬è®¡ç®—d(42,18) = 24, d(43,18) = 25, æˆ‘ä»¬ä½¿ç”¨MINä¸ºè·ç¦»å³(42,43)ç±»å¯¹(18)çš„è·ç¦»ä¸º24.</p>
<p>ä»¥æ­¤ç±»æ¨å¾—åˆ°(42,43)å¯¹æ‰€æœ‰å…¶ä»–ç±»çš„è·ç¦»ã€‚</p>
<p><img src="/images/image-20191107105452108.png"></p>
<p>æ¥ä¸‹æ¥é‡å¤æ­¥éª¤1ï¼Œæ­¥éª¤2ï¼Œæ‰¾åˆ°è·ç¦»æœ€å°çš„ä¸¤ä¸ªç±»ä¸ºd(25,27) = 2ï¼Œåˆå¹¶å®ƒä»¬åœ¨è®¡ç®—(25,27)å¯¹å…¶ä»–ç±»çš„è·ç¦»ã€‚ä¸€ç›´é‡å¤åˆ°åªæœ‰ä¸€ä¸ªç±»ã€‚å…·ä½“æ­¥éª¤ä¸ºï¼š</p>
<p><img src="/images/image-20191107111021167.png"></p>
<p>æˆ‘ä»¬å¾—åˆ°æœ€åçš„dendogramä¸ºï¼š</p>
<p><img src="/images/image-20191107111103027.png"></p>
<p>ä½ æƒ³è¦åˆ†å‡ ç±»å°±åœ¨è¿™ä¸ªæ ‘å›¾é‡Œåˆ‡ä¸€åˆ€ï¼Œä¾‹å¦‚ä½ æƒ³è¦æœ‰2ä¸ªç±»ï¼Œå°±æŠŠ<span class="math inline">\(C_5\)</span>åˆ‡äº†ï¼Œå°±æŠŠæ•°æ®åˆ†ä¸ºäº†<span class="math inline">\((C_4,C_1)\)</span>ä¸¤ç±»ã€‚</p>
<p><strong>åé¢çš„å„ç§æ–¹æ³•éƒ½ç±»ä¼¼MIN</strong></p></li>
<li><p>MAX:</p>
<p>æŠŠç”¨æœ€å°çš„è·ç¦»å˜æˆç”¨æœ€é•¿çš„è·ç¦»ï¼š</p>
<p><img src="/images/image-20191107111351503.png"></p></li>
<li><p>Group Average:</p>
<p>è·ç¦»å˜æˆè®¡ç®—æ‰€æœ‰ç‚¹åˆ°ç‚¹çš„è·ç¦»çš„å¹³å‡å€¼ï¼š</p>
<p><img src="/images/image-20191107111529510.png"></p></li>
<li><p>Centroid:</p>
<p>è®¡ç®—æ¯ä¸ªç±»çš„ç±»ä¸­å¿ƒï¼Œè·ç¦»ä¸º2ä¸ªç±»ä¸­å¿ƒçš„è·ç¦»ï¼š</p>
<p><img src="/images/image-20191107111600986.png"></p></li>
</ol>
<h5 id="limitation-of-min">Limitation of MIN:</h5>
<ol type="1">
<li><p>å¯¹noiseå¾ˆæ•æ„Ÿï¼š</p>
<p>åªç”¨ä¸€ä¸ªå€¼</p></li>
<li><p>äº§ç”Ÿé“¾æ¥ç°è±¡(chaining phenomenon):</p>
<p>ä¸¤ä¸ªæœ¬æ¥å¾ˆè¿œï¼Œä¸€å®šä¸å±äºåŒä¸€ç±»çš„ç‚¹pï¼Œqï¼Œä½†æ˜¯å®ƒä»¬ä¹‹é—´æœ‰å¾ˆå¤šç‚¹ï¼Œè¿™æ ·pæ¯æ¬¡æœ€è¿‘çš„ç‚¹éƒ½æ˜¯pï¼Œqè¿çº¿ä¹‹é—´çš„ç‚¹ï¼Œæœ€åä¼šå¯¼è‡´på’Œqåˆ†åˆ°äº†åŒä¸€ç±»ã€‚</p></li>
</ol>
<h5 id="limitation-of-max">Limitation of MAX:</h5>
<p><img src="/images/image-20191107113748190.png"></p>
<p>å¾ˆéš¾å¯¹å¤§å°ä¸åŒçš„ç±»è¿›è¡Œåˆ†ç±»ã€‚</p>
<p>ä½†æ˜¯MAXæ–¹æ³•æŠ—å™ªå£°å’Œå¼‚å¸¸å€¼çš„æ•ˆæœå¾ˆå¥½ï¼Œæ‰€ä»¥ä¸€èˆ¬ç¼–ç¨‹è½¯ä»¶å…³äºåˆ†å±‚èšç±»çš„å‚æ•°é»˜è®¤éƒ½ç”¨MAXæ–¹æ³•ã€‚</p>
<h4 id="divisive-æ–¹æ³•">divisive æ–¹æ³•</h4>
<h5 id="æ„å»ºæœ€å°æ‰«ææ ‘minimum-spanning-tree">æ„å»ºæœ€å°æ‰«ææ ‘ï¼ˆMinimum Spanning Treeï¼‰</h5>
<p><img src="/images/image-20191107114517311.png"></p>
<p>å¦‚å›¾ï¼Œé¦–å…ˆæ‰¾è·ç¦»æœ€è¿‘çš„ä¸¤ä¸ªç‚¹è¿çº¿ï¼Œæ¥ä¸‹æ¥æ‰¾æœ€è¿‘çš„(p,q)è¿çº¿ï¼Œå…¶ä¸­pæ˜¯åœ¨å·²ç»è¿çº¿çš„ç‚¹ä¸­ï¼Œqæ˜¯æ²¡æœ‰è¢«è¿çº¿çš„ç‚¹ã€‚æ‰¾åˆ°qåå°†qå’Œpè¿çº¿ã€‚é‡å¤è¿‡ç¨‹ã€‚</p>
<p>å½“ä½ å®Œæˆäº†Minimum Spanning Treeåï¼Œä½ å‡æ‰æœ€åå‡ æ¬¡è¿æˆçš„çº¿ï¼Œå°±æ˜¯å®Œæˆåˆ†ç±»ï¼ˆæ ¹æ®ä½ è¦æœ‰å¤šå°‘ä¸ªç±»ï¼‰</p>
<h3 id="density-based-clustering">4. Density-based clustering</h3>
<p>åŸºäºå¯†åº¦çš„èšç±»ç»å¸¸ç”¨åœ¨æˆ‘ä»¬çš„æ•°æ®åœ¨å±€éƒ¨åŒºåŸŸæœ‰å¾ˆé«˜çš„å¯†åº¦æ—¶</p>
<p>ä¾‹å¦‚ï¼š</p>
<p><img src="/images/image-20191107115105917.png"></p>
<h4 id="dbscan">DBSCAN</h4>
<p>DBSCANå°±æ˜¯é’ˆå¯¹ä»¥ä¸Šæƒ…å†µçš„ç®—æ³•ã€‚</p>
<p>é¦–å…ˆæˆ‘ä»¬æœ‰2ä¸ªè‡ªå·±å®šä¹‰çš„å€¼ï¼š</p>
<ol type="1">
<li><p>EPS: ä»¥ç‚¹ä¸ºåœ†å¿ƒï¼Œä»¥EPSä¸ºåŠå¾„åšåœ†ï¼Œåœ†å†…å…¶ä»–æ•°æ®ç‚¹çš„ä¸ªæ•°æˆ‘ä»¬å°±ç§°ä¸ºå¯†åº¦ã€‚</p></li>
<li><p>MinPtsï¼šå½“åœ†å†…çš„ç‚¹ä¸ªæ•°ï¼ˆå¯†åº¦ï¼‰è¶…è¿‡äº†MinPtsæ—¶ï¼ˆåŒ…å«ç‚¹æœ¬èº«ï¼‰ï¼Œæˆ‘ä»¬ç§°è¿™ä¸ªç‚¹æ˜¯core point</p>
<p>â€‹ å¦‚æœè¿™ä¸ªç‚¹å¯†åº¦æ²¡æœ‰è¶…è¿‡MinPtsï¼Œä½†æ˜¯å®ƒåœ¨core pointé™„è¿‘ï¼Œç§°ä¸ºBorder point</p>
<p>â€‹ æˆ–è€…è¯´Border pointçš„EPSé‡Œä¼šæœ‰core pointç‚¹å‡ºç°ã€‚</p>
<p>â€‹ å¦‚æœè¿™ä¸ªç‚¹æ—¢ä¸æ˜¯core pointä¹Ÿä¸æ˜¯border pointï¼Œå°±ç§°ä¸ºnoise point</p></li>
</ol>
<p>ä¾‹å­ï¼š</p>
<p><img src="/images/image-20191107120016766.png"></p>
<p>å…·ä½“èšç±»æ­¥éª¤ï¼š</p>
<ol type="1">
<li>å°†æ‰€æœ‰ç‚¹è´´ä¸Šcoreï¼Œborderï¼Œnoise pointçš„æ ‡ç­¾</li>
<li>åˆ é™¤noise point</li>
<li>å°†åœ¨EPSèŒƒå›´å†…çš„æ‰€æœ‰core pointsè¿ä¸Šçº¿</li>
<li>æ¯ä¸€ä¸ªå®Œæˆè¿çº¿çš„core pointsç»„å³ä½œä¸ºä¸€ç±»</li>
<li>å°†æ¯ä¸ªborder pointåˆ†é…ç»™ä»»æ„å®ƒä¸´è¿‘çš„core pointçš„é‚£ä¸€ç±»</li>
</ol>
<p>ä¾‹å­ï¼š</p>
<p><img src="/images/image-20191107120304306.png"></p>
<h4 id="dbscançš„ç¼ºç‚¹">DBSCANçš„ç¼ºç‚¹</h4>
<p>DBSCANå¤„ç†ä¸äº†å…·æœ‰ä¸åŒçš„å¯†åº¦çš„ç±»ï¼ˆä¸€ä¸ªç±»é‡Œé¢æœ‰å¤šç§å¯†åº¦ï¼‰ã€‚å®ƒåªèƒ½åŒºåˆ†é«˜å¯†åº¦çš„ç±»å’Œä½å¯†åº¦çš„ç±»ï¼Œä½†æ˜¯å¤„ç†ä¸äº†å¯†åº¦éƒ½ç›¸åŒï¼Œä½†å…¶å®æ˜¯ä¸åŒçš„ç±»çš„æƒ…å†µã€‚éœ€è¦æœ‰ä¸€ç‚¹çš„èƒŒæ™¯çŸ¥è¯†æ‰èƒ½ä½¿ç”¨ã€‚</p>
<h4 id="å¦‚ä½•é€‰å–epså’Œminpts">å¦‚ä½•é€‰å–EPSå’ŒMinPts</h4>
<ol type="1">
<li>é€‰å–ä¸€ä¸ªkï¼Œè®¡ç®—k-distanceï¼Œå³ç‚¹åˆ°ç¦»å®ƒæœ€è¿‘çš„ç¬¬Kä¸ªç‚¹çš„è·ç¦»</li>
<li>å¯¹è¿™äº›k-distanceè¿›è¡Œå‡åºæ’åº</li>
<li>plotç”»å›¾å¦‚ä¸‹</li>
</ol>
<p><img src="/images/image-20191107125740790.png"></p>
<p>çºµåæ ‡ä¸ºæ’åºçš„k-distanceï¼Œæ¨ªåæ ‡ä¸ºç‚¹çš„æ•°é‡ã€‚</p>
<p>æˆ‘ä»¬åœ¨å›¾ä¸­å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªsharp changeï¼ˆå›¾ä¸­k-distance = 10ï¼‰çš„æ—¶å€™ï¼Œ</p>
<p>æˆ‘ä»¬å°±é€‰è¿™ä¸ªk-distance(10)ä¸ºEPSï¼Œk(4)ä¸ºMinPtsã€‚</p>
<p>ä¸€èˆ¬æ ¹æ®ç»éªŒï¼Œkéƒ½é€‰4</p>
<h3 id="cluster-validity">5. Cluster Validity</h3>
<p>å¯¹äºclassificationï¼Œæˆ‘ä»¬æœ‰accuracyï¼Œrecallï¼Œprecisionç­‰è¯„ä¼°æ–¹å¼ã€‚</p>
<p>é‚£å¯¹äºClusteræˆ‘ä»¬èƒ½æœ‰ä»€ä¹ˆå‘¢ï¼Ÿ</p>
<p>é¦–å…ˆæˆ‘ä»¬çŸ¥é“è¿™æ˜¯å¾ˆå›°éš¾çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶æ²¡æœ‰æ­£ç¡®ç­”æ¡ˆï¼Œæˆ‘ä»¬è‡ªå·±éƒ½ä¸çŸ¥é“æ•°æ®é›†çš„æ ‡ç­¾ã€‚å½“ä¸€ç§åˆ†ç±»ç®—æ³•ç»™æ•°æ®ä¸Šæ ‡ç­¾åï¼Œè‡ªç„¶è€Œç„¶æˆ‘ä»¬ä¹Ÿä¸çŸ¥é“å®ƒæ˜¯å¯¹çš„è¿˜æ˜¯é”™çš„ã€‚</p>
<p>ä½†æ˜¯æˆ‘ä»¬è¿˜æ˜¯è¦å¯»æ‰¾ä¸€äº›å¯èƒ½çš„æ–¹æ³•çš„ï¼Œç”¨è¿™äº›æ–¹æ³•å¯ä»¥å¸®æˆ‘ä»¬æ¥åˆ¤æ–­ï¼š</p>
<ol type="1">
<li>è¿™ä¹ˆå¤šèšç±»æ–¹æ³•ï¼Œå“ªä¸ªæ¯”è¾ƒå¥½ï¼Œè¯¥ç”¨å“ªä¸ªï¼Ÿ</li>
<li>ç±»å’Œç±»ä¹‹é—´çš„ä¿¡æ¯</li>
<li>ç±»å†…éƒ¨çš„ä¿¡æ¯</li>
<li>é¿å…å™ªå£°å¼‚å¸¸å€¼ä¹‹ç±»çš„å¹²æ‰°</li>
</ol>
<p>æ ¹æ®ä¸åŒçš„èšç±»ç®—æ³•ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºå®Œå…¨ä¸åŒçš„åˆ†ç±»ã€‚ä¾‹å¦‚ï¼š</p>
<p><img src="/images/image-20191107130848744.png"></p>
<p>ä¸‹é¢ä»‹ç»å‡ ç§ä¸åŒçš„æ–¹å¼ï¼š</p>
<h4 id="groud-truth">1. Groud Truth</h4>
<p>Groud Truthæ˜¯è¿™ä¸ªæ•°æ®é›†çœŸå®çš„æ ‡ç­¾ã€‚</p>
<p>éå¸¸çš„æ‰¯æ·¡ï¼Œå› ä¸ºæˆ‘ä»¬éƒ½çŸ¥é“çœŸå®çš„æ ‡ç­¾è¿˜å¹²å˜›è¦ç”¨èšç±»æ–¹æ³•ã€‚</p>
<p>ä¸è¿‡è¿™ä¸ªæ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬éªŒè¯æˆ‘ä»¬èšç±»ç®—æ³•ï¼Œå½“å‘ç°äº†ä¸€ç§æ–°çš„èšç±»ç®—æ³•çš„æ—¶å€™ï¼Œå¯ä»¥åœ¨å·²çŸ¥æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œå¹¶ç”¨Groud Truthæ¥åˆ¤æ–­ã€‚</p>
<p>æ–¹æ³•ä¹Ÿç‰¹åˆ«çš„ç®€å•ã€‚</p>
<p>groud truthï¼š çœŸå®çš„æ ‡ç­¾/ç±»ä¸º$ P = {P_1,â€¦,P_s}$</p>
<p>èšç±»æ–¹æ³•å¾—å‡ºçš„æ ‡ç­¾/ç±»ä¸º <span class="math inline">\(C = \{C_1,â€¦,C_t\}\)</span></p>
<p><img src="/images/image-20191107131546291.png"></p>
<p><img src="/images/image-20191107131602994.png"></p>
<p>ä¾‹å­ï¼š</p>
<p><img src="/images/image-20191107132455283.png"></p>
<ol start="2" type="1">
<li>Purity</li>
</ol>
<p><img src="/images/image-20191107134314241.png"></p>
<h4 id="internal-measure">2. Internal Measure</h4>
<p>åªä½¿ç”¨æ•°æ®é›†è‡ªèº«çš„ä¿¡æ¯æ¥åˆ¤æ–­ã€‚</p>
<ol type="1">
<li>Cohesionï¼šç±»çš„èšæ‹¢ç¨‹åº¦</li>
<li>Separationï¼šç±»å’Œç±»ä¹‹é—´çš„ç¦»æ•£ç¨‹åº¦</li>
</ol>
<h5 id="sseå’Œbss">SSEå’ŒBSS</h5>
<p>è®¡ç®—cohesionçš„æ–¹æ³•ç±»ä¼¼äºä¹‹å‰æåˆ°çš„SSEï¼š</p>
<p><span class="math inline">\(S S E=W S S=\sum_{i} \sum_{x \in C_{i}}\left(x-m_{i}\right)^{2}\)</span></p>
<p>è®¡ç®—separationç”¨WSSï¼š</p>
<p><span class="math inline">\(\begin{aligned} B S S &amp;=\sum_{i}\left|C_{i}\right|\left(m-m_{i}\right)^{2} \\-&amp; \text { Where }|C| \text { is the size of cluster } i \end{aligned}\)</span></p>
<p>ä¾‹å­ï¼š</p>
<p><img src="/images/image-20191107135247397.png"></p>
<h5 id="silhouette-coefficient">Silhouette Coefficientï¼š</h5>
<p>è¿™ç§æ–¹æ³•æ—¢è€ƒè™‘äº†cohesionä¹Ÿè€ƒè™‘äº†separation</p>
<p><img src="/images/image-20191107135420153.png"></p>
<p>aæ˜¯iç‚¹åˆ°å®ƒç±»å†…å…¶ä»–ç‚¹çš„å¹³å‡è·ç¦»ï¼ˆcohesionï¼‰</p>
<p>bæ˜¯ç¦»iæœ€è¿‘çš„ç±»çš„æ‰€æœ‰ç‚¹çš„å¹³å‡è·ç¦»ï¼ˆseparationï¼‰</p>
<h5 id="correlation-with-distance-matrix">Correlation with Distance Matrix</h5>
<p>è®¡ç®—2ä¸ªçŸ©é˜µï¼š</p>
<ol type="1">
<li>distance matrixï¼ˆæ¯ä¸ªæ•°æ®çš„è·ç¦»ï¼‰</li>
<li>incidence matrixï¼ˆæ¯ä¸ªæ•°æ®æ˜¯å¦å±äºåŒä¸€ç±»ï¼‰</li>
</ol>
<p>è®¡ç®—å…¬å¼ä¸ºï¼š</p>
<p><span class="math inline">\(r=\frac{\sum_{i=1, j=1}^{n}\left(d_{i j}-\bar{d}\right)\left(c_{i j}-\bar{c}\right)}{\sqrt{\sum_{i=1, j=1}^{n}\left(d_{i j}-\bar{d}\right)^{2}} \sqrt{\sum_{i=1, j=1}^{n}\left(c_{i j}-\bar{c}\right)^{2}}}\)</span></p>
<p>dæ˜¯distance matrixï¼Œcæ˜¯incidence matrix</p>
<p>|r| è¶Šé«˜ä»£è¡¨èšç±»æ•ˆæœè¶Šå¥½ã€‚ ï¼ˆåªè€ƒè™‘ç»å¯¹å€¼ï¼‰</p>
<p>ä¸‹å›¾æ˜¯ç”¨k-meanså¯¹2ä¸ªæ•°æ®é›†åšèšç±»åï¼Œæ±‚å‡ºçš„correlationå€¼</p>
<p><img src="/images/image-20191107140059885.png"></p>
<p>|Corr|è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½ã€‚</p>
<h4 id="statistical-framework-for-sse">3. Statistical Framework for SSE</h4>
<p>å½“æˆ‘ä»¬æƒ³è¦è¯„ä¼°ä¸€ä¸ªèšç±»æ–¹æ³•çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ªæ–¹æ³•ç”¨åœ¨ç›®æ ‡æ•°æ®é›†å’Œå¾ˆå¤šä¸ªéšæœºç”Ÿæˆçš„æ•°æ®é›†ä¸­ã€‚</p>
<p>åˆ†åˆ«è®¡ç®—å®ƒä»¬çš„SSEã€‚</p>
<p>å½“æˆ‘ä»¬å‘ç°ï¼Œç›®æ ‡æ•°æ®é›†çš„SSEå’Œéšæœºæ•°æ®é›†çš„SSEéå¸¸çš„ä¸åŒæ—¶ï¼Œæˆ‘ä»¬å°±èƒ½çŸ¥é“è¿™ä¸ªèšç±»æ–¹å¼å¯¹ç›®æ ‡æ•°æ®é›†æ˜¯æœ‰æ•ˆçš„ï¼ˆå³å®ƒåªå¯¹ç›®æ ‡æ•°æ®é›†æ•ˆæœå¥½ï¼Œè€Œä¸æ˜¯éšä¾¿ä»€ä¹ˆæ•°æ®é›†éƒ½æ˜¯æ•ˆæœå¥½çš„ï¼‰ã€‚</p>
<h2 id="iv.-association-rule-å…³è”æ€§åˆ†æ">IV. Association Rule å…³è”æ€§åˆ†æ</h2>
<hr>
<p>ç›®çš„ï¼šå‘ç°äº‹ç‰©ä¹‹é—´æœ‰è¶£çš„è§„åˆ™ã€‚</p>
<p>è¦æ˜¯æˆ‘ä»¬å­¦ä¼šäº†è¿™ä¸ªï¼Œå¯ä»¥åšå¹¿å‘Šæ¨èã€‚</p>
<p>ä¾‹å¦‚ä½ é€›è¶…å¸‚çš„æ—¶å€™çœ‹ä¸€ä¸ªä¸œè¥¿ï¼Œæ¯”å¦‚ç‰›å¥¶ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“<strong>è´­ä¹°ç‰›å¥¶çš„äººæœ‰å¾ˆé«˜æ¦‚ç‡ä¹°é¢åŒ…</strong>ï¼Œé‚£è¶…å¸‚åœ¨ç‰›å¥¶æ—è¾¹å°±åœ¨é¢åŒ…ï¼Œæ¶ˆè´¹è€…å¾ˆæœ‰å¯èƒ½ç›´æ¥é¡ºæ‰‹æ‹¿äº†ã€‚</p>
<p>è€Œå¦‚ä½•è®¡ç®—è¿™ä¸ªæ¦‚ç‡ï¼Œäº†è§£å¯¹è±¡ä¹‹é—´çš„å…³è”æ€§åˆ†æï¼Œå°±æ˜¯æ­¤ç« çš„å†…å®¹ã€‚</p>
<p>å®šä¹‰notationï¼š</p>
<p>ä¸€ä¸ªassociation ruleå¯ä»¥è¡¨ç¤ºä¸º $ X Y$</p>
<p>å‰é¢çš„ä¾‹å­å¯ä»¥æ˜¾ç¤ºä¸º <span class="math inline">\(ç‰›å¥¶ \rightarrow é¢åŒ…\)</span></p>
<h3 id="measures">1. Measures</h3>
<h4 id="support">1. Support</h4>
<p>Rule: $ X Y$</p>
<p>æœ‰2ç§supportçš„ç®—æ³•ï¼Œç¬¬ä¸€ç§æ˜¯è®¡ç®—åŸºæ•°ï¼ˆé¢‘æ•°ï¼‰</p>
<p><span class="math inline">\(Support( X \rightarrow Y) = | X \cup Y|\)</span></p>
<p>ç¬¬äºŒç§æ˜¯è®¡ç®—æ¦‚ç‡</p>
<p><span class="math inline">\(Support( X \rightarrow Y) = Pï¼ˆX \cup Yï¼‰\)</span></p>
<p>ä¾‹å­ï¼š</p>
<p>æˆ‘ä»¬æœ‰ä¸€ä¸ªé›†åˆä¸º{milk, coke, pepsi, beer, juice}</p>
<p>æˆ‘ä»¬çš„é˜ˆå€¼è®¾å®šä¸ºå½“support &gt; 3 æ—¶ï¼Œåˆ¤æ–­è¿™ä¸ªruleæ˜¯frequent</p>
<p><span class="math inline">\(B_1 = \{m, c, b\}, B_2 = \{m, p, j\}, B_3 = \{m, b\}, B_4 = \{c, j\}, B_5 = \{m, p, b\}, B_6 = \{m, c, b, j\}, B_7 = \{c, b, j\}, B_8 = \{b, c\}\)</span></p>
<p><span class="math inline">\(S(m \rightarrow b) = 4, S(b \rightarrow c)= 4 , S(c \rightarrow j) =3\)</span></p>
<p>Support å¹¶ä¸å…³å¿ƒé¡ºåºï¼Œä¸Šé¢çš„åè¿‡æ¥ä¹Ÿè¡Œã€‚</p>
<h4 id="confidence">2. Confidence</h4>
<p><span class="math inline">\(\text { Confidence }(\mathrm{X} \rightarrow \mathrm{Y})=\mathrm{P}(\mathrm{Y} | \mathrm{X})=\mathrm{P}(\mathrm{X} \cup \mathrm{Y}) / \mathrm{P}(\mathrm{X})\)</span></p>
<p>æ ¹æ®å®šä¹‰æˆ‘ä»¬çŸ¥é“ï¼š</p>
<p><span class="math inline">\(\text { Support }(\mathrm{X} \rightarrow \mathrm{Y}) = \text { Support }(\mathrm{Y} \rightarrow \mathrm{X})\)</span></p>
<p><span class="math inline">\(\text { Confidence }(\mathrm{X} \rightarrow \mathrm{Y}) \neq \text { Confidence }(\mathrm{Y} \rightarrow \mathrm{X})\)</span></p>
<p>æ‰¾åˆ°ä¸€ä¸ªæˆ‘ä»¬è®¤ä¸ºæœ‰å…³è”çš„å…³ç³»éœ€è¦åŒæ—¶è¶…è¿‡æˆ‘ä»¬ç»™å®šçš„supporté˜ˆå€¼å’Œconfidenceé˜ˆå€¼ã€‚</p>
<h3 id="apriori-algorithm">2. Apriori Algorithm</h3>
<p>å¯¹äºä¸€ä¸ªè¶…çº§å¤§çš„æ•°æ®é›†ï¼Œå¯¹æ¯ä¸ªitemå’Œå®ƒä»¬çš„ç»„åˆåˆ†åˆ«æ±‚å‡ºsupportå’Œconfidenceæ¥æ‰¾å‡ºå…³è”æ€§ä¿¡æ¯æ˜¯åŸºæœ¬ä¸å¯èƒ½çš„ï¼ˆè®¡ç®—é‡å¤ªå¤§ï¼‰ã€‚</p>
<p>é¦–å…ˆæˆ‘ä»¬å‘ç°å¯¹äºä¸€ä¸ªç»„åˆ<span class="math inline">\(\{X,Y,Z,â€¦,\}\)</span>, å®ƒçš„supportéƒ½æ˜¯ç›¸ç­‰çš„è€Œæ— å…³<span class="math inline">\((\rightarrow)\)</span>ã€‚ä½†æ˜¯confidenceæ˜¯ä¸åŒçš„ã€‚</p>
<p>ç”±æ­¤æˆ‘ä»¬å‘ç°ï¼Œè®¡ç®—supportéå¸¸çš„å¿«ï¼ˆæ¯”confidenceå¿«ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å…ˆæ‰¾æ»¡è¶³supportçš„itemï¼Œå†åœ¨è¿™ä¸ªitemé‡Œæ‰¾æ»¡è¶³confidenceçš„å…³ç³»ã€‚</p>
<p>è¶…å¸‚é‡Œé¢æœ‰å•†å“{A,B,C,D,E}ï¼Œåˆ™å®ƒä»¬æ‰€æœ‰å¯ä»¥çš„itemç»„åˆä¸ºå¦‚ä¸‹ï¼š</p>
<p><img src="/images/image-20191107151133433.png"></p>
<p>Aprioriå®šç†ï¼šå¦‚æœä¸€ä¸ªitemä¸æ˜¯frequentçš„ï¼Œå®ƒçš„æ‰€æœ‰å­é›†subitemséƒ½ä¸æ˜¯frequentçš„ã€‚</p>
<p><img src="/images/image-20191107152732396.png"></p>
<p>å¦‚å›¾å½“ä½ è®¡ç®—å‡ºABä¸æ˜¯frequentåï¼Œä¸ç”¨åœ¨è®¡ç®—ä»»ä½•åŒ…å«ABçš„itemäº†ï¼Œå¤§å¤§çš„å‡å°‘äº†è®¡ç®—é‡ã€‚</p>
<p>ç®—æ³•è¿‡ç¨‹ï¼š</p>
<ol type="1">
<li>è®¡ç®—å•ä¸ªitemçš„support</li>
<li>å‰”é™¤ä½äºé˜ˆå€¼supportçš„itemï¼Œä¿ç•™æ»¡è¶³æ¡ä»¶çš„å•ä¸ªitem</li>
<li>ç”ŸæˆåŒ…å«2ä¸ªå¯¹è±¡çš„itemï¼Œè®¡ç®—supportï¼Œå‰”é™¤ä¸æ»¡è¶³çš„ï¼Œä¿ç•™æ»¡è¶³çš„</li>
<li>ç»§ç»­ç”Ÿäº§ï¼Œè®¡ç®—ï¼Œåˆ é™¤ï¼Œä¿ç•™</li>
</ol>
<p>ä¾‹å­ï¼š</p>
<p><img src="/images/image-20191107155311317.png"></p>
<p>Cæ˜¯ç”Ÿæˆçš„å¯èƒ½itemï¼ŒLæ˜¯æ‰€æœ‰æ»¡è¶³minsupçš„itemç»„åˆã€‚</p>
<p>æ‰€æœ‰çš„Lå°±æ˜¯æˆ‘ä»¬è¦æ‰¾çš„æ»¡è¶³minsupçš„itemã€‚</p>
<p>é—®é¢˜æ˜¯æ€ä¹ˆç”Ÿäº§Cå‘¢ï¼Ÿ</p>
<p>åˆ†2æ­¥ï¼š</p>
<ol type="1">
<li>self-join <span class="math inline">\(L_k\)</span></li>
<li>pruning</li>
</ol>
<p>self-joinçš„ä¾‹å­:</p>
<p>ä¸Šå›¾çš„<span class="math inline">\(L_2 = \{ AC,BC,BE,CE\}\)</span></p>
<p>ç¬¬ä¸€æ­¥ï¼š<span class="math inline">\(L_2 \ join\  L_2 = {ABC,ABE,ACE,BCE}\)</span></p>
<p>ç¬¬äºŒæ­¥ï¼šABE ä¼šè¢«åˆ æ‰å› ä¸º AEä¸åœ¨<span class="math inline">\(L_2\)</span>é‡Œé¢ï¼Œæ ¹æ®Aprioriç®—æ³•ï¼ŒAEä¸æ˜¯frequentï¼Œå®ƒçš„å­ç±»å¿…ä¸frequentã€‚å› æ­¤ABEä¼šè¢«åˆ æ‰ã€‚</p>
<p>å› æ­¤æˆ‘ä»¬å¾—åˆ°<span class="math inline">\(C_3 = \{BCE,ABC,ACE\}\)</span></p>
<h3 id="rule-generation">3. Rule Generation</h3>
<p>å½“æˆ‘ä»¬å¾—åˆ°æ‰€æœ‰æ»¡è¶³minSUPçš„itemåï¼Œæˆ‘ä»¬è¿˜è¦æ‰¾åˆ°æ»¡è¶³minCONFçš„itemï¼Œå³<span class="math inline">\(\rightarrow\)</span>è¯¥æ˜¯è°æŒ‡å‘è°ã€‚</p>
<p>ä¾‹å¦‚å½“æˆ‘ä»¬çŸ¥é“item{A,B,C,D}æ»¡è¶³minSUPï¼Œå®ƒæœ‰ä»¥ä¸‹è¿™ä¹ˆå¤šç§å¯èƒ½ï¼š</p>
<p><img src="/images/image-20191107160738277.png"></p>
<p>æ€»å…±ä¼šæœ‰<span class="math inline">\(2^{k-2}\)</span>ç§å¯èƒ½ï¼Œkä¸ºitemåŒ…å«çš„ä¸ªæ•°ã€‚</p>
<p>å¯¹äºconfidenceï¼Œæˆ‘ä»¬èƒ½æ‰¾åˆ°è¿™æ ·çš„å…³ç³»ï¼š</p>
<p><img src="/images/image-20191107161118117.png"></p>
<p>2ä¸ªå…³ç³»ï¼Œå·¦è¾¹å¤šæ•°çš„ä¸€å®šå¤§ã€‚</p>
<p><img src="/images/image-20191107161637990.png"></p>
<h3 id="maximal-frequent-itemset">Maximal Frequent Itemset</h3>
<p>ä¸€ä¸ªitemsetå¯ä»¥è¢«ç§°ä¸ºMaximal Frequent Itemsetå½“å®ƒæ˜¯frequentå¹¶ä¸”å®ƒçš„ä¸‹ä¸€å±‚è¶…é›†ä¸æ˜¯frequentçš„ã€‚</p>
<p><img src="/images/image-20191107162038096.png"></p>
<p>ä¾‹å­ï¼š</p>
<p><img src="/images/image-20191107162232045.png"></p>
<h3 id="closed-itemset">Closed itemset</h3>
<p>ä¸€ä¸ªitemsetæ˜¯closed itemsetï¼Œé‚£ä¹ˆå®ƒçš„immediate supersetçš„supportså’Œå®ƒçš„supportä¸ä¸€æ ·ã€‚</p>
<p>ä¾‹å­ï¼š</p>
<p>æˆ‘ä»¬æœ‰è¿™æ ·ä¸€ä¸ªtransaction</p>
<p><img src="/images/image-20191107162626321.png"></p>
<p>è®¡ç®—supportï¼š</p>
<p><img src="/images/image-20191107162704381.png"></p>
<p>é»„è‰²çš„éƒ¨åˆ†å°±æ˜¯closed itemsetã€‚</p>
<p>ä¾‹å¦‚æˆ‘ä»¬è€ƒè™‘{B}, S(B) = 5,</p>
<p>Bçš„immediate supersetsçš„supportä¸ºS(A,B) = 4, S(B,C)=3, S(B,D) = 4</p>
<p>ä»–ä»¬éƒ½ä¸ç­‰äºS(B)ï¼Œå› æ­¤Bæ˜¯closed itemsetã€‚</p>
<h2 id="v.-anomalyoutlier-å¼‚å¸¸å€¼æ£€æµ‹">V. Anomaly/Outlier å¼‚å¸¸å€¼æ£€æµ‹</h2>
<h3 id="æ¦‚å¿µä»‹ç»">1. æ¦‚å¿µä»‹ç»</h3>
<h4 id="ä»€ä¹ˆæ˜¯å¼‚å¸¸å€¼">ä»€ä¹ˆæ˜¯å¼‚å¸¸å€¼ï¼Ÿ</h4>
<blockquote>
<p>The set of data points that are <strong>considerably different</strong> from the remainder of the data</p>
<p>ä¸å…¶ä½™æ•°æ®æœ‰<strong>æ˜¾ç€å·®å¼‚</strong>çš„ä¸€ç»„æ•°æ®ç‚¹</p>
</blockquote>
<p>æ ç²¾æé—®1: é‚£æ€ä¹ˆåˆ¤æ–­è¿™ä¸ªæ•°æ®ç‚¹æ˜¯å’Œå…¶ä»–çš„æ•°æ®æ˜¾è‘—å·®å¼‚çš„å‘¢ï¼Ÿ</p>
<h4 id="å¼‚å¸¸å€¼æ£€æµ‹å¯ä»¥ç”¨åœ¨å“ª">å¼‚å¸¸å€¼æ£€æµ‹å¯ä»¥ç”¨åœ¨å“ªï¼Ÿ</h4>
<ul>
<li><p>æ•°æ®æ¸…æ´—ï¼Œå¼‚å¸¸å€¼ä¼šé€ æˆé”™è¯¯çš„åˆ¤æ–­</p></li>
<li><p>æŠŠæ‰¾å‡ºé‚£ä¸ªå¼‚å¸¸å€¼å½“ä½œç›®æ ‡ï¼Œä¾‹å¦‚æ•…éšœæ£€æµ‹ï¼Œç½‘ç»œå…¥ä¾µæ£€æµ‹ï¼ˆå› ä¸ºå¼‚å¸¸å€¼å’Œæ­£å¸¸å€¼æœ‰å¾ˆå¤§çš„å·®å¼‚ï¼Œæ‰€ä»¥å®ƒä»¬å¯èƒ½ä¼šæ˜¯æ•…éšœæˆ–è€…äººä¸ºæ•…æ„çš„è¡Œä¸ºï¼‰</p></li>
</ul>
<h4 id="å¼‚å¸¸å€¼outlier-å’Œå™ªå£°noiseæ˜¯ä¸æ˜¯ä¸€ä¸ªæ¦‚å¿µ">å¼‚å¸¸å€¼ï¼ˆoutlier) å’Œå™ªå£°ï¼ˆnoiseï¼‰æ˜¯ä¸æ˜¯ä¸€ä¸ªæ¦‚å¿µï¼Ÿ</h4>
<p><strong>ä¸æ˜¯ï¼</strong></p>
<p>å™ªå£°æ˜¯éšæœºå‡ºç°çš„é”™è¯¯ã€‚æ¯”å¦‚ä½ è¾“å…¥çš„æ—¶å€™æ‰“é”™äº†å­—ï¼Œåˆ«äººå¡«é—®å·è°ƒæŸ¥çš„æ—¶å€™å‹¾é”™äº†é€‰é¡¹ï¼ˆä¸æ˜¯æ•…æ„çš„ï¼‰ã€‚</p>
<p>å¼‚å¸¸å€¼æ˜¯<strong>çœŸå®å‘ç”Ÿ</strong>çš„ï¼Œä½†æ˜¯å’Œæ­£å¸¸çš„æ•°æ®ä¸ä¸€æ ·ï¼ˆä¾‹å¦‚ç»åœ°æ±‚ç”Ÿå¼€æŒ‚çš„å­¤å„¿çš„æ•°æ®å˜æ€çš„ç¦»è°±ï¼‰ã€‚å› æ­¤ï¼Œå¼‚å¸¸å€¼æ˜¯å¾ˆæœ‰è¶£çš„è€Œä¸”æœ‰æ—¶å€™å¯ä»¥ä½œä¸ºæ•°æ®æŒ–æ˜çš„ç›®çš„ï¼ˆæŸ¥å‡ºé‚£äº›å¼€æŒ‚çš„å­¤å„¿ï¼Œå†å°æ‰å®ƒä»¬ï¼‰ã€‚</p>
<p>æ€»ç»“ï¼š</p>
<ul>
<li><p>å™ªå£°ï¼ˆnoiseï¼‰æ— è®ºå¹²å˜›éƒ½æ˜¯ç¬¬ä¸€ä¸ªéœ€è¦è¢«æ¸…æ´—æ‰çš„ï¼ˆå™ªå£°ï¼šæˆ‘åšé”™äº†ä»€ä¹ˆï¼‰</p>
<p>æ ç²¾ç¬‘é½ï¼šé‚£æ¸…æ´—å™ªå£°çš„æ–¹æ³•å‘¢ï¼Ÿ</p>
<p><del>ï¼ˆæˆ‘çš„æƒ³å½“ç„¶ï¼šåœ¨å®é™…æƒ…å†µä¸­ï¼Œå™ªå£°å’Œå¼‚å¸¸å€¼åœ¨ç”¨æ¸…æ´—æ–¹æ³•çš„æ—¶å€™éƒ½åŒæ—¶è¢«å»æ‰äº†ã€‚è€Œå¦‚æœç›®çš„æ˜¯æ‰¾å‡ºå¼‚å¸¸å€¼çš„è¯ï¼Œåœ¨æ‰¾åˆ°å¼‚å¸¸å€¼å’Œå™ªå£°çš„åˆé›†ä¹‹åï¼Œæ ¹æ®ä¸šåŠ¡é€»è¾‘æ¥åˆ¤æ–­ã€‚å› ä¸ºå™ªå£°éƒ½æ˜¯éšæœºçš„é”™è¯¯ï¼Œè€Œå¼‚å¸¸å€¼æ˜¯æœ‰é€»è¾‘çš„ï¼Œè€Œä¸”è¿™ä¸ªåˆé›†åº”è¯¥æ˜¯æ¯”è¾ƒå°çš„ï¼Œå¯ä»¥å•ç‹¬åˆ¤æ–­å§ã€‚</del></p>
<p>å‘é‚®ä»¶ç»™è€å¸ˆåçš„å›å¤ï¼š<a href="https://medium.com/@aatl2012/the-basic-difference-between-noise-and-outliers-in-data-cd3ff32343e0" target="_blank" rel="noopener">å™ªå£°å’Œå¼‚å¸¸ç‚¹çš„åŒºåˆ«</a></p></li>
<li><p>å¼‚å¸¸å€¼ï¼ˆoutlierï¼‰ç»å¸¸åœ¨å™ªå£°è¢«å‰”é™¤åä¹Ÿè¢«åˆ æ‰ï¼Œä½†ä¹Ÿæœ‰å¯èƒ½é¡¹ç›®ç›®çš„å°±æ˜¯æ£€æµ‹å‡ºå¼‚å¸¸å€¼ã€‚</p></li>
</ul>
<h4 id="å¼‚å¸¸å€¼è¯„ä¼°">å¼‚å¸¸å€¼è¯„ä¼°</h4>
<p>æˆ‘ä»¬æ‰¾å‡ºæ¥çš„å¼‚å¸¸å€¼æ˜¯<strong>æ­£ç¡®çš„ï¼ŒçœŸçš„</strong>çš„å¼‚å¸¸å€¼å˜›ï¼Ÿè¯¥æ€ä¹ˆè¯„ä¼°ï¼ˆevaluateï¼‰æˆ‘è¿™ä¸ªæ–¹æ³•æ‰¾å‡ºæ¥çš„å¼‚å¸¸å€¼å¥½ä¸å¥½ï¼Ÿ</p>
<p>å¾ˆéš¾ç•Œå®šï¼Œå› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“æ­£ç¡®ç­”æ¡ˆå•Šã€‚å¼‚å¸¸å€¼æ£€æµ‹ä¹Ÿæ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Œä»–çš„è¯„ä¼°å’Œèšç±»ï¼ˆclusteringï¼‰ç±»ä¼¼ã€‚</p>
<h4 id="å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ¡ˆschema">å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ¡ˆï¼ˆschemaï¼‰</h4>
<p>é¡¾åæ€ä¹‰ï¼Œé¦–å…ˆå®šä¹‰å‡ºæ­£å¸¸çš„å€¼ï¼ˆå¤§éƒ¨åˆ†çš„æ•°æ®ï¼‰ã€‚ç„¶åå’Œæˆ‘ä»¬å®šä¹‰çš„æ­£å¸¸å€¼æœ‰æ˜¾è‘—å·®å¼‚çš„å€¼å°±æ˜¯å¼‚å¸¸å€¼äº†å˜›ã€‚</p>
<h4 id="å¼‚å¸¸å€¼ç±»å‹">å¼‚å¸¸å€¼ç±»å‹</h4>
<ol type="1">
<li><p>å…¨å±€å¼‚å¸¸å€¼ï¼ˆGlobal outlierï¼‰ï¼šè¿™ä¸ªç‚¹å’Œç»å¤§å¤šæ•°æ­£å¸¸å€¼éƒ½æœ‰æ˜¾è‘—å·®å¼‚ã€‚ï¼ˆå…¶å®å°±æ˜¯å®šä¹‰ï¼Œæœ€åœŸå‘³ï¼Œæœ€ç›´è§‚çš„ï¼Œä¹Ÿæ˜¯å¤§å¤šæ—¶å€™æˆ‘ä»¬æ‰€è¦æ‰¾çš„å¼‚å¸¸å€¼ï¼‰</p></li>
<li><p>Contextual outlierï¼ˆä¸Šä¸‹æ–‡å¼‚å¸¸å€¼ï¼Ÿæ²¡æ‰¾åˆ°ä¸“ä¸šçš„ä¸­æ–‡åè¯ï¼‰ï¼šè¿™ä¸ªç‚¹å’Œç‰¹å®šçš„ä¸Šä¸‹æ–‡æ•°æ®æœ‰æ˜¾è‘—å·®å¼‚ã€‚</p>
<p>ä¸€ä¸ªæ–¹ä¾¿ç†è§£çš„ä¾‹å­ï¼šä»Šå¤©æ¸©åº¦40åº¦ï¼Œæˆ‘ä»¬ä¸€èˆ¬çš„å¤©æ°”20å¤šåº¦æˆ–è€…30å¤šåº¦ã€‚å¦‚æœç”¨å…¨å±€å¼‚å¸¸å€¼çš„æ–¹æ³•ï¼Œå¯èƒ½è¿™ä¸ª40è¿˜æ˜¯åŒ…æ‹¬åœ¨æ­£å¸¸çš„æµ®åŠ¨èŒƒå›´é‡Œçš„ï¼Œä½†æ˜¯40åº¦æ˜¯éå¸¸ææ€–çš„å¤©æ°”ï¼Œä¸€å®šæ˜¯ç®—å¼‚å¸¸å€¼çš„ã€‚è¿™æ—¶å€™æˆ‘ä»¬å°±è¦ç»“åˆæˆ‘ä»¬çš„ä¸Šä¸‹æ–‡ï¼Œå³æˆ‘ä»¬æ˜¯åœ¨ä»€ä¹ˆ<strong>æ—¶é—´æ®µ</strong>ï¼Ÿä»€ä¹ˆ<strong>åœ°ç‚¹</strong>ï¼Ÿ ç­‰ç­‰ä¸Šä¸‹æ–‡å› ç´ å»è€ƒè™‘å¼‚å¸¸å€¼ã€‚</p></li>
<li><p>é›†ä½“å¼‚å¸¸ï¼ˆcollective outlierï¼‰ï¼šå•ä¸ªçš„æ•°æ®ä¸æ˜¯å¼‚å¸¸å€¼ï¼Œä½†æ˜¯å¤šä¸ªä¸ªä½“ä¸€èµ·å‡ºç°æ—¶ä¸€ç§å¼‚å¸¸ã€‚</p>
<p>æŸä¸€æˆ·å°åŒºæœ‰ä¸€æˆ·äººå®¶æ¬èµ°äº†ï¼Œä¸æ˜¯å¼‚å¸¸ã€‚åæˆ·äººå®¶ä¸€èµ·æ¬èµ°äº†ï¼Œæ˜¯ä¸€ä¸ªå¼‚å¸¸ã€‚</p>
<p>ä¸€å°ç”µè„‘æ‹’ç»å‘é€è¯·æ±‚ï¼Œå¯èƒ½çªç„¶failäº†ï¼Œä¸æ˜¯å¼‚å¸¸ã€‚åå°ç”µè„‘æ‹’ç»å‘é€è¯·æ±‚ï¼Œæ˜¯å¼‚å¸¸ï¼Œå¯èƒ½è¢«é»‘å®¢æ”»å‡»äº†ã€‚</p>
<p>FPXçš„5ä¸ªäººæ¯ä¸ªäººéƒ½ä¸æ˜¯å˜æ€å¼ºçš„é€‰æ‰‹ï¼Œä¸æ˜¯å¼‚å¸¸ã€‚5ä¸ªäººç»„åˆåœ¨ä¸€èµ·è´¼çŒ›ï¼Œæˆç¬¬ä¸€åäº†æ˜¯å¼‚å¸¸ã€‚</p>
<p>ä¸¾ä¾‹å­å®åœ¨å¤ªå¥½ç©äº†ã€‚</p></li>
</ol>
<h3 id="å¼‚å¸¸å€¼æ£€æµ‹çš„å„ç§æ–¹æ³•anomaly-detection">2. å¼‚å¸¸å€¼æ£€æµ‹çš„å„ç§æ–¹æ³•ï¼ˆAnomaly Detectionï¼‰</h3>
<h4 id="ç»Ÿè®¡æ£€éªŒstatistical-based">1.ç»Ÿè®¡æ£€éªŒï¼ˆStatistical-basedï¼‰</h4>
<ul>
<li>å‡è®¾è¿™ä¸ªæ•°æ®é›†æ˜¯æ ¹æ®ä¸€ç§éšæœºè¿‡ç¨‹ï¼ˆstochastic processï¼‰ äº§ç”Ÿçš„ã€‚å°±æ˜¯è¯´æœä»æŸç§æ¨¡å‹ï¼ˆä¾‹å¦‚æ­£å¤ªåˆ†å¸ƒï¼‰ã€‚</li>
<li>ç”¨å„ç§æ•°æ®æ‹Ÿåˆï¼ˆdata fittingï¼‰çš„æ–¹æ³•ï¼Œä¾‹å¦‚æœ€å°äºŒä¹˜ï¼Œæ¢¯åº¦ä¸‹é™ï¼ŒæŠŠè¿™ç§åˆ†å¸ƒè¡¨ç¤ºå‡ºæ¥ï¼Œç„¶åå¤„äº<strong>ä½æ¦‚ç‡</strong>çš„é‚£äº›æ•°æ®ç‚¹å°±æ˜¯å¼‚å¸¸å€¼ã€‚</li>
</ul>
<ol type="1">
<li><p>æœ‰å‚æ•°æ¨¡å‹</p>
<p>å‡è®¾è¿™ä¸ªæ•°æ®é›†æœä»æŸç§æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œä¾‹å¦‚æˆ‘ä»¬å‡è®¾æ•°æ®é›†æ˜¯æœä»æˆ‘ä»¬æœ€å–œæ¬¢çš„æ­£å¤ªåˆ†å¸ƒã€‚</p>
<p><img src="/images/image-20191014213158445.png"></p>
<p>å®ƒéœ€è¦å‚æ•°ï¼š<strong>å¹³å‡å€¼ï¼Œæ–¹å·®</strong></p>
<p>æ ¹æ®è¿™ä¸ªå›¾æˆ‘ä»¬çŸ¥é“ï¼Œå½“æˆ‘ä»¬çš„æ•°æ®æ˜¯åœ¨è¿™ä¸ªå›¾é‡Œå·¦å³2.5%çš„æ—¶å€™ï¼Œæ•°æ®æ˜¯å¼‚å¸¸å€¼ã€‚</p>
<p><strong>æ ç²¾ç¬‘é½</strong>ï¼šé‚£åˆ°åº•å’‹ç®—å•Šï¼Ÿ å‚æ•°å’‹ç®—å•Šï¼Ÿï¼Ÿè¿™ä¸ª2.5%åˆæ˜¯ä»€ä¹ˆä¸œè¥¿å•Šï¼Ÿï¼Ÿï¼Ÿ</p>
<p>æ­£å¤ªåˆ†å¸ƒçš„å…¬å¼æ˜¯è¿™æ ·çš„ï¼š <span class="math display">\[g(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}((x-\mu) / \sigma)^{2}}\]</span></p>
<p>å…¶ä¸­<span class="math inline">\(\mu\)</span>æ˜¯æˆ‘ä»¬æ•°æ®é›†çš„å¹³å‡å€¼ï¼Œ<span class="math inline">\(\sigma\)</span>æ˜¯æˆ‘ä»¬æ•°æ®é›†çš„æ–¹å·®ï¼Œè¿™2ä¸ªå°±æ˜¯æˆ‘ä»¬è¿™ä¸ªæ–¹æ³•éœ€è¦çš„æ±‚çš„å‚æ•°ï¼</p>
<p>2.5%ä¸ç”¨ç®¡ï¼Œæœ‰å…´è¶£çš„å¯ä»¥å»å­¦ä¹ ç»Ÿè®¡â€”â€”æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚</p>
<p>ä½†æ˜¯æ ¹æ®ç»éªŒè¡¨æ˜ï¼Œ<span class="math display">\[ \mu \pm 3\sigma\]</span> è¿™ä¸€æ®µæ•°æ®åœ¨æ­£å¤ªåˆ†å¸ƒé‡Œå¯ä»¥æ¶µç›–99.7%çš„æ•°æ®ã€‚</p>
<p>æ‰€ä»¥æˆ‘ä»¬å°±è®¡ç®—ï¼š<span class="math display">\[ \mu \pm 3\sigma\]</span> ï¼Œåªè¦æˆ‘ä»¬çš„å€¼ $ x &lt; - 3$ æˆ–è€… $ x &gt; + 3$ ,é‚£xå°±æ˜¯å¼‚å¸¸å€¼ï¼</p>
<p>è¿™é‡Œç›´æ¥æŠŠä¹¦ä¸­çš„ä¾‹å­æ¬è¿‡æ¥ï¼š</p>
<p>â€‹ æˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®é›†æ˜¯{<span class="math inline">\({24.0, 28.9, 28.9, 29.0, 29.1, 29.1, 29.2, 29.2, 29.3, 29.4}\)</span>}, å¹¶ä¸”æˆ‘ä»¬çŸ¥é“å®ƒæ˜¯æœä»æ­£å¤ªåˆ†å¸ƒçš„ã€‚ï¼ˆè¦å¥½å¤šå‡è®¾æ¡ä»¶ğŸ˜€ï¼‰</p>
<p>â€‹ æˆ‘ä»¬å°±èƒ½æ±‚å‡ºæ¥: <span class="math inline">\(\hat{\mu}=28.61 , \hat{\sigma}=\sqrt{2.29}=1.51\)</span></p>
<p>â€‹ é‚£<span class="math display">\[ \mu - 3\sigma = 24.08\]</span>,<span class="math display">\[ \mu + 3\sigma = 33.14\]</span></p>
<p>â€‹ å¤§åŠŸå‘Šæˆï¼Œæˆ‘ä»¬å‘ç°åªæœ‰ <strong>24</strong> æ˜¯åœ¨è¿™ä¸ªåŒºé—´å¤–é¢çš„ï¼Œæ‰€ä»¥<strong>24</strong>å°±æ˜¯è¿™ä¸ªæ•°æ®é›†çš„å¼‚å¸¸å€¼ï¼BingoğŸ˜‰~</p>
<p>æ€»ç»“ä¸€ä¸‹å“ˆï¼šè¿™ä¸ªæ–¹æ³•è¦æ±‚çš„æ¡ä»¶ä¹Ÿå¤ªå¤šäº†ã€‚</p>
<ul>
<li>ç¬¬ä¸€è¦æ±‚æ•°æ®é›†æœä»ä¸€ç§æ¨¡å‹ã€‚å®è·µä¸­æˆ‘ä»¬æ‹¿åˆ°ä¸€ä¸ªæ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†åˆä¸ä¼šå‘Šè¯‰æˆ‘ä»¬å®ƒæ˜¯å±äºä»€ä¹ˆæ¨¡å‹ï¼Œä»€ä¹ˆæ¦‚ç‡åˆ†å¸ƒçš„ã€‚</li>
<li>ç¬¬äºŒç»å¤§å¤šæ•°åªèƒ½åº”å¯¹ä¸€ç»´çš„æƒ…å†µï¼Œå°±åƒæˆ‘ä»¬çš„ä¾‹å­ã€‚ä½†æ˜¯å®é™…æ“ä½œæˆ‘ä»¬çš„æ•°æ®é›†ç‰¹å¾éƒ½æ˜¯å°‘è¯´ä¹Ÿæœ‰åå‡ ä¸ªå§ï¼ˆå¹´é¾„ï¼Œæ€§åˆ«ç­‰ç­‰ï¼‰ã€‚å¯¹äºä¸€ä¸ªé«˜ç»´çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æ˜¯å¾ˆéš¾å»ä¼°è®¡å®ƒæœä»ä»€ä¹ˆåˆ†å¸ƒçš„ã€‚ï¼ˆæˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼Œä½†æ˜¯è¿™å­¦æœŸçš„æ•°å­¦è¯¾æ¨äº†ä¸€ä¸ªäºŒç»´çš„æ­£å¤ªåˆ†å¸ƒå°±æŒºå¤æ‚äº†ï¼Œè€Œä¸”å·²ç»æ˜¯ä¸‰ç»´çš„å›¾äº†ã€‚æ‰€ä»¥è¿™é‡Œåªè®°äº†ç»“è®ºï¼‰</li>
</ul></li>
<li><p>æ— å‚æ•°æ¨¡å‹</p>
<p>æˆ‘ä»¬æƒ³ç”¨ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒå»è¡¨è¾¾ä¸€ä¸ªæ•°æ®é›†çš„ç›®çš„å…¶å®æ˜¯ä¸ºäº†æ‰¾åˆ°æ•°æ®é›†çš„<strong>æ­£å¸¸å€¼ï¼ˆnormal dataï¼‰</strong>ã€‚ä½†æ˜¯å…¶å®æˆ‘ä»¬å¯ä»¥é ç›®æµ‹ï¼ˆå¬ä¸Šå»æ›´åŠ ä¸é è°±ï¼‰ã€‚å°±æ˜¯æˆ‘ä»¬è¯†åˆ«è¿™ä¸ª<strong>æ­£å¸¸å€¼</strong>ä¸ç”¨ä¸€ä¸ªå…ˆè§„å®šå¥½çš„ç»“æ„/æ¨¡å‹ã€‚</p>
<p>é‚£æˆ‘ä»¬ç”¨ä»€ä¹ˆæ¥é¢„æµ‹ï¼Ÿï¼ˆéƒ½è½®ä¸åˆ°æ ç²¾ç¬‘é½æ¥æé—®çš„é—®é¢˜ï¼‰ ç”¨åœŸå‘³çš„histogramï¼ˆç›´æ–¹å›¾ï¼‰</p>
<p><img src="/images/image-20191014221044545.png"></p>
<p>å‡è®¾è¿™æ˜¯ä¸€å¼ äº¤æ˜“è®°å½•çš„ç›´æ–¹å›¾ï¼Œæˆ‘ä»¬è§„å®šäº†binï¼ˆç›´æ–¹æ ¼ï¼‰çš„é•¿åº¦æ˜¯1ã€‚ç„¶åå°±å‡ºæ¥äº†è¿™ä¸ªå›¾ã€‚ç„¶åæˆ‘ä»¬å‘ç°åªæœ‰0.2%çš„äº¤æ˜“æ˜¯å¤§äº5000çš„ã€‚é‚£æ‰€æœ‰åœ¨è¿™ä¸ªäº¤æ˜“è®°å½•é‡Œé¢å¤§äº5000çš„éƒ½æ˜¯<strong>å¼‚å¸¸å€¼</strong>äº†ã€‚å¥½ç®€å•å¥½çˆ½å¥½å¿«ä¹ã€‚</p>
<p><strong>æ ç²¾ç¬‘é½</strong>ï¼šé‚£è¿™ä¸ªbinå’‹ç®—ï¼Ÿï¼Ÿæˆ‘è¦æ˜¯ç®—binæ˜¯0.5ä¸€æ ¼çš„è¯ï¼Œé‚£æ˜¯ä¸æ˜¯æœ‰å¯èƒ½5500ä»¥ä¸Šçš„æ‰æ˜¯å¼‚å¸¸å€¼å•Šï¼Ÿé‚£æˆ‘å‰é¢å¥½ç®€å•å¥½çˆ½å¥½å¿«ä¹çš„æ–¹æ³•è¯´5100æ˜¯å¼‚å¸¸å€¼ï¼Œè¿™ä¸ªåˆè¯´5100ä¸æ˜¯å¼‚å¸¸å€¼ï¼Ÿä½ è¿™ä¸ªäººé ä¸é è°±å•Šï¼Ÿï¼Ÿ</p>
<p>æ‰€ä»¥è¿™ä¸ªæ–¹æ³•æˆ‘è§‰å¾—åº”è¯¥ä¹Ÿç®—æ˜¯æœ‰å‚æ•°çš„ï¼Œå‚æ•°å°±æ˜¯binçš„å–å€¼ã€‚</p>
<p>binå–å€¼å¸¦æ¥çš„é—®é¢˜ï¼š</p>
<ul>
<li><p>binå€¼å¤ªå¤§ï¼šé‚£å°±ä¼šè®©æœ‰çš„å¼‚å¸¸ç‚¹è·‘åˆ°æ­£å¸¸å€¼é‡Œé¢å»äº†ã€‚æƒ³è±¡ä¸€ä¸‹æˆ‘ä»¬åªæœ‰ä¸€ä¸ªbinï¼ˆè¿™ä¸ªbinå€¼å°±æ˜¯å…¨æ•°æ®é›†çš„é•¿åº¦ï¼‰ï¼Œè¿™ä¸ªbinçš„é¢‘ç‡æ˜¯100%ï¼Œå¼‚å¸¸å€¼éƒ½è·‘åˆ°biné‡Œé¢å»äº†ã€‚false negative</p></li>
<li><p>binå€¼å¤ªå°ï¼šé‚£æˆ‘ä»¬çš„æ­£å¸¸å€¼å¯èƒ½ä¼šè¢«åˆ¤æ–­æˆå¼‚å¸¸å€¼äº†ï¼Œæƒ³è±¡ä¸€ä¸‹æˆ‘ä»¬æœ‰Nä¸ªbinï¼ˆNå°±æ˜¯æˆ‘ä»¬æ•°æ®é›†çš„é•¿åº¦ï¼‰ï¼Œè¿™æ ·æˆ‘ä»¬æ¯ä¸ªæ•°æ®ç‚¹çš„é¢‘æ•°éƒ½æ˜¯1ï¼Œé¢‘ç‡éƒ½æ˜¯ä¸€æ ·ä¸”éå¸¸å°ï¼Œé‚£æ¯ä¸ªæ•°éƒ½æ˜¯å¼‚å¸¸å€¼äº†ã€‚false positive</p>
<p>è¿™é‡Œçš„false negativeå’Œfalse positiveæ˜¯ç¬¬ä¸€å¼ åˆ†ç±»æ··æ·†çŸ©é˜µï¼ˆconfusion matrixï¼‰çš„çŸ¥è¯†å†…å®¹ï¼Œè¢«æ··æ·†äº†çš„åŒå­¦è¯·å‰å¾€<a href="#æ··æ·†çŸ©é˜µï¼ˆconfusion%20matrixï¼‰">æ··æ·†çŸ©é˜µ</a>ã€‚</p></li>
</ul></li>
</ol>
<h4 id="æ¥è¿‘åº¦proximity-based">2. æ¥è¿‘åº¦ï¼ˆProximity-basedï¼‰</h4>
<h5 id="åŸºäºè·ç¦»distance">1. åŸºäºè·ç¦»ï¼ˆdistanceï¼‰</h5>
<p>ç›´æ¥ç»™å‡ºå…¬å¼</p>
<p><span class="math display">\[ {\frac{\left\|\left\{o^{\prime} | \operatorname{dist}\left(o, o^{\prime}\right) \leq r\right\}\right\|}{\|D\|} \leq \pi}\]</span></p>
<p>å¥½æ‡µé€¼ğŸ¤£ï¼Œæ²¡äº‹ã€‚</p>
<p>åŸç†å°±æ˜¯</p>
<ol type="1">
<li>ä½ è‡ªå·±å®šä¹‰ä¸€ä¸ªè·ç¦»ã€‚</li>
<li>ç„¶åè®¡ç®—æ‰€æœ‰æ•°æ®ç‚¹å’Œç°åœ¨åœ¨åˆ¤æ–­æ˜¯ä¸æ˜¯å¼‚å¸¸å€¼çš„è¿™ä¸ªæ•°æ®ç‚¹çš„è·ç¦»ã€‚</li>
<li>æ•°å‡ºæœ‰å¤šå°‘ä¸ªç‚¹æ˜¯åœ¨ä½ è‡ªå·±å®šä¹‰çš„è·ç¦»é‡Œé¢çš„ï¼Œå°±æ˜¯<strong>æ­¥éª¤2</strong>çš„è·ç¦»å°äº<strong>æ­¥éª¤1</strong>çš„è·ç¦»ã€‚</li>
<li>æŠŠ<strong>æ­¥éª¤3</strong>æ•°å‡ºæ¥çš„æ•°é™¤ä»¥æ•°æ®é›†çš„é•¿åº¦ï¼ˆæœ‰å¤šå°‘ä¸ªæ•°æ®ç‚¹ï¼‰ï¼Œå¦‚æœå°äºä¸€å®šçš„æ¯”ä¾‹ï¼Œå°±æ˜¯å¼‚å¸¸å€¼ã€‚</li>
</ol>
<p>è¿˜æ˜¯å¥½æ‡µé€¼ğŸ¤£</p>
<p>é¦–å…ˆ <span class="math inline">\(\pi\)</span>å’Œ<span class="math inline">\(r\)</span> æ˜¯æˆ‘ä»¬è‡ªå·±è®¾è®¡çš„å€¼ã€‚<span class="math inline">\(r\)</span> å°±æ˜¯<strong>æ­¥éª¤1</strong>çš„è·ç¦»ã€‚$ $ ä¸æ˜¯é‚£ä¸ª3.1415â€¦â€¦ï¼Œæ˜¯<strong>æ­¥éª¤4</strong>çš„é‚£ä¸ªæ¯”ä¾‹ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæˆ‘ä»¬è®¾è®¡çš„0åˆ°1ä¹‹é—´åˆ°ç™¾åˆ†æ•°ã€‚<span class="math inline">\(o^{\prime}\)</span> ä»£è¡¨æ‰€æœ‰å…¶ä»–æ•°æ®ç‚¹ã€‚</p>
<p>$ {O^{} | (O, O^{}) r}$ ä»£è¡¨<strong>æ­¥éª¤2</strong> ï¼Œ<span class="math inline">\(\|D\|\)</span> å’Œåˆ†å­çš„ ||â€¦|| éƒ½ä»£è¡¨cardinalityï¼ˆåŸºæ•°ï¼‰ï¼Œå°±æ˜¯æœ‰å¤šå°‘ä¸ªï¼Œ<strong>æ­¥éª¤3</strong>å’Œ<strong>æ­¥éª¤4</strong></p>
<p>åº”è¯¥æ‡‚äº†ï¼Œåœ¨åŠ æ·±ä¸€ä¸‹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å®šä¹‰ <span class="math inline">\(k=\pi\|D\|\)</span>, é‚£kå…¶å®ä¹Ÿä»£è¡¨ä¸€ä¸ªthresholdï¼ˆé˜ˆï¼ˆyuï¼‰å€¼ï¼‰ï¼Œå®ƒçš„æ„ä¹‰æ˜¯å¦‚æœä½ <strong>æ­¥éª¤3</strong>æ•°å‡ºæ¥çš„åŸºæ•°ï¼ˆä¸ªæ•°ï¼‰å°‘äº <span class="math inline">\(k\)</span> çš„è¯ï¼Œè¿™ä¸ªå€¼å°±æ˜¯å¼‚å¸¸å€¼ã€‚</p>
<p>ç°åœ¨è€ƒè™‘ä¸€ä¸‹ä¼˜åŒ–æ–¹æ³•ï¼Œä½œä¸ºä¸€ä¸ªæœªæ¥çš„æ•°æ®ç§‘å­¦å®¶ï¼Œç»å¸¸å¤„ç†ç‰¹åˆ«å¤§çš„æ•°æ®é›†ï¼Œä¸å­¦ä¼˜åŒ–ç­‰äºæ¯ä¸€æ¬¡è°ƒè¯•éƒ½è¦ç­‰å¾ˆä¹…<del>æˆ–è€…æƒ³è¦ä¹°ä¸€å°æ–°çš„ç”µè„‘</del></p>
<p>ä¼˜åŒ–ï¼š å¦‚æœ<strong>æ­¥éª¤3</strong>å·²ç»æ•°åˆ°äº†<span class="math inline">\(k\)</span>ä¸ªå€¼ï¼Œå°±åœæ­¢å¾ªç¯ã€‚ï¼ˆæ²¡å¿…è¦ç»§ç»­ç»§ç»­ä¸‹å»äº†ï¼Œè¿™ä¸ªæ•°æ®é›†å·²ç»è¯æ˜è‡ªå·±ä¸æ˜¯å¼‚å¸¸å€¼äº†ï¼Œæˆ‘ä»¬ä¸éœ€è¦çŸ¥é“å®ƒæœ‰å¤šé è¿‘æ•°æ®é›†çš„ä¸­å¿ƒï¼‰</p>
<h5 id="åŸºäºå¯†åº¦density-based-outlier-detection">2. åŸºäºå¯†åº¦ï¼ˆDensity-Based Outlier Detectionï¼‰</h5>
<p>ä¸åƒä¸Šä¸€ç§æ–¹æ³•ç”¨ä¸€ä¸ªæ•°æ®å€¼å’Œæ‰€æœ‰å…¶ä»–æ•°æ®å€¼æ¯”è¾ƒï¼Œæˆ‘ä»¬åªæ¯”è¾ƒå’Œå®ƒç›¸é‚»çš„æ•°æ®ã€‚</p>
<p><img src="/images/image-20191015101212861.png"></p>
<p>å¦‚å›¾ï¼Œæ ¹æ®ä¸é è°±çš„ç›®æµ‹ï¼Œæˆ‘ä»¬è§‰å¾—</p>
<blockquote>
<p><span class="math inline">\(O_1,O_2\)</span>æ˜¯<span class="math inline">\(C_1\)</span>çš„å¼‚å¸¸å€¼ï¼Œ<span class="math inline">\(O_1,O_2\)</span>å°±æ˜¯<strong>å±€éƒ¨å¼‚å¸¸å€¼</strong>ï¼ˆåªé’ˆå¯¹<span class="math inline">\(C_1\)</span>ï¼‰</p>
<p><span class="math inline">\(O_3\)</span>æ˜¯<strong>å…¨å±€å¼‚å¸¸å€¼</strong></p>
<p><span class="math inline">\(O_4\)</span>æ˜¯æ­£å¸¸å€¼ï¼Œä¸æ˜¯å¼‚å¸¸å€¼ã€‚</p>
</blockquote>
<p>ä½†æ˜¯ï¼Œè¯•æƒ³ä¸€ä¸‹æˆ‘ä»¬ç”¨å‰é¢çš„ä¸€ç§æ£€æµ‹æ–¹æ³•ï¼ˆåŸºäºè·ç¦»ï¼‰ï¼Œæˆ‘ä»¬æ˜¯æ‰¾ä¸åˆ°<span class="math inline">\(O_1,O_2\)</span>æ˜¯å¼‚å¸¸å€¼çš„ã€‚ï¼ˆå› ä¸ºå®ƒä»¬ç¦»<span class="math inline">\(C_1\)</span>å…¶å®æŒºè¿‘çš„ã€‚ï¼‰</p>
<p>ä½†æ˜¯åŸºäºå¯†åº¦çš„æ–¹æ³•æˆ‘ä»¬æ˜¯å¯ä»¥æ‰¾çš„ã€‚</p>
<p>è¿™ä¸ªæ–¹æ³•è¯¾ä»¶é‡Œé¢è¯´æ˜¯ç”¨æœ€ä½çš„LOFå€¼ï¼Œtutorialé‡Œè¯´ç”¨æœ€é«˜çš„LOFå€¼æ˜¯å¼‚å¸¸å€¼ï¼ŒæŠŠæˆ‘ææ··äº†å¾ˆä¹…ï¼Œè€å¸ˆå›å¤é‚®ä»¶åææ‡‚ã€‚</p>
<p><img src="/images/1920px-LOF-idea.svg.png"></p>
<p>åŸºæœ¬æ€æƒ³ï¼šæ¯”å¦‚æˆ‘ä»¬æ£€æµ‹Aæ˜¯ä¸æ˜¯ä¸€ä¸ªå¼‚å¸¸å€¼ï¼Œæˆ‘ä»¬æ˜¯æ¯”è¾ƒ<strong>Açš„å¯†åº¦å’ŒAå¯†åº¦èŒƒå›´é‡Œé¢å…¶ä»–ç‚¹çš„å¯†åº¦çš„å¹³å‡å€¼</strong>ã€‚</p>
<p>æ€ä¹ˆæ¯”å‘¢ï¼Ÿå°±æ˜¯ç”¨å®ƒä»¬çš„æ¯”å€¼ï¼Œå®ƒæœ‰ä¸€ä¸ªç‰¹åˆ«çš„åå­—ï¼ˆLOF: Local outlier factor)ã€‚</p>
<p>è¿™é‡Œåˆ†å­æ˜¯èŒƒå›´å†…ç‚¹ä»¬å¯†åº¦å¹³å‡å€¼ï¼Œåˆ†æ¯æ˜¯Açš„å¯†åº¦</p>
<ol type="1">
<li>å¦‚æœè¿™ä¸ªæ¯”å€¼æ¥è¿‘1: è¯´æ˜Açš„å¯†åº¦å’Œå®ƒèŒƒå›´å†…çš„ç‚¹ä»¬çš„å¯†åº¦éƒ½å·®ä¸å¤šï¼Œé‚£å°±è¯´æ˜ä¸æ˜¯ç¦»ç¾¤ç‚¹äº†ã€‚</li>
<li>å¦‚æœè¿™ä¸ªæ¯”å€¼å°äº1: è¯´æ˜Açš„å¯†åº¦æ¯”èŒƒå›´å†…çš„ç‚¹ä»¬å¤§ï¼ŒAæ›´é è¿‘å®ƒä»¬ã€‚</li>
<li>å¦‚æœè¿™ä¸ªæ¯”å€¼å¤§äº1: è¯´æ˜Açš„å¯†åº¦æ¯”èŒƒå›´å†…çš„ç‚¹ä»¬çš„å¯†åº¦å¹³å‡å€¼å°ï¼ŒAæ˜¯å¼‚å¸¸å€¼ã€‚</li>
</ol>
<p>æ ç²¾ç¬‘é½ï¼š</p>
<ol type="1">
<li><p>è¿™ä¸ªèŒƒå›´æ˜¯æ€ä¹ˆå®šä¹‰çš„ï¼Ÿä¸ºä»€ä¹ˆå›¾é‡Œé¢Açš„èŒƒå›´é‚£ä¹ˆå¤§ï¼Œåœ¨AèŒƒå›´é‡Œçš„é‚£3ä¸ªç‚¹çš„èŒƒå›´é‚£ä¹ˆå°ï¼Ÿ</p></li>
<li><p>è¿™ä¸ªå¯†åº¦æ˜¯ä»å“ªé‡Œå†’å‡ºæ¥çš„ï¼Œå’‹ç®—å•Šï¼Ÿ</p></li>
</ol>
<p>èŒƒå›´ï¼šç¦»è¦æ£€æµ‹çš„ç‚¹æœ€è¿‘çš„ç¬¬Kä¸ªç‚¹çš„è·ç¦»æ˜¯è¿™ä¸ªæ£€æµ‹çš„èŒƒå›´ã€‚</p>
<p>â€‹ å›¾ä¸­æˆ‘ä»¬é€‰çš„<span class="math inline">\(K=3\)</span>, æ‰€ä»¥Aé‡Œé¢åŒ…å«äº†3ä¸ªç‚¹ï¼Œå…¶ä»–çš„ç‚¹çš„èŒƒå›´ä¹Ÿéƒ½æ˜¯3ä¸ªç‚¹ã€‚</p>
<p>â€‹ è¿™æ ·å°±é¿å…äº†æˆ‘ä»¬åŸºäºè·ç¦»æ–¹æ³•çš„é—®é¢˜ï¼Œæˆ‘ä»¬ç°åœ¨å®šä¹‰çš„è·ç¦»æ˜¯åŠ¨æ€çš„ï¼Œæ˜¯èƒ½æ ¹æ®ä¸åŒçš„ç‚¹æ¥å˜ æ¢çš„ã€‚</p>
<p>å¯†åº¦ï¼šå…¶å®æ›´å…·ä½“çš„åè¯æ˜¯<strong>å±€éƒ¨å¯è¾¾æ€§å¯†åº¦ï¼ˆlrdï¼‰</strong></p>
<p>â€‹ <del>å¯†åº¦ç­‰äºè´¨é‡é™¤ä»¥ä½“ç§¯</del> â€”â€” ä¸æ˜¯ä½ æƒ³çš„é‚£æ ·ã€‚ ä½†æ˜¯æ ¸å¿ƒæ€æƒ³å·®ä¸å¤šå•¦ï¼Œå°±æ˜¯æˆ‘ä»¬æƒ³çŸ¥é“åœ¨è¿™ä¸ªæ£€æµ‹ç‚¹å¯†ä¸ å¯†é›†ã€‚ä½†æ˜¯è¿™ä¸ªäº‹æƒ…æ˜¯éå¸¸éš¾ç”¨æ•°å­¦è¡¨è¾¾å‡ºæ¥çš„ï¼ˆå¯¹æˆ‘ä»¬å­¦æ¸£è¿˜éå¸¸éš¾ç†è§£ğŸ˜±è¿™ä¹ˆå¤šæ•°å­¦ç¬¦å·æ˜¯ä»€ä¹ˆé¬¼ å•¦ï¼Œä¸ºäº†è®©å¤§å®¶æŸ¥é˜…å…¶ä»–èµ„æ–™çš„æ—¶å€™ç¬¦å·ä¸€æ ·ï¼Œä¸‹é¢æˆ‘ä¹Ÿè¢«è¿«ä½¿ç”¨è¿™ä¹ˆå¤šä¸æƒ³çœ‹æ˜ç™½çš„æ•°å­¦ç¬¦å·ï¼‰ã€‚</p>
<p>â€‹ è´¨é‡ï¼šæ˜¯æˆ‘ä»¬çš„è§„å®šçš„Kã€‚å›¾ä¸­çš„ä¾‹å­å°±æ˜¯ <span class="math inline">\(3\)</span>.</p>
<p>â€‹ å¾ˆå¤šç”¨<span class="math inline">\(|N_{k}(A)|\)</span>è¡¨ç¤ºï¼Œå…¶ä¸­<span class="math inline">\(N_{k}(A)\)</span> å°±æ˜¯è¢«Açš„èŒƒå›´åŒ…å«çš„ç‚¹ä»¬çš„é›†åˆã€‚<span class="math inline">\(|x|\)</span>ä»£è¡¨xçš„åŸºæ•°ï¼ˆä¸ªæ•°ï¼‰ã€‚</p>
<p>â€‹ æ˜æ˜å°±æ˜¯ Kå˜›ï¼Œä¸ºä»€ä¹ˆè¦è¿™ä¹ˆå¤æ‚ã€‚ã€‚ã€‚è¦æ˜¯æˆ‘ç†è§£é”™äº†è¯·å‘Šè¯‰æˆ‘ã€‚</p>
<p>â€‹ ä¸ºä»€ä¹ˆè¦ç”¨Kå‘¢ï¼Œå› ä¸ºè¿™æ ·æˆ‘ä»¬æ‰€æœ‰çš„ç‚¹çš„åˆ†å­éƒ½æ˜¯ä¸€æ ·çš„äº†ï¼Œçœ‹å®ƒä»¬å¯†ä¸å¯†é›†ï¼Œå°±çœ‹åˆ†æ¯äº†ã€‚</p>
<p>â€‹ ä½“ç§¯ï¼š æ˜¯æˆ‘ä»¬<span class="math inline">\(N_{k}(A)\)</span>é‡Œæ‰€æœ‰ç‚¹åˆ°Açš„è·ç¦»å’Œã€‚</p>
<p>â€‹ ç„¶è€Œè·ç¦»ä¸æ˜¯å•çº¯çš„è·ç¦»ï¼Œæœ‰ä¸€ä¸ªæ–°çš„è·ç¦»å®šä¹‰æ–¹å¼å«<strong>å¯è¾¾è·ç¦»</strong></p>
<p>â€‹ reachability-distance <span class="math inline">\(r-d(A,C) = max(k-distance(C),d(A,C))\)</span></p>
<p>â€‹ <span class="math inline">\(k-distance(C)\)</span>æ˜¯Cçš„èŒƒå›´ã€‚</p>
<p>â€‹ ä¹Ÿå°±æ˜¯å½“æˆ‘ä»¬ç®—AèŒƒå›´é‡Œå…¶ä»–ç‚¹ï¼ˆä¾‹å¦‚Cï¼‰å¯¹Açš„è·ç¦»çš„æ—¶å€™ï¼Œè¿™ä¸ªè·ç¦»<strong>è‡³å°‘å¾—æ˜¯Cçš„èŒƒå›´</strong>ã€‚</p>
<p>â€‹ ï¼ˆå…¶ä¸­ç†è®ºæˆ‘ä¹Ÿæ²¡æ‡‚ï¼Œæ±‚å¤§ç¥è§£é‡Šï¼‰</p>
<p>â€‹ å¯†åº¦è®¡ç®—å…¬å¼ï¼š</p>
<p>â€‹ <span class="math inline">\(\operatorname{lrd}_{k}(A):= \frac {|N_{k}(A)|}{\sum_{B \in N_{k}(A)} {reachability-distance}_{k}(A, B)}\)</span></p>
<p>â€‹ åˆ†å­å°±æ˜¯æˆ‘ä»¬è¯´çš„è´¨é‡å˜›ï¼Œåˆ†æ¯å°±æ˜¯æˆ‘ä»¬è¯´çš„ä½“ç§¯å˜›ã€‚</p>
<p>ç»ˆäºåˆ°LOFäº†ï¼Œç°åœ¨æˆ‘ä»¬å†æ¥çœ‹æˆ‘ä»¬çš„åŸºæœ¬æ€æƒ³ï¼šæ¯”è¾ƒ<strong>Açš„å¯†åº¦å’ŒAå¯†åº¦èŒƒå›´é‡Œé¢å…¶ä»–ç‚¹çš„å¯†åº¦çš„å¹³å‡å€¼</strong>ã€‚</p>
<p>æˆ‘ä»¬å·²ç»ä¼šæ±‚å¯†åº¦äº†ï¼Œç›´æ¥çœ‹å…¬å¼ï¼š</p>
<p>â€‹ <span class="math inline">\(\operatorname{LOF}_{k}(A) = \frac{\sum_{B \in N_{k}(A)} \operatorname{lrd}(B)}{\left|N_{k}(A)\right|} / \operatorname{lrd}(A)\)</span></p>
<p>å·¦åŠéƒ¨åˆ†<strong>Aå¯†åº¦èŒƒå›´é‡Œé¢å…¶ä»–ç‚¹çš„å¯†åº¦çš„å¹³å‡å€¼</strong>ï¼Œå³åŠéƒ¨åˆ†<strong>Açš„å¯†åº¦</strong></p>
<p>ç°åœ¨ä¹Ÿå¾ˆå¥½ç†è§£ä¸ºä»€ä¹ˆtutorialé‡Œé¢è¯´é«˜çš„LOFå€¼æ˜¯å¼‚å¸¸å€¼ï¼Œè¯¾ä»¶é‡Œé¢è¯´ä½çš„LOFå€¼æ˜¯å¼‚å¸¸å€¼äº†ã€‚</p>
<p>å› ä¸ºå®ƒä»¬çš„æ¯”å€¼åˆ†å­åˆ†æ¯åäº†ã€‚ã€‚ã€‚åˆ†å­åˆ†æ¯åäº†ã€‚ã€‚ã€‚åäº†ã€‚ã€‚ã€‚</p>
<p>ä¸ºäº†åº”ä»˜è€ƒè¯•çš„æ­¥éª¤æµç¨‹å›¾ï¼š</p>
<ol type="1">
<li>æ ¹æ®é¢˜ç›®ç»™çš„Kå€¼å’Œå®šä¹‰è·ç¦»çš„æ–¹æ³•ï¼ˆæ›¼å“ˆé¡¿ï¼Œæ¬§å‡ é‡Œå¾—ï¼‰æ‰¾å‡ºæ¯ä¸ªç‚¹çš„k-distanceï¼ˆèŒƒå›´ï¼‰</li>
<li>æ‰¾å‡ºæ¯ä¸ªç‚¹é‡Œé¢k-distanceï¼ˆèŒƒå›´ï¼‰é‡Œé¢çš„å…¶ä»–ç‚¹</li>
<li>è®¡ç®—æ‰€æœ‰ç‚¹çš„å¯†åº¦</li>
<li>è®¡ç®—æ¯ä¸ªç‚¹çš„LOF</li>
<li>çœ‹ä½ æ€ä¹ˆæ¯”å†³å®šå–æœ€å¤§å€¼è¿˜æ˜¯æœ€å°å€¼ä½œä¸ºå¼‚å¸¸å€¼ã€‚</li>
</ol>
<p>ï¼ˆç»ˆäºå®Œäº†ï¼Œå”¯ä¸€ä¸€ä¸ªç–‘æƒ‘çš„å°±æ˜¯<strong>å¯è¾¾è·ç¦»</strong>çš„å«ä¹‰äº†ï¼‰</p>
<p>æ€»ç»“ï¼š</p>
<blockquote>
<p>åŸºäºè·ç¦»ï¼šå¼‚å¸¸å€¼çš„é™„è¿‘ä¸å¤Ÿæ»¡è¶³ä½ è¦æ±‚çš„ç‚¹çš„æ•°é‡ã€‚</p>
<p>åŸºäºå¯†åº¦ï¼šå¼‚å¸¸å€¼çš„å¯†åº¦ä¸å¤Ÿæ»¡è¶³ä½ è¦æ±‚çš„å¯†åº¦ã€‚</p>
</blockquote>
<h4 id="èšç±»æ£€éªŒcluster-based">3. èšç±»æ£€éªŒï¼ˆcluster-basedï¼‰</h4>
<p><img src="/images/image-20191015110704026.png"></p>
<p>çœ‹å›¾å°±æ˜ç™½äº†ï¼ŒæŠŠæ•°æ®é›†ç”¨æŸç§èšç±»æ–¹æ³•èšç±»ã€‚</p>
<blockquote>
<p>1.è¿™ä¸ªç‚¹åˆ°å±äºå®ƒçš„ç±»çš„ç±»ä¸­å¿ƒçš„è·ç¦»éå¸¸çš„é•¿</p>
<p>2.åŒ…å«å¾ˆå°æ•°é‡çš„èšç±»éƒ½æ˜¯<strong>å¼‚å¸¸å€¼</strong>ã€‚</p>
</blockquote>
<p>ç¬¬ä¸€ç§æƒ…å†µï¼š</p>
<p>â€‹ é‚£æˆ‘ä»¬å°±è®¡ç®—è¿™ä¸ªç‚¹<span class="math inline">\(o\)</span>åˆ°ç±»ä¸­å¿ƒ<span class="math inline">\(c\)</span>åˆ°è·ç¦»<span class="math inline">\(d(o,c)\)</span>,å’Œå…¶ä»–å±äºè¿™ä¸ªç±»çš„ç‚¹ä»¬<span class="math inline">\(o_i\)</span>åˆ°ç±»ä¸­å¿ƒ<span class="math inline">\(c\)</span>çš„è·ç¦»å¹³å‡å€¼<span class="math inline">\(d(o_i,c)_{avg}\)</span>,ç„¶åè®¡ç®—å®ƒä»¬çš„æ¯”å€¼ï¼Œå¦‚æœè¿™ä¸ªæ¯”å€¼å¾ˆå¤§ï¼Œè¯´æ˜è¿™ä¸ªç‚¹<span class="math inline">\(o\)</span>å°±æ˜¯å¼‚å¸¸å€¼ã€‚ ç”¨æ¯”å€¼çš„åŸå› å°±æ˜¯ç»™æˆ‘ä»¬ä¸€ä¸ªå…³äºè¿™ä¸ªç±»æœ¬èº«å¤§å°çš„åˆ¤æ–­ä¾æ®ï¼Œä¸ç„¶æœ‰çš„ç±»å¾ˆå¤§ï¼Œæœ‰çš„ç±»å¾ˆå°ã€‚å¦‚æœåªæ ¹æ®è·ç¦»å®šä¹‰é˜ˆå€¼ä¼šå‡ºç°é”™è¯¯ã€‚</p>
<p>ç¬¬äºŒç§æƒ…å†µï¼š</p>
<p>â€‹ å‡è®¾æœ‰ä¸€ä¸ªæ•°æ®ç‚¹<span class="math inline">\(p\)</span></p>
<p>â€‹ å¦‚æœ<span class="math inline">\(p\)</span>åœ¨ä¸€ä¸ªæ¯”è¾ƒå¤§çš„èšç±»<span class="math inline">\(c_1\)</span>é‡Œé¢ï¼š</p>
<p>â€‹ <span class="math inline">\(CBLOF = c_1çš„size \times på’Œc_1çš„ç›¸ä¼¼åº¦\)</span></p>
<p>â€‹ å¦‚æœ<span class="math inline">\(p\)</span>åœ¨ä¸€ä¸ªæ¯”è¾ƒå°çš„èšç±»<span class="math inline">\(c_2\)</span>é‡Œé¢ï¼š</p>
<p>â€‹ <span class="math inline">\(CBLOF = c_2çš„size \times på’Œç¦»å®ƒæœ€è¿‘çš„å¤§èšç±»çš„ç›¸ä¼¼åº¦\)</span></p>
<p>ç›¸ä¼¼åº¦æœ‰å¾ˆå¤šç§ä¸åŒçš„æ–¹å¼ã€‚CBLOFæ¯”è¾ƒå°çš„å€¼å°±æ˜¯å¼‚å¸¸å€¼ã€‚ ï¼ˆ2ä¸ªå‚æ•°ï¼Œç›¸ä¼¼åº¦ä½æˆ–è€…ç±»æ¯”è¾ƒå°ï¼Œå°±ä»£è¡¨ç¦»ç¾¤äº†ï¼‰</p>
<hr>
<h2 id="vi.-recommender-systems">VI. Recommender Systems</h2>
<h3 id="user-based">1. User-based</h3>
<p><img src="/images/image-20191108094208703.png"></p>
<h4 id="pearson-correlation">Pearson correlation</h4>
<p>ä½¿ç”¨pearsonç›¸å…³ç³»æ•°æ¥ä½œä¸ºç›¸ä¼¼åº¦çš„è€ƒé‡</p>
<p><img src="/images/image-20191108094303685.png"></p>
<p>å…¶å®å’Œæˆ‘ä»¬ä»‹ç»è¿‡çš„<a href="#Cosine%20Similarity">cosine similarity</a>éå¸¸ç›¸ä¼¼ï¼š</p>
<p>åœ¨cosine similarityä¸­ï¼š$ (a, b)=$</p>
<p>ä¸Šé¢å°±æ˜¯ä¸¤æ¡å‘é‡çš„ç‚¹ç§¯ï¼Œä¸‹é¢æ˜¯å„è‡ªå‘é‡çš„æ¨¡</p>
<p>è€Œpearsonè¿™ä¸ªäººå°±æ˜¯æŠŠcosine similarityåšäº†ä¸ªä¸­å¿ƒæ ‡å‡†åŒ–ï¼ˆæ¯ä¸€é¡¹éƒ½å‡å»å¹³å‡å€¼ï¼‰ï¼Œå°±åƒæ­£å¤ªåˆ†å¸ƒå˜æˆæ ‡å‡†æ­£æ€åˆ†å¸ƒä¸€æ ·ã€‚</p>
<p>ä¾‹å­ï¼šè®¡ç®—Aliceå¯¹äºâ€œItem5â€å¯èƒ½çš„è¯„åˆ†ï¼ˆå…´è¶£ï¼‰æ˜¯å¤šå°‘ï¼Ÿ</p>
<p>æ€è·¯ï¼š</p>
<ol type="1">
<li>è®¡ç®—Aliceå’Œæ‰€æœ‰å…¶ä»–userçš„similarity</li>
<li>æ ¹æ®ä½ é€‰ç”¨çš„neighborç­–ç•¥ï¼ˆé€‰kä¸ªç›¸å…³çš„userä½œä¸ºå‚è€ƒï¼‰é€‰ç›¸ä¼¼åº¦æœ€é«˜çš„kä¸ªuser</li>
<li>æ ¹æ®é‚£kä¸ªuesrçš„item5è¯„åˆ†ï¼Œè®¡ç®—<span class="math inline">\(\operatorname{pred}(\boldsymbol{a}, \boldsymbol{p})=\overline{\boldsymbol{r}_{a}}+\frac{\sum_{b \in N} \operatorname{sim}(\boldsymbol{a}, \boldsymbol{b}) *\left(\boldsymbol{r}_{b, p}-\overline{\boldsymbol{r}}_{b}\right)}{\sum_{b \in N} \operatorname{sim}(\boldsymbol{a}, \boldsymbol{b})}\)</span></li>
</ol>
<p>è®¡ç®—ï¼š</p>
<ol type="1">
<li><span class="math inline">\(\hat{Alice}=4,\hat{user1}=2.25\)</span>, <strong>æ³¨æ„è¿™é‡Œuser1çš„å¹³å‡å€¼ä¸åŒ…æ‹¬item5ï¼Œåé¢çš„è®¡ç®—ä¹Ÿä¸åŒ…æ‹¬ï¼Œitem5çš„å€¼ä»…åœ¨é¢„æµ‹é‚£ä¸€æ­¥ä½¿ç”¨ï¼‰</strong>ï¼Œ<span class="math inline">\(sim(Alice,user1) = \frac{(5-4)(3-2.25)+(3-4)(1-2.25)+(4-4)(2-2.25)+(4-4)(3-2.25)}{\sqrt{(5-4)^2+(3-4)^2}\times \sqrt{(3-2.25)^2+(1-2.25)^2+(2-2.25)^2+(3-2.25)^2}}= 0.85\)</span></li>
<li>åŒç†å¯å¾—<span class="math inline">\(sim(Alice,user2) = 0.7\)</span>, <span class="math inline">\(sim(Alice,user3) = 0\)</span>, <span class="math inline">\(sim(Alice,user4) = -0.79\)</span></li>
<li>æˆ‘ä»¬é€‰ç”¨2-nearest neighbours: user1å’Œuser2</li>
<li><span class="math inline">\(Pred(Alice,item5) = \hat{Alice} + \frac{sim(Alice,user1)*(user1.item5-\hat{user1}+sim(Alice,user2)*(user2.item5-\hat{user2})}{sim(Alice,user1)+sim(Alice,user2)} = 4+\frac{0.85\times (3-2.25)+0.7\times(5-3.5)}{0.85+0.7} = 5.08\)</span></li>
</ol>
<h4 id="peason-correlationçš„ç¼ºç‚¹">Peason correlationçš„ç¼ºç‚¹</h4>
<ol type="1">
<li>å¹¶éæ‰€æœ‰é‚»å±…çš„è¯„åˆ†éƒ½åŒæ ·â€œæœ‰ä»·å€¼â€ â€“å°±æ™®éå–œæ¬¢çš„å•†å“è¾¾æˆåè®®å¹¶æ²¡æœ‰åƒå¯¹æœ‰äº‰è®®çš„å•†å“è¾¾æˆåè®®é‚£æ ·å†…å®¹ä¸°å¯Œ â€“å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼šå¯¹å…·æœ‰è¾ƒå¤§å·®å¼‚çš„é¡¹ç›®ç»™äºˆæ›´å¤šæƒé‡</li>
<li>å…±åŒé¡¹ç›®æ•°çš„å€¼ â€“åŒæ—¶è€ƒè™‘çš®å°”é€Šç›¸ä¼¼æ€§å’Œå…±åŒè¯„å®šé¡¹ç›®çš„æ•°é‡</li>
<li>æ¡ˆä¾‹æ”¾å¤§ â€“ç›´è§‰ï¼šç»™äºˆâ€œéå¸¸ç›¸ä¼¼â€çš„é‚»å±…æ›´å¤šçš„æƒé‡ï¼Œå³ç›¸ä¼¼åº¦å€¼æ¥è¿‘1çš„é‚»å±…ã€‚</li>
<li>neighborhoodé€‰æ‹© â€“ä½¿ç”¨ç›¸ä¼¼æ€§é˜ˆå€¼æˆ–å›ºå®šæ•°ç›®çš„é‚»å±…</li>
</ol>
<h3 id="item-based">2. Item-based</h3>
<p><img src="/images/image-20191108102944155.png"></p>
<p>ç®€å•çš„è¯´å°±æ˜¯user-basedå°±ç”¨userå‘é‡ï¼ˆè¡Œå‘é‡ï¼‰ï¼Œitem-basedå°±æ˜¯ç”¨itemå‘é‡ï¼ˆåˆ—å‘é‡ï¼‰ã€‚</p>
<h3 id="latent-factor-modellfm">3.Latent Factor Model(LFM)</h3>
<p>æˆ‘ä»¬å°†ä¼ ç»Ÿçš„ç”¨æˆ·/å•†å“è¡¨æ ¼ï¼Œè½¬å˜ä¸ºâ€œå•†å“-å•†å“å±æ€§â€ä¸â€œå•†å“å±æ€§-ç”¨æˆ·å–œå¥½â€2ä¸ªè¡¨æ ¼</p>
<p>å³å¦‚ä¸‹æ ¼å¼ï¼š<span class="math inline">\(A_{m \times n}=U_{m \times k} V_{K \times n}\)</span></p>
<p>æˆ‘ä»¬æœ‰å¦‚ä¸‹åŸå§‹è¡¨æ ¼ï¼ˆ0è¡¨ç¤ºæ²¡æœ‰è¯„ä»·ï¼Œéœ€è¦æˆ‘ä»¬é¢„æµ‹ï¼‰</p>
<p><img src="/images/image-20191108110747662.png"></p>
<p>æˆ‘ä»¬æƒ³å¾—åˆ°çŸ©é˜µUå’ŒçŸ©é˜µVï¼Œå°±æ˜¯æ„é€ å‡½æ•°ç„¶åä½¿loss functionï¼ˆæŸå¤±å‡½æ•°ï¼‰æœ€å°å³å…¬å¼ï¼š</p>
<p><span class="math inline">\(J(U, V ; A)=\sum_{i=1}^{m} \sum_{j=1}^{n}\left(a_{i j}-\sum_{r=1}^{k} u_{i r} \cdot v_{r j}\right)^{2}+\lambda\left(\sum_{i=1}^{m} \sum_{r=1}^{k} u_{i r}^{2}+\sum_{j=1}^{n} \sum_{r=1}^{k} v_{r=1}^{2}\right)\)</span></p>
<p>ç­‰å¼å³è¾¹çš„ç¬¬ä¸€é¡¹å°±æ˜¯æˆ‘ä»¬çš„æŸå¤±å‡½æ•°ï¼Œç¬¬äºŒé¡¹æ˜¯L2æ­£åˆ™åŒ–é¡¹ï¼ˆå¯¹åº”äºredgeå›å½’çš„æ­£åˆ™åŒ–ï¼‰ï¼Œå¯ä»¥é™ä½è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜å¯¼è‡´åˆ†è§£åçš„çŸ©é˜µå…ƒç´ å¤ªå¤§ï¼ˆèƒŒå°±å®Œäº‹äº†ï¼‰</p>
<p>å¯¹æˆ‘ä»¬çš„æ„é€ å‡½æ•°æ±‚æ¢¯åº¦ï¼š</p>
<p><span class="math inline">\(\left\{\begin{array}{l}{\frac{\partial J(U, V ; A)}{\partial u_{i r}}=-2\left(a_{i j}-\sum_{r=1}^{k} u_{i r} v_{r j}\right) \cdot v_{r j}+2 \lambda u_{i r}} \\ {\frac{\partial J(U, V ; A)}{\partial v_{r j}}=-2\left(a_{i j}-\sum_{r=1}^{k} u_{i r} v_{r j}\right) \cdot u_{i r}+2 \lambda v_{r j}}\end{array}, 1 \leq r \leqslant k\right.\)</span></p>
<p>ç„¶åå› ä¸ºæ•°æ®é‡è¿‡å¤§æ‰€ä»¥é€‰ç”¨SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œ<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="noopener">Stochastic gradient descent</a>ï¼‰</p>
<p>æ±‚å¾—Uå’ŒVçŸ©é˜µåç›¸ä¹˜ï¼Œå¯ä»¥å‘ç°æ‰€å¾—ç»“æœæ¥è¿‘åŸçŸ©é˜µï¼Œå¹¶ä¸”åŸå…ˆä¸º0çš„ç‚¹æœ‰æ•°å€¼äº†ï¼ˆå³é¢„æµ‹å€¼ï¼‰</p>
<p>LFMçš„ä¼˜ç‚¹ï¼š</p>
<ol type="1">
<li>high accuracy</li>
<li>Auto group items â€“</li>
<li>Scalability is good â€“</li>
<li>Learning-based</li>
</ol>
<p>ç¼ºç‚¹ï¼š</p>
<ol type="1">
<li>Incremental updating</li>
<li>Real-time â€“</li>
<li>Explanation</li>
</ol>
<h3 id="model-based">4. Model-based</h3>
<p>ç”¨æˆ‘ä»¬å‰é¢å­¦è¿‡çš„å„ç§æ¨¡å‹æ¥è¿›è¡Œå¯¹ratingçš„é¢„æµ‹</p>
<h4 id="bayes">1. Bayes</h4>
<p><a href="#3.%20NaÃ¯ve%20Bayesï¼ˆæœ´ç´ è´å¶æ–¯">å¤ä¹ </a></p>
<p>ä¾‹å¦‚æˆ‘ä»¬æœ‰è¿™æ ·ä¸€ä¸ªè¡¨ï¼š</p>
<p><img src="/images/image-20191108113503171.png"></p>
<p>è®¡ç®—<span class="math inline">\(P(Item1 = 1, Item2 = 3, Item3 =3, Item4 =2|Item5 = 1)\)</span></p>
<p><span class="math inline">\(P(Item1 = 1, Item2 = 3, Item3 =3, Item4 =2|Item5 = 2)\)</span></p>
<p><span class="math inline">\(P(Item1 = 1, Item2 = 3, Item3 =3, Item4 =2|Item5 = 3)\)</span></p>
<p><span class="math inline">\(P(Item1 = 1, Item2 = 3, Item3 =3, Item4 =2|Item5 = 4)\)</span></p>
<p><span class="math inline">\(P(Item1 = 1, Item2 = 3, Item3 =3, Item4 =2|Item5 = 5)\)</span></p>
<p>é€‰æœ€é«˜çš„æ¦‚ç‡</p>
]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>Data Mining</tag>
      </tags>
  </entry>
  <entry>
    <title>Writing Task 1: Bar Chart</title>
    <url>/2022/06/05/Writing-Task-1-Bar-Chart/</url>
    <content><![CDATA[<p>Bar chart writing skills</p>
<a id="more"></a>
<h2 id="summary">Summary</h2>
<ol type="1">
<li><p>paraphrase the question (introduction)</p></li>
<li><p>make a general comparison (overview)</p>
<p>the green part</p>
<p><img src="overview.png" width="50%" height="50%"></p></li>
<li><p>Compare specific numbers (details)</p>
<p>lot of comparisons by showing numbers</p></li>
</ol>
<h2 id="example">Example</h2>
<p><img src="example.png"></p>
<p><strong>Introduction</strong>:</p>
<p>Question:</p>
<p>The chart below shows global sales of the top five mobile phone brands between 2009 and 2013.</p>
<p>Practice:</p>
<p>The bar chart compares global mobile phone sales using 5 popular brands over a period of 4 years.</p>
<p>Template:</p>
<p>The bar chart compares the number of mobile phone sold worldwide by the five most popular manufacturers in the years 2009, 2011 and 2013.</p>
<p><strong>Overview</strong>: 2 sentences, 2 main points</p>
<p>Practice:</p>
<p>In 2009 and 2011, the sales of mobile phones made by Nokia were significantly greater than other brands. However, Samsung became the most popular mobile phone brand in the world in 2013.</p>
<p>Template:</p>
<p>It is clear that Nokia sold the most mobile phones between 2009 and 2011, but Samsung became the best selling brand in 2013. Samsung and Apple saw the biggest rises in sales over the 5-year period.</p>
<p><strong>Details in P3</strong>:</p>
<p>Practice:</p>
<p>In 2009, Nokia sold nearly 450 mobile phones, which was almost double the number of handset sales of Samsung, which was the second manufacturer at that time. But after 4 years, Nokia fell nearly 200 units sold which was also the number of Samsungâ€™s increasing sold. By 2013, mobile pyhone sales of Samsung had reached 450 million units and became the market leader.</p>
<p>Template:</p>
<p>In 2009, Nokia sold close to 450 million mobile phones, which was <strong>almost double</strong> the number of handsets sold by the <strong>second most successful</strong> manufacturer, Samsung. Over the following four years, however, Nokiaâ€™s sales figures fell by approximately 200 million units, <strong>whereas</strong> Samsung saw sales rise by a similar amount. By 2013, Samsung had <strong>became the marker leader</strong> with sales reaching 450 million units.</p>
<p>Review:</p>
<p>In 2009, Nokia sold close to 450 million mobile phones, which was almost double the number of handsets sold by the second successful manufacturer, Samsung. Over the following four years, however, <strong>Nokiaâ€™s sales figures</strong> fell by approximately 200 million units, whereas Samsung <strong>saw sales rise by a similar amount</strong>. By 2013, Samsung had became the market leader with <strong>sales reaching 450 million units</strong>.</p>
<p><strong>Details in P4</strong>:</p>
<p>Template:</p>
<p><strong>The other three</strong> top selling mobile phone brands between 2009 and 2013 were LG, ZTE and Apple. In 2009, these companies sold around 125 million, 50 million and 25 million mobile handsets <strong>respectively</strong>, but Apple <strong>overtook</strong> the other two vendors in 2011. In 2013, purchases of Apple handsets <strong>reached</strong> 150 million units, <strong>while</strong> LG <strong>saw declining sales and</strong> the figures for ZTE rose only <strong>slightly</strong>.</p>
<p>Review:</p>
<p>The other three top selling mobile phone brands between 2009 and 2013 were LG, ZTE and Apple. In 2009, these companies sold around 150 million, 50 million and 20 million mobile handsets respectively, but Apple overtook the other two vendors in 2011. In 2013, purchases of Apple handsets reached 150 million units, while LG saw declining sales and the figures for ZTE rose only slightly.</p>
<h2 id="vocabulary">Vocabulary</h2>
<ul>
<li>sold worldwide</li>
<li>sales figures, purchases</li>
<li>Most popular, best selling brand, top selling</li>
<li>second most succesful manufacturer</li>
<li>Market leader</li>
<li>mobile phones, handsets, units</li>
<li>brands, manufacturer, vendor, company</li>
<li>saw the biggest rises, saw declining sales</li>
<li>close to, almost, approximately, around</li>
<li>double the number of</li>
<li>rise by a similar amount</li>
<li>respectively</li>
<li>Overtook the other two vendors</li>
</ul>
]]></content>
      <categories>
        <category>IELTS</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Writing Task 1: Line Graph</title>
    <url>/2022/06/05/Writing-Task-1-Line-Graph/</url>
    <content><![CDATA[<p>Line graphs show numbers changing over a period of time</p>
<p>A line graph always has 2,3,4,5 lines on a graph. And your job is to compare the lines, not decribe them sepratately.</p>
<a id="more"></a>
<h2 id="summary">Summary</h2>
<p><strong>First</strong>, make a very general comparison. (Overview)</p>
<p>Just like the blue line is the highest. Very very general</p>
<p><img src="overview.png" width="50%" height="50%"></p>
<p><strong>Second</strong>, compare the lines at specific points. (Details)</p>
<p>The yellow parts:</p>
<p><img src="detail.png" width="50%" height="50%"></p>
<h2 id="example">Example</h2>
<p><img src="example.png"></p>
<p><strong>Introduction</strong>: change the order and key words</p>
<p>The line graph compares the amount of electricity produced in France using 4 different sources of power over a period of 32 years.</p>
<p><strong>Overview</strong>: 2 sentences, 2 main points</p>
<p>It is clear that nuclear power was by far the most important means of electricity generation over the period shown. Renewables provided the lowest amount of electricity in each year.</p>
<p><strong>Details</strong>: 2 paragraghs, compare the lines</p>
<p><strong>Paragragh 3</strong>: comparison with the start</p>
<p>In 1980, thermal power stations were the main source of electricity in France, generating around 120 terawatt hours of power. Nuclear and hydroelectric power stations produced just under 75 terawatt hours of electricity each, and renewables provided a negligible amount. Just one year later, nuclear power overtook thermal power as the primary source of electricity.</p>
<p><strong>Tips when describing numbers</strong></p>
<p>Canâ€™t write:</p>
<ul>
<li>Nuclear was 75 TW-h of electricity.</li>
<li>Nuclear produced 75 TW-h of electricity.</li>
</ul>
<p>Should write:</p>
<ul>
<li>Nuclear power was used to produce â€¦</li>
<li>Nuclear power stations produced â€¦</li>
</ul>
<p>Not just take words from graph, think more clearfully.</p>
<p><strong>Paragragh 4</strong>: describle tendency, peak</p>
<p>Between 1980 and 2005, electricity production from nuclear power rose dramatically to a peak of 430 terawatt hours. By contrast, the figure for thermal power fell to only 50 terawatt hours in 1985, and remained at this level for the rest of the period. Hydroelectric power generation remained relatively stable, at between 50 and 80 terawatt hours, for the whole 32-year period, but renewable electricity production saw only a small rise to approximately 25 terawatt hours by 2012.</p>
<h2 id="vocabulary">Vocabulary</h2>
<ul>
<li>amount of electricity produced</li>
<li>source of / provided / generating</li>
<li>means of electricity generation</li>
<li>over a period of / over the period shown</li>
<li>by far the most important</li>
<li>a negligible amount</li>
<li>nuclear power overtook thermal power</li>
<li>as the primary source of electricty</li>
<li>rose dramatically to a peak of</li>
<li>by contrast</li>
<li>the figure for nulcear power</li>
<li>remained at this level, remained relatively stable</li>
<li>saw only a small rise</li>
<li>approximately 25</li>
</ul>
]]></content>
      <categories>
        <category>IELTS</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Writing Task 1: Description</title>
    <url>/2022/06/05/Writing-Task-1-Description/</url>
    <content><![CDATA[<p>no conclusion, but summary (overview)</p>
<p>a report, describing task</p>
<a id="more"></a>
<h2 id="summary">Summary</h2>
<p><strong>Question types</strong></p>
<ol type="1">
<li>Line graph</li>
<li>Bar chart</li>
<li>Pie chart</li>
<li>Table</li>
<li>Diagram-comparing</li>
<li>Diagram-process</li>
</ol>
<p>The first four types are most common, and they are almost the same: <strong>describe, compare, changes/trends</strong> numbers.</p>
<p><strong>Eassy Structure</strong></p>
<p>4 paragraphs</p>
<ol type="1">
<li><p><strong>Introduction</strong></p>
<p>1 sentence: paraphrase the question</p>
<p>just try to change key words, or order</p></li>
<li><p><strong>Overview</strong> (or put it at the end)</p>
<p>2 sentences: The main, general things</p></li>
<li><p><strong>Details</strong></p></li>
<li><p><strong>Details</strong></p>
<p>2 paragraphs makes you organize or group the information better</p></li>
</ol>
<p><strong>No conclusion!</strong></p>
]]></content>
      <categories>
        <category>IELTS</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Contrastive Learning Survey (7) â€” Contrastive Learning theory</title>
    <url>/2022/05/19/Contrastive-Learning-Survey-7-%E2%80%94-Contrastive-Learning-theory/</url>
    <content><![CDATA[<p>Why Contrastive Learning can work? It is also an interesting field and has many excellent papers.</p>
<p>At present, <strong>mutual information</strong> is a key point in my cognition. Of course, this chapter will continue to be updated as my research progresses.</p>
<a id="more"></a>
<h2 id="what-makes-for-good-views-for-contrastive-learning">What Makes for Good Views for Contrastive Learning</h2>
<h2 id="demystifying-self-supervised-learning-an-information-theoretical-framework">Demystifying Self-Supervised Learning: An Information-Theoretical Framework</h2>
<h2 id="on-mutual-information-in-contrastive-learning-for-visual-representations">On Mutual Information in Contrastive Learning for Visual Representations</h2>
]]></content>
      <categories>
        <category>Contrastive Learning</category>
      </categories>
      <tags>
        <tag>self-supervised learning</tag>
        <tag>computer vision</tag>
      </tags>
  </entry>
  <entry>
    <title>Contrastive Learning Survey(6) â€” Contrastive Learning with positive/negative sampling</title>
    <url>/2022/05/17/Contrastive-Learning-Survey-6-%E2%80%94-Contrastive-Learning-with-positive-negative-sampling/</url>
    <content><![CDATA[<p>In addition to the lack of high-level semantic representations, the sampling of positive and negative samples for instance discrimination tasks also has obvious problems. Positive samples are all from its own data augmentation, while negative samples are directly sampled from a large memory (within the same batch, memory bank or a momentum encoder). The problem with this is that negative samples may have positive sample, which they are the same class, but are different instances. Another problem is that hard negative samples and easy negative samples have the same weights in InfoNCE loss , which make model too simple if it focus on too many easy negative samples. In this chapter, we will see many interesting works with different methods on solving sampling problem in CL.</p>
<a id="more"></a>
<h2 id="what-should-not-be-contrastive-in-contrastive-learning">What Should Not Be Contrastive In Contrastive Learning</h2>
<p>Link: https://arxiv.org/abs/2008.05659</p>
<p><strong>Motivation: </strong></p>
<p><img src="LooC/motivation.png"></p>
<p>This paper proposed that data augmentation may hurt image representations, especially when they are used on downstream tasks. For example, representation vectors by color augmentation are used on a bird identification task , or representation vectors by rotation augmentation are used on a animal identifacation task.</p>
<p>Sounds reasonable, and it did!</p>
<p><strong>Result:</strong></p>
<p><img src="LooC/result.png" width="75%" height="75%"></p>
<p>Table 1 shows that add rotation augmentation on MoCo will significantly reduces its accuracy on IN-100(ImageNet-100) this dataset. And their model LooC has a better result.</p>
<p><strong>Method:</strong></p>
<p><img src="LooC/method.png"></p>
<p>And add all these <span class="math inline">\(Z\)</span> regions into loss function:</p>
<p><span class="math display">\[\mathcal{L}_{q}=-\frac{1}{n+1}\left(\log \frac{E_{0,0}^{+}}{E_{0,0}^{+}+\sum_{k^{-}} E_{0,0}^{-}}+\sum_{i=1}^{n} \log \frac{E_{i, i}^{+}}{\sum_{j=0}^{n} E_{i, j}^{+}+\sum_{k^{-}} E_{i, i}^{-}}\right)\]</span></p>
<p>where <span class="math inline">\(E_{i, j}^{\{+,-\}}=\exp \left(\boldsymbol{z}_{i}^{q} \cdot \boldsymbol{z}_{i}^{k_{j}^{\{+,-\}}} / \tau\right)\)</span></p>
<p><strong>Insights:</strong></p>
<p>I think the movitation is quite reasonable. But actually, I am not clear about the method and this paper do not provide pesudo code or open source code. I am not sure that if aggregating all different augmentation infomations into the same loss function will solve the problem of different specific downstream tasks. It's more like an improvement in robustness, rather than aiming at adapting with specific downstream tasks.</p>
<h2 id="boosting-contrastive-self-supervised-learning-with-false-negative-cancellation">Boosting Contrastive Self-Supervised Learning with False Negative Cancellation</h2>
<p>Link: https://arxiv.org/abs/2010.02037</p>
<p><strong>Motivation</strong>:</p>
<p>Negative samples by instance discrimination will contain many potential positive samples(images in the same class). And these false negative samples will hurt CL in 2 ways: <strong>discarding semantic information</strong> and <strong>slow convergence</strong></p>
<p><strong>Method</strong>:</p>
<p><img src="FPC/method.png"></p>
<p>There are 3 parts:</p>
<ol type="1">
<li>Traditional CL framework</li>
<li>False Negative Identification</li>
<li>False Negative Cancellation
<ol type="1">
<li>Elimination</li>
<li>Attraction</li>
</ol></li>
</ol>
<p>The first part is the model backbone. This paper use SimCLR and MoCo.</p>
<p>The second part is the method how to find those false negatives without label information.</p>
<p>The approach to identify false negatives based on the following observations:</p>
<ol type="1">
<li>False negatives are samples from different images with the same semantic content, therefore they should hold certain similarity (e.g., dog features).</li>
<li>A false negative may not be as similar to the anchor as it is to other augmentations of the same image, as each augmentation only holds a specific view of the object</li>
</ol>
<p>Based on above observations, the strategy follows as:</p>
<ol type="1">
<li>Create a support set contains the image itself, its original augmentation(put into model), and extra augmentations (to hold the second observation).</li>
<li>Each negative sample compute a similarity score set, e.g. a negative sample will have 8 similarity scores if the size of support set is 8. This paper use <strong>cosine similarity</strong>.</li>
<li>Using an aggregation method for the similarity score set, <strong>Maximum</strong> or <strong>Mean</strong></li>
<li>Selecting the most similar negative samples as false negative samples.
<ol type="1">
<li>Select <strong>top k</strong> high similarity samples</li>
<li>Select the samples those similarity score <strong>is greater than a threshold</strong></li>
</ol></li>
</ol>
<p>The third part is how to use these false negative samples. There are 2 approaches:</p>
<p><strong>False Negative Elimination</strong>: These false negatives do not contrast against them, which means drop them from the loss function.</p>
<p><strong>False Negative Attraction</strong>: These false negatives will be added into positive samples just like supervised contrastive learningâ€™s loss function.</p>
<p><strong>Insights</strong>:</p>
<p>This paper gives many insights from their exhaustive experiments since it is from Google.</p>
<p><img src="FPC/elimination.png" width="75%" height="75%"></p>
<p>The Gap between FNE and baseline will be larger with the increasing random crop ratio. It is reasonable since larger crop ratio will lead to a higher ratio of false negatives.</p>
<p><img src="FPC/attraction.png" width="75%" height="75%"></p>
<p>The attraction strategy is much more sensitive to the quality of the found false negatives compared to the elimination strategy.</p>
<p>The paper has many other interesting findins and really recommend you to read paper.</p>
<h2 id="unsupervised-representation-learning-by-invariance-propagation">Unsupervised Representation Learning by Invariance Propagation</h2>
<p>Link: https://arxiv.org/abs/2010.11694</p>
<p><strong>Motivation:</strong></p>
<p>This paper also aims to solve the positive/negative sampling problem.</p>
<ol type="1">
<li>negative samples by random may have positive samples.</li>
<li>Positive samples also should have different levels. Easy positive samples and hard positive samples have the same impact on model now.</li>
</ol>
<p><strong>Method:</strong></p>
<p>This paper has a assumption that:</p>
<blockquote>
<p>If two points v1 and v2 in a high-density region are close, then their semantic information should be similar.</p>
</blockquote>
<p>Hence, we can use metric method like KNN to calculate anchorsâ€™ positive samples step by step.</p>
<p><img src="Invariance_Propagation/method.png"></p>
<p><strong>Strategy of Positive samples of an anchor image</strong>:</p>
<p>The process is illustrated in Fig 1. In each step, all k-nearest neighbors of the current discovered positive samples are added to the positive sample set. The process repeats l steps.</p>
<p>The difference between tradition KNN and their approach can be shown in the last step of Figure 1. Samples in the dashed circle are positive samples discovered by KNN. By comparison, in their approach, point B is included in K-nearest neighbors of point A and point C is not included.</p>
<p><strong>Hard Sampleing Strategy:</strong></p>
<p>The core is that this paper think positive samples which are already close to the anchor and negative samples which are already far away from the anchor are easy samples. They do not need to be optimized more. However, those positive samples that are far away in the positive sample set(C in Fig 1 )and the negative samples that are relatively close(B in Fig 1) are difficult samples that are more helpful to the model.</p>
<p>Strategy:</p>
<ol type="1">
<li><p>Select P samples with the lowest similarity to construct the hard positive sample set <span class="math inline">\(\mathcal{N}^{h}(i)\)</span>.</p>
<p>These hard positive samples deviate far from the anchor sample such that they provide more intra-class variations, which is beneficial to learn more abstract invariance.</p></li>
<li><p>Denote the M nearest neighbors of <span class="math inline">\(v_i\)</span> as <span class="math inline">\(\mathcal{N}_{M}(i)\)</span>, The <span class="math inline">\(M\)</span> is large enough such that <span class="math inline">\(\mathcal{N}(i) \subseteq \mathcal{N}_{M}(i)\)</span>. Then denote the hard negative sample set <span class="math inline">\(\mathcal{N}_{n e g}(i)=\mathcal{N}_{M}(i)-\mathcal{N}(i)\)</span>.</p></li>
</ol>
<p>Then the hard sample mining loss can be like:</p>
<p><span class="math display">\[\begin{aligned} \mathcal{L}_{i n v}\left(x_{i}\right) &amp;=-\log P_{v_{i}}\left(\mathcal{N}^{h}(i) \mid B(i)\right) \\ &amp;=-\log \frac{\sum_{p \in \mathcal{N}^{h}(i)} \exp \left(\bar{v}_{p} \cdot v_{i} / \tau\right)}{\sum_{n \in B(i)} \exp \left(\bar{v}_{n} \cdot v_{i} / \tau\right)} \end{aligned}\]</span></p>
<p>The overall loss function is just InfoNCE + hard sample mining loss, as follow:</p>
<p><span class="math display">\[\mathcal{L}\left(x_{i}\right)=\mathcal{L}_{i n s}\left(x_{i}\right)+\lambda_{i n v} \cdot \omega(t) \cdot \mathcal{L}_{i n v}\left(x_{i}\right)\]</span></p>
<p>where <span class="math inline">\(\omega(t)\)</span> as 0 is the first T epochs, because it is not reliable in frist <span class="math inline">\(t\)</span> epoches.</p>
<h2 id="contrastive-learning-with-hard-negative-samples">Contrastive Learning With Hard Negative Samples</h2>
<p>Link: https://arxiv.org/abs/2010.04592</p>
<p><strong>Motivation:</strong></p>
<p>This paper also focus on negative samples. In metric learning, some work proves that <strong>hard negative samples</strong> can help guide a learning method to correct its mistakes more quickly (Schroff et al., 2015; Song et al., 2016). Therefore, this paper also wants to use hard negative samples in contrastive learning. Compared with metric learning, contrasitive learning is a unsupervised task, so there are 2 challenges:</p>
<ol type="1">
<li>We do not have access to any true similarity of dissimilarity information.</li>
<li>We need an efficient sampling strategy for this tunable distribution.</li>
</ol>
<p>This paper use Figure 1 to show the defination of Hard Negatives.</p>
<p><img src="HNS/motivation.png"></p>
<p><strong>Method:</strong></p>
<blockquote>
<p>We begin by asking <em>what makes a good negative sample?</em></p>
<p>To answer this question we adopt the following two guiding principles:</p>
<p>Principle 1. q <em>should only sample â€œtrue negativesâ€</em> <span class="math inline">\(x_i\)</span> <em>whose labels differ from that of the anchor</em> x<em>.</em></p>
<p>Principle 2. <em>The most useful negative samples are ones that the embedding currently believes to be similar to the anchor.</em></p>
</blockquote>
<p>Because of no supervision, upholding Principle 1 is impossible to do exactly. This paper aims to upholds Principle 1 approximately, and simultaneously combines this idea with the key additional conceptual ingredient of â€œhardnessâ€ (encapsulated in Principle 2).</p>
<p>The theory of this article is very solid with lots of mathmatical prooves, which I want to skip them and only give the final loss pesudo code.</p>
<p><img src="HNS/loss.png"></p>
<p><strong>Insights:</strong></p>
<p>I think that the core of this paper is that they think negative samples those are not very close to and also are not too far from the anchor in the encoding space(called <strong>Hard negative samples</strong>) are useful for model to learn better representation.</p>
<p>Hence, I think it is similar with the previous paper: find those ture negative samples without prior knowledge, where the difference is on different statistics methods.</p>
<h2 id="hard-negative-mixing-for-contrastive-learning">Hard Negative Mixing for Contrastive Learning</h2>
<p>Link: https://arxiv.org/abs/2010.01028</p>
<h2 id="conditional-negative-sampling-for-contrastive-learning-of-visual-representations">Conditional Negative Sampling For Contrastive Learning of Visual Representations</h2>
<p>Link: https://arxiv.org/abs/2010.02037</p>
<h2 id="debiased-contrastive-learning">Debiased Contrastive Learning</h2>
<p>Link: https://arxiv.org/abs/2007.00224</p>
<h2 id="contrastive-crop">Contrastive Crop</h2>
<p>Link: https://arxiv.org/abs/2202.03278</p>
<p>Github: https://github.com/xyupeng/ContrastiveCrop</p>
<p><strong>Motivation:</strong></p>
<p>Random crop will hurt image and decrease image semantic.</p>
<p>A method for sampling true right positive samples.</p>
<p>æå‡ºäº†ä¸€ç§æŠ€æœ¯ï¼Œ</p>
<p>ä»å›¾ä¸­å¯ä»¥å‘ç°ï¼Œå¯¹æ¯”å­¦ä¹ æ¨¡å‹æœ¬èº«å°±å¯ä»¥æ•æ‰ç‰©ä½“å¤§æ¦‚çš„ä½ç½®ä¿¡æ¯ï¼Œä»è€Œæ¥æŒ‡å¯¼cropsçš„é€‰å–ã€‚</p>
<p>åˆ©ç”¨è¿™ç§ç‰¹å¾æå–ç‰©ä½“è¾¹æ¡†ï¼Œé¿å…é”™è¯¯æ­£æ ·æœ¬å¯¹</p>
<p>ä¸­å¿ƒæŠ‘åˆ¶ï¼Œä¸Šè¿°æ–¹æ³•ç”±äºå…¶å¸¦æ¥äº†æ›´å°çš„é€‰å–èŒƒå›´ï¼Œç”Ÿæˆå…·æœ‰è¾ƒé«˜ç›¸ä¼¼åº¦çš„æ ·æœ¬å¯¹çš„å¯èƒ½æ€§ä¹Ÿæé«˜äº†</p>
<p>é™ä½cropsé›†ä¸­åœ¨å›¾ç‰‡ä¸­å¿ƒçš„æ¦‚ç‡ï¼Œä»è€Œå¢å¤§é‡‡æ ·çš„æ–¹å·®ã€‚</p>
<h2 id="conclusion">Conclusion</h2>
<p>So far, we have seen a lot of works on how to sample more accurate and better quality positive/negative samples without labels.</p>
<ol type="1">
<li><p>Use similarity metrics by imagesâ€™ embedding vectors to get more reliable samples based on the assumption that same class embedding will be close/similar in the hyperplane.</p></li>
<li><p>Restrict data augmentations to achieve more reliable samples by Contrastive Crop or LooC.</p></li>
</ol>
<p>So can we combine these 2 ideas that using similarity metrics of the anchor and its augs to sort positive samples with data augmentations as true positive samples and low similarity augs as true negative samples even they are augmentations from the same image.</p>
]]></content>
      <categories>
        <category>Contrastive Learning</category>
      </categories>
      <tags>
        <tag>self-supervised learning</tag>
        <tag>computer vision</tag>
      </tags>
  </entry>
  <entry>
    <title>Contrastive Learning Survey (5) â€” Contrastive Learning with Clustering</title>
    <url>/2022/05/06/Contrastive-Learning-Survey-5-%E2%80%94-Contrastive%20Learning%20with%20Clustering/</url>
    <content><![CDATA[<p>So far, we have discussed Contrastive Learning(CL) frameworks such as InstDisc, SimCLR, MoCo, SimSiam, BYOL, which still the mainstream framework in the lastest CL works. Instead of frameworks, CL still has many also problems or weakness. This chapter will discribe one of it weakness: lacking semantic presententation due to instance discrimination pretext task, i.e. a model good at discriminating instances will lose higher level information like their clusters. Therefore, letâ€™s see how researchers integrate clustering methods into Contrastive Learning. Moreover, this chapter and the next will be like a short survey of the lastest CL papers, which means I will not write every paper clearly and discuss every techniques. We will focus on papersâ€™ motivation, main method and try to give my own insights.</p>
<a id="more"></a>
<h2 id="swav">SWaV</h2>
<p>To be improved.</p>
<h2 id="pcl-prototypical-contrastive-learning">PCL: PROTOTYPICAL CONTRASTIVE LEARNING</h2>
<p>Paper: Li J, Zhou P, Xiong C, et al. Prototypical contrastive learning of unsupervised representations[J]. arXiv preprint arXiv:2005.04966, 2020.</p>
<p>Arxiv: https://arxiv.org/abs/2005.04966</p>
<p>This paper proposes 2 drawbacks of previous contrastive learning framework.</p>
<ol type="1">
<li><blockquote>
<p>The task of instance discrimination could be solved by exploiting low-level image differences, thus the learned embeddings do not necessarily capture high-level semantics. This is supported by the fact thatthe accuracy of instance classification often rapidly rises to a high level (&gt;90% within 10 epochs) and further training gives limited informative signals. A recent study also shows that better performance of instance discrimination could worsen the performance on downstream tasks.</p>
</blockquote></li>
<li>cannot capture the semantic structure of data, since each negative sample shares similar semantics by InfoNCE.</li>
</ol>
<h3 id="framework">Framework</h3>
<p><img src="PCL/framework.png"></p>
<p><img src="PCL/pseudo_code.png"></p>
<p>Figure 2 and pseudo-code can help understand PCL:</p>
<ol type="1">
<li>use features from momentum encoder to do k-means to get prototypes/cluster centers and concentration(discuss later).</li>
<li>Use ProtoNCE to update network, which ProtoNCE will use prototypes by the previous step.</li>
<li>update momentum encoder like MoCo</li>
</ol>
<h3 id="protonce">ProtoNCE</h3>
<p><span class="math display">\[\mathcal{L}_{\text {ProtoNCE }}=\sum_{i=1}^{n}-\left(\log \frac{\exp \left(v_{i} \cdot v_{i}^{\prime} / \tau\right)}{\sum_{j=0}^{r} \exp \left(v_{i} \cdot v_{j}^{\prime} / \tau\right)}+\frac{1}{M} \sum_{m=1}^{M} \log \frac{\exp \left(v_{i} \cdot c_{s}^{m} / \phi_{s}^{m}\right)}{\sum_{j=0}^{r} \exp \left(v_{i} \cdot c_{j}^{m} / \phi_{j}^{m}\right)}\right)\]</span></p>
<p>where <span class="math inline">\(c\)</span> is cluster information. So this loss combines between Clusters info and Instances Info. <span class="math inline">\(M\)</span> means that they will do k-means M times or M different k clusters, they proposed that M times k-means will have different propotype semantic structure from high semantice to low semantic.</p>
<p>Another different paramter is <span class="math inline">\(\phi\)</span>, where it always is a hyperparamter of <span class="math inline">\(\tau\)</span>. Here <span class="math inline">\(\phi\)</span> is a dynamic paramter learning with training.</p>
<p><span class="math display">\[\phi=\frac{\sum_{z=1}^{Z}\left\|v_{z}^{\prime}-c\right\| 2}{Z \log (Z+\alpha)}\]</span></p>
<p>where <span class="math inline">\(Î±\)</span> is a smooth parameter to ensure that small clusters do not have an overly-large <span class="math inline">\(\phi\)</span>. They normalize <span class="math inline">\(\phi\)</span> for each set of prototypes <span class="math inline">\(C_m\)</span> such that they have a mean of <span class="math inline">\(\tau\)</span>.</p>
<h2 id="hcsc-hierarchical-contrastive-selective-coding">HCSC: Hierarchical Contrastive Selective Coding</h2>
<p><img src="HCSC/figure1.png" width="75%" height="75%"></p>
<p>This paper proposed that a large dataset contains multiple semantic, e.g. â€œmammals <span class="math inline">\(\to\)</span> Dogs <span class="math inline">\(\to\)</span> Labradorsâ€, where this kind of hierarachical semantics do not be noticed in previous researches. Actucally, PCL does this like multiple times k-means. But this paper does more work on this.</p>
<p>Another novelty job is that they propose to select high-quality positive and negative pairs which confirms that they are true negative and positive samples.</p>
<p>For positive samples: select most similar prototype on each semantic hierarchy to build more abundant positive pairs.</p>
<p>For negative samples: use a Bernoulli sampling to determine if a sample is kept or discarded by their semantic correlation value.</p>
<p>### Hierarchical K-means</p>
<p><img src="HCSC/hierarchical_k_means.png" width="50%" height="50%"></p>
<p>The difference between HCSC and PCL is that they iterate k-means in the clusters, where PCL do gobal k-means many times.</p>
<h3 id="sample-selecting">Sample selecting</h3>
<p><img src="HCSC/sample_method.png"></p>
<p>For example, mammals will be a global semantic which be shown as a biggest green point. Then â€œdogsâ€ will be â€œmammalsâ€ branch and be shown as a smaller green point, and â€œpoodlesâ€ and â€œlabradorsâ€ will be shown as the smallest green points. Every other different color points are those who are different with â€œMammalsâ€.</p>
<p>For (b), when selecting samples, positive pairs will be selected in all green points with different sizes to ensure they have different hierarchical semantic, e.g. not only the augs from themselves, but also â€œpoodlesâ€, â€œdogsâ€, â€œmammalsâ€ for a â€œlabradorsâ€ image. And Negative pairs will be selected in different color points and sample them by a Bernoulli sampling (dicuss later).</p>
<p><strong>selecting code</strong></p>
<p><strong>selecting example</strong></p>
<p><img src="HCSC/query.png" width="75%" height="75%"></p>
<p><img src="HCSC/cluster.png"></p>
]]></content>
      <categories>
        <category>Contrastive Learning</category>
      </categories>
      <tags>
        <tag>self-supervised learning</tag>
        <tag>computer vision</tag>
      </tags>
  </entry>
  <entry>
    <title>Contrastive Learning Survey (4) â€” SimSiam</title>
    <url>/2022/04/18/Contrastive-Learning-Survey-4-%E2%80%94-SimSiam/</url>
    <content><![CDATA[<p>This article aims to introduce a new kind of framework in contrastive learning which does not use negative samples.</p>
<p>Paper: Chen, X., &amp; He, K. (2021). Exploring simple siamese representation learning. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 15750-15758). Updated version accessed at: https://arxiv.org/abs/2011.10566</p>
<a id="more"></a>
<h2 id="novelty">Novelty</h2>
<p>Simsiam is not the first framework which does not use negative samples. BYOL (Grill, J. B., Strub, F., AltchÃ©, F., Tallec, C., Richemond, P., Buchatskaya, E., ... &amp; Valko, M. (2020). Bootstrap your own latent-a new approach to self-supervised learning. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 21271-21284.) is early than it and has already achieved better accuracy on ImageNet. Generally, SimSiam is just a special case of BYOL. But SimSiamâ€™s beautiful subtraction of BYOL makes people realize the essence of success in contrastive learning without negative samples.</p>
<h2 id="note">Note</h2>
<p>Simsiam does very detailed ablation experiments about its used techniques like: stop-gradient, predictor, hypothesis, loss function, batch normalization. If you want to check result plots, I highly recommend reading the original paper and I will not put them in this blog again.</p>
<h2 id="framework">Framework</h2>
<p><img src="simsiam.png"></p>
<p>Just like paperâ€™s name, it is a real simple framework. Model only use a same encoder(encoder + projector) and a predictor.</p>
<p>Predictor is new one which does not decrible in previous article. A predictor is just an extra MLP. Simsiam wants to make the feature from predictor from <span class="math inline">\(x_1\)</span> (augmentation from image x) can be similar with <span class="math inline">\(x_2\)</span>. Hence, its loss function is just cosine similarity.</p>
<p><img src="byol.png"></p>
<p>Figure 2 is BYOLâ€™s architecture. You can see Simsiam is totally same as BYOL, which the only difference is that BYOL uses a Momentum Encoder <span class="math inline">\(y_{\xi}^{\prime}\)</span> , where <span class="math inline">\(\xi \leftarrow \tau \xi+(1-\tau) \theta\)</span></p>
<p>Hence, you can think of SimSiam as just a special case of BYOL when <span class="math inline">\(\gamma = 1\)</span>.</p>
<p>This architecture is very easy to think of, but why was it not first proposed by BYOL until 2020? The biggest problem with training methods without negative samples is that the model will quickly find a shortcut and cause <strong>model collapsing</strong>. Imagine that our encoder only needs to learn to output 0 at this time, the cosine similarity of the loss function is equal to 0, and the model will no longer be updated.</p>
<p>How BYOL and SimSiam solve model collapsing? They both use a trick: <strong>stop gradient</strong>. We will discuss in detail in the following chapter.</p>
<h2 id="loss-function">Loss Function</h2>
<p><span class="math display">\[\mathcal{D}\left(p_{1}, z_{2}\right)=-\frac{p_{1}}{\left\|p_{1}\right\|_{2}} \cdot \frac{z_{2}}{\left\|z_{2}\right\|_{2}}\]</span></p>
<p>Denote that the encoder is <span class="math inline">\(f\)</span>, prediction is <span class="math inline">\(h\)</span>, then we have <span class="math inline">\(p = h(f(x))\)</span>, <span class="math inline">\(z = f(x)\)</span></p>
<p>So the equation is just the cosince similarity of the features between encoder and prediction.</p>
<p><span class="math display">\[\mathcal{L}=\frac{1}{2} \mathcal{D}\left(p_{1}, z_{2}\right)+\frac{1}{2} \mathcal{D}\left(p_{2}, z_{1}\right)\]</span></p>
<p>SimSiam define a summetrized loss like above, where <span class="math inline">\(1, 2\)</span> is the index of two augmentation images from the same image.</p>
<h2 id="stop-gradient">Stop Gradient</h2>
<p>Stop Gradient means that when the model start to do gradient decent, the branch with stop-gradient will be treated as a constant. Hence, the loss function also can be seen as: <span class="math display">\[\mathcal{L}=\frac{1}{2} \mathcal{D}\left(p_{1}, \text { stopgrad }\left(z_{2}\right)\right)+\frac{1}{2} \mathcal{D}\left(p_{2}, \text { stopgrad }\left(z_{1}\right)\right)\]</span></p>
<p>SimSiam has proved that stop-gradient is the key which prevents model from collapsing solutions with every detailed experiments. You can read them in the paper about experiments and results.</p>
<p>Authorsâ€™ hypothesis is that:</p>
<blockquote>
<p>SimSiam is an implementation of an Expectation-Maximization (EM) like algorithm.</p>
</blockquote>
<p>The loss function can be like:</p>
<p><span class="math display">\[\mathcal{L}(\theta, \eta)=\mathbb{E}_{x, \mathcal{T}}\left[\left\|\mathcal{F}_{\theta}(\mathcal{T}(x))-\eta_{x}\right\|_{2}^{2}\right]\]</span></p>
<p>I think that what paper writes is very clear and I could not simplify any more:</p>
<blockquote>
<p><img src="hypothesis.png" width="50%" height="50%"></p>
</blockquote>
<p>Than, we can use SGD to solve (7) and get new <span class="math inline">\(\eta_{x}\)</span> by the updated encoder <span class="math inline">\(\mathcal{F}\)</span>.</p>
<p>For the prediction and the complete hypothesis, I highly recommend reading the original paper since I could not simplify them more.</p>
<h2 id="comparision">Comparision</h2>
<p><img src="evaluation.png"></p>
<p>Table 4 is a global table where hasall networks we talked about in the previous chapters. And it clearly showes that SimSiam uses the simplest framework (no negative pair, no momentum encoder, very samll batch size) achieves a good result(68.1% acc) on ImageNet.</p>
<p>Hence, although SimSiam does not have any new techniques, it probably shows the kernel of unsupervised learning in the image domain. Such beautiful subtraction operations and exhaustive experimental evidence earned this work the Best Paper Honorable Mentions of CVPR 2022.</p>
]]></content>
      <categories>
        <category>Contrastive Learning</category>
      </categories>
      <tags>
        <tag>self-supervised learning</tag>
        <tag>computer vision</tag>
      </tags>
  </entry>
  <entry>
    <title>Contrastive Learning Survey (3) â€” MoCo</title>
    <url>/2022/03/05/Contrastive-Learning-Survey-3-%E2%80%94-MoCo/</url>
    <content><![CDATA[<p>This article decribes a milestone in contrastive learning and the best paper <strong>Nominee</strong> in CVPR 2020.</p>
<p>Paper: He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. <em>arXiv preprint arXiv:1911.05722</em>, 2019. Updated version accessed at: https://arxiv.org/abs/1911.05722.</p>
<a id="more"></a>
<h2 id="bottleneck-of-past-work">Bottleneck of past work</h2>
<p>The requirement of huge batch size makes simCLR hard to land in real scenes. This paper proposes a framework called MoCo which can not only overcome feature consistency but also provide a large negative samples for training.</p>
<h2 id="model-framework">Model Framework</h2>
<p><img src="figure1.png" width="50%" height="50%"></p>
<p>As usual, a CV paper can roughly understand what the paper wants to express through Figure 1.</p>
<p>A quick review of simCLR:</p>
<p><img src="simCLR_framework.jpg" width="50%" height="50%"></p>
<p>The only difference is that MoCo uses a momentum encoder instead of two same encoders.</p>
<p>Note: simCLR and MoCo are the same period work, and in MoCo V2, projector which is proposed by simCLR is used.</p>
<h2 id="queue-to-save-negative-samples">Queue to save negative samples</h2>
<p>We have talked about the cons of InstDisc: inconsistency of negative samples since fast updating encoders; SimCLR: very large batch size to keep negative samples have consistent distribution with positive samples.</p>
<p>To overcome these bottlenecks, Moco wants to have a large dictionary to save negative samples, where these negative samples also are <strong>kept as consistent as possible despite its evolution</strong>. proposed to use a queue to save negative samples by using queueâ€™s FIFO(first in first out) characteristic. Then, a queue with much larger size than batch size is used and you can use small batch size to train networks.</p>
<p>The queue size is recommended as 65536. And after each batch, the earliest negative samples with batch size(n) in the queue will be deleted, and the latest n negative samples will be added to the queue to keep the queue updated.</p>
<p>So far, this queue seems to be like memory bank in InstDisc. And how negative samples in this queue can have same consistency will be discussed in next chapter: a momentum encoder.</p>
<h2 id="momentum-encoder">Momentum encoder</h2>
<p>A momentum encoder is like:</p>
<p><span class="math display">\[\theta_{\mathrm{k}} \leftarrow m \theta_{\mathrm{k}}+(1-m) \theta_{\mathrm{q}}\]</span></p>
<p>where <span class="math inline">\(\theta_{\mathrm{k}}\)</span> is momentum encoder, <span class="math inline">\(\theta_{\mathrm{q}}\)</span> is the encoder updated by back-propagation. <span class="math inline">\(m\)</span> always equals 0.999. Hence, the momentum encoder is updated very slowly and ensure that the negative samples from <span class="math inline">\(\theta_{\mathrm{k}}\)</span> are kept consistency.</p>
<h2 id="loss-function">Loss Function</h2>
<p><span class="math display">\[\mathcal{L}_{q}=-\log \frac{\exp \left(q \cdot k_{+} / \tau\right)}{\sum_{i=0}^{K} \exp \left(q \cdot k_{i} / \tau\right)}\]</span></p>
<p>Moco proposed infoNCE loss and soon become the mainstream loss in contrastive learning. The only difference between NCE loss and infoNCE loss is that infoNCE only uses K negative samples from instead of all negative samples.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Moco achieves 68.6% accuracy by linear classification protocol on ImageNet. Compared with 54% in InstDice, Moco can be regarded as a milestone work in contrastive learning.</p>
]]></content>
      <categories>
        <category>Contrastive Learning</category>
      </categories>
      <tags>
        <tag>self-supervised learning</tag>
        <tag>computer vision</tag>
      </tags>
  </entry>
  <entry>
    <title>Contrastive Learning Survey (2) â€” SimCLR</title>
    <url>/2022/03/02/Contrastive-Learning-Survey-2-%E2%80%94-SimCLR/</url>
    <content><![CDATA[<p>This article decribes an end-to-end framework for contrastive learning called SimCLR.</p>
<p>Paper: Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. <em>arXiv:2002.05709</em>, 2020. Updated version accessed at: https://arxiv.org/abs/2002.05709.</p>
<a id="more"></a>
<h2 id="bottleneck-of-past-work">Bottleneck of past work</h2>
<p>Although InstDisc which we talked in the last chapter proposed a nice pretext task which makes self-supervised learning seems possible in CV field, the accuracy/performance of InstDisc on image-net is not high, which means that the representation learning from InstDisc is not good.</p>
<p><img src="figure1.png" width="50%" height="50%"></p>
<p>Unlike other CV papers that put network structure or images in Figure 1, Figure 1 in simCLR just want to tell people that how good performance simCLR is. Actually, as the name of the paper defines, this is a very simple framework, but it is the first time that unsupervised learning can beat the benchmark supervised model on image-net, which demonstrates the power and confidence of unsupervised learning.</p>
<h2 id="model-framework">Model Framework</h2>
<p><img src="figure2.png" width="50%" height="50%"></p>
<p>If Figure 2 is hard to understand, here is a structure drawn by me.</p>
<p><img src="simCLR_framework.jpg" width="50%" height="50%"></p>
<p>The difference from InstDisc to simCLR is that it straightly uses images in the same batch as negative samples instead of using memory bank. In this way, all image vectors are obtained from the same encoder every time, which naturally solves the problem of inconsistent feature distribution in InstDisc.</p>
<h2 id="model-insights">Model Insights</h2>
<blockquote>
<ol type="1">
<li>Composition of multiple data augmentation operations is crucial in defining the contrastive prediction tasks that yield effective representations. In addition, unsupervised contrastive learning benefits from stronger data augmentation than supervised learning.</li>
<li>Introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations.</li>
<li>Representation learning with contrastive cross entropy loss benefits from normalized embeddings and an appropriately adjusted temperature parameter.</li>
<li>Contrastive learning benefits from larger batch sizes and longer training compared to its supervised counterpart. Like supervised learning, contrastive learning benefits from deeper and wider networks.</li>
</ol>
</blockquote>
<p>These 4 insights are the most important essence of this paper. We will discuss in detail one by one.</p>
<h3 id="data-augmentation">Data augmentation</h3>
<p>The first insight demonstrates that data guamentation is an important condition for a successful training in contrastive learning.</p>
<p><img src="augmentation.png" width="50%" height="50%"></p>
<p>Figure 5 shows that the combination of <strong>random cropping and random color distortion</strong> is the best data augmentation for improving accuracy. This result makes almost all subsequent comparative learning work use this data augmentation strategy.</p>
<p>Moreover, it conjectures that:</p>
<blockquote>
<p>one serious issue when using only random cropping as data augmentation is that most patches from an image share a similar color distribution. Neural nets may exploit this shortcut to solve the predictive task. Therefore, it is critical to compose cropping with color distortion in order to learn generalizable features.</p>
</blockquote>
<p>The original paper shows very detailed augmentation strategy and experiment results to prove this point in chapter 3.1, 3.2.</p>
<h3 id="projection">Projection</h3>
<p><img src="projector.png" width="50%" height="50%"></p>
<p>Projection is just a single dense layer with ReLu and is used bewteen the output of encoder and the input of loss function. It can make the outputs of encoder smoother. The experiment results show that add a nonlinear projector(one dense layer + Relu) improves accuracy a lot.</p>
<blockquote>
<p>We observe that a nonlinear projection is better than a linear projection (+3%), and much better than no projection (&gt;10%).</p>
</blockquote>
<p>Another interesting result is that:</p>
<blockquote>
<p>Furthermore, even when nonlinear projection is used, the layer before the projection head, h, is still much better (&gt;10%) than the layer after, z = g(h), which shows that <em>the hidden layer before the projection head is a better representation than the layer after</em>.</p>
</blockquote>
<p>which means that projection is really usefull for contrastive learning.</p>
<h3 id="hyperparameters">Hyperparameters</h3>
<p><img src="hyperparameters.png" width="50%" height="50%"></p>
<p>Table 5 shows that temperature <span class="math inline">\(\tau\)</span> has a strong effect on accuracy. Also, if model is trained without l2 norm, it may overfit (much higher in contrastive acc but low acc in downstream tasks).</p>
<p>In conclusion, this part talks more about tricks. It proves that <span class="math inline">\(\tau\)</span> is an important hyper parameter and l2 norm help representation learning.</p>
<p>Actually, this chapter in the paper also describes about the loss function. But I decided to dedicate a specialized article for all loss functions in contrastive learning. We will talk it in that article.</p>
<h3 id="huge-negative-samples">Huge negative samples</h3>
<p><img src="negative_size.png" width="50%" height="50%"></p>
<p>Figure 9 shows that the accuracy of simCLR increases significantly with the increase of batch size. Since the size of batch size is the size of negative samples, the paper says that a huge negative sample set is the key to improving the performance of contrastive learning.</p>
<p>However, if you use a batch size which is more than 1024 to train, it requires huge calculation recourses and many tricks to make it start training. Hence, simCLR is hard to preproduce by ourselves and hard to land in real scene.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Although simCLR almost can not be preproduced and landed, it proves many useful tricks and insights for contrastive learning. Hence, simCLR has a huge impact on follow-up research due to its easy framework and strong results.</p>
]]></content>
      <categories>
        <category>Contrastive Learning</category>
      </categories>
      <tags>
        <tag>self-supervised learning</tag>
        <tag>computer vision</tag>
      </tags>
  </entry>
  <entry>
    <title>DATA7202 å¤šå…ƒçº¿æ€§å›å½’</title>
    <url>/2020/02/29/DATA7202-1st-Week-Review/</url>
    <content><![CDATA[<p>UQ 2020 Semester 1 DATA7202 <em>Statistical Methods for Data Science</em></p>
<p>å¤šå…ƒçº¿æ€§å›å½’å¤ä¹ ç¬”è®°ï¼ˆä¸ä¸€å®šå¯¹ï¼‰ï¼Œçœç•¥äº†å¾ˆå¤šè¯æ˜ï¼Œæˆ‘èƒ½å’‹åŠï¼Œæˆ‘ä¹Ÿçœ‹ä¸æ‡‚å‘€</p>
<p>å¥½æ–‡ï¼š</p>
<p>https://zhuanlan.zhihu.com/p/25436791</p>
<p>https://www.cnblogs.com/nxld/p/6435677.html</p>
<p>https://zhuanlan.zhihu.com/p/48541799ï¼ˆå‡ ä¹æ‰€æœ‰æ•°å­¦è¯æ˜å¯ä»¥æ‰¾åˆ°ï¼‰</p>
<a id="more"></a>
<h1 id="ç¦»æ•£å’Œè¿ç»­">ç¦»æ•£å’Œè¿ç»­</h1>
<p>å¦‚æœéšæœºå˜é‡çš„å€¼å¯ä»¥éƒ½å¯ä»¥é€ä¸ªåˆ—ä¸¾å‡ºæ¥ï¼Œåˆ™ä¸ºç¦»æ•£å‹éšæœºå˜é‡ã€‚å¦‚æœéšæœºå˜é‡Xçš„å–å€¼æ— æ³•é€ä¸ªåˆ—ä¸¾åˆ™ä¸ºè¿ç»­å‹å˜é‡ã€‚</p>
<h1 id="æå¤§ä¼¼ç„¶ä¼°è®¡mle">æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰</h1>
<h2 id="ä¼¼ç„¶">ä¼¼ç„¶</h2>
<p>å‡å¦‚æˆ‘ä»¬æŠ›ç¡¬å¸100æ¬¡ï¼Œæœ‰80æ¬¡æœä¸Šçš„ï¼Œæˆ‘ä»¬çŒœæµ‹æŠ›ç¡¬å¸ç»“æœæœä¸Šçš„æ¦‚ç‡ä¸º80%ã€‚</p>
<p>è¿™å°±æ˜¯<strong>ä¼¼ç„¶</strong>ï¼ æˆ‘ä»¬é€šè¿‡<strong>äº‹å®</strong>ï¼Œæ¥åæ¨æµ‹å‡ºè¿™ä¸ª<strong>äº‹ä»¶</strong>çš„å„ç§å‚æ•°ã€‚ï¼ˆæŠ›ç¡¬å¸æ˜¯æ¦‚ç‡ï¼Œä¹Ÿå¯ä»¥æ˜¯å‡å€¼ï¼Œå¯ä»¥æ˜¯æ–¹å·®ç­‰ç­‰ï¼‰</p>
<p>è®¡ç®—ä¸€ä¸ªäº‹ä»¶çš„æ¦‚ç‡éœ€è¦<strong>å‚æ•°</strong>å’Œ<strong>æ•°æ®</strong></p>
<p>æ¯”å¦‚ä¸€ä¸ªäº‹ä»¶çš„æ•°æ®æ˜¯<span class="math inline">\((x_1,â€¦, x_n)\)</span>, å®ƒçš„å‚æ•°æ˜¯<span class="math inline">\((\theta_1,â€¦,\theta_k)\)</span>:</p>
<p>å®ƒçš„<strong>æ¦‚ç‡å‡½æ•°</strong>è¡¨ç¤ºä¸º <span class="math inline">\(f\left(x_1,â€¦,x_{i} ; \theta_{1}, \dots, \theta_{k}\right)\)</span>, ä¹Ÿè¡¨ç¤ºä¸º<span class="math inline">\(f\left(x_1,â€¦,x_{i} {ï½œ} \theta_{1}, \dots, \theta_{k}\right)\)</span>,</p>
<p>æ­¤æ—¶å‚æ•°å·²çŸ¥ï¼Œæ•°æ®æœªçŸ¥ï¼Œå½“ä½ çŸ¥é“äº†æ•°æ®ï¼Œä½ å°±å¯ä»¥è®¡ç®—å‡ºæ¦‚ç‡ã€‚</p>
<p>å®ƒçš„<strong>ä¼¼ç„¶å‡½æ•°</strong>è¡¨ç¤ºä¸º <span class="math inline">\(L_{n}\left(\theta_{1}, \ldots, \theta_{k} ; x_{1}, \ldots, x_{n}\right)\)</span></p>
<p>æ­¤æ—¶æ•°æ®å·²çŸ¥ï¼Œå‚æ•°æœªçŸ¥ã€‚</p>
<p>å®ƒä»¬çš„å…¬å¼ç›¸ç­‰éƒ½è¡¨ç¤ºï¼Œéƒ½æ˜¯è®¡ç®—å‡ºäº‹ä»¶å‘ç”Ÿæ¦‚ç‡çš„å…¬å¼ã€‚</p>
<h2 id="mle">MLE</h2>
<p>æå¤§ä¼¼ç„¶ä¼°è®¡çš„ç†å¿µå°±æ˜¯é€‰å–æœ€æœ‰å¯èƒ½çš„ä¼¼ç„¶ï¼</p>
<p>ç¿»è¯‘ä¸€ä¸‹å°±æ˜¯æ±‚å‡º<span class="math inline">\(L_{n}\left(\theta_{1}, \ldots, \theta_{k} ; x_{1}, \ldots, x_{n}\right)\)</span> æ¯ä¸ªå‚æ•°<span class="math inline">\(\theta\)</span> çš„æœ€å¤§å€¼ï¼</p>
<p>æœ€å¤§çš„å‚æ•°å€¼çš„æ„ä¹‰ä»£è¡¨ç€æœ€æœ‰å¯èƒ½æ˜¯è¿™ä¸ªäº‹ä»¶çš„çœŸå®å‚æ•°ï¼</p>
<p>æ€ä¹ˆæ±‚å‘¢ï¼Ÿ</p>
<p>æå¤§ä¼¼ç„¶ä¼°è®¡é¦–å…ˆå‡è®¾äº‹ä»¶ç»“æœæ¯ä¸€æ¬¡éƒ½æ˜¯<strong>ç‹¬ç«‹</strong>çš„ã€‚</p>
<p>æ ¹æ®ç‹¬ç«‹æ€§è´¨ï¼Œ<span class="math inline">\(f\left(x_1,â€¦,x_{i} {ï½œ} \theta_{1}, \dots, \theta_{k}\right) = \prod_{i=1}^{n} f\left(x_{i} ; \theta_{1}, \ldots, \theta_{k}\right)\)</span></p>
<p><span class="math inline">\(\prod\)</span> è¡¨ç¤ºè¿ä¹˜å·ï¼Œä¾‹å¦‚ï¼š$_{i=1}^{n} x_i = x_1 x_2 â€¦ x_n $</p>
<p>å› æ­¤æˆ‘ä»¬æœ‰äº†æå¤§ä¼¼ç„¶ä¼°è®¡çš„å…¬å¼ï¼š</p>
<p>$L_{n}(<em>{1}, , </em>{k} ; x_{1}, , x_{n}) = <em>{i=1}^{n} f(x</em>{i} ; <em>{1}, , </em>{k}) $</p>
<h2 id="ä¾‹å­æŠ›ç¡¬å¸">ä¾‹å­ï¼ˆæŠ›ç¡¬å¸ï¼‰</h2>
<p>æŠ›10æšç¡¬å¸ï¼Œè®°å½•å‡ºç°æ­£é¢çš„æ¬¡æ•°ã€‚</p>
<p>å‡å¦‚å‡ºç°æ­£é¢çš„æ¬¡æ•°ä¸º4.</p>
<p>è®¡ç®—æ¦‚ç‡ï¼š</p>
<p>å³æ±‚ <span class="math inline">\(f(4|\theta)\)</span></p>
<p>æ ¹æ®äºŒé¡¹åˆ†å¸ƒå®šç†ï¼Œ<span class="math inline">\(f(x|\theta) = \binom {10}{4} \theta^4 (1-\theta)^6\)</span></p>
<p>æˆ‘ä»¬è¦æ±‚å®ƒçš„æœ€å¤§å€¼ï¼š</p>
<ol type="1">
<li>æ±‚å¯¼ï¼š</li>
</ol>
<p>å¯¹<span class="math inline">\(theta\)</span>æ±‚å¯¼</p>
<p>$ =  (4<sup>3(1-)</sup>6-6<sup>4(1-)</sup>5)$</p>
<ol start="2" type="1">
<li><p>ä»¤å…¶ç­‰äº0</p>
<p>$4(1-)-6= 0 $</p>
<p><span class="math inline">\(\theta = 0.4\)</span></p></li>
</ol>
<p>å‡è®¾æˆ‘ä»¬ä¸çŸ¥é“æŠ•ç¡¬å¸çš„æ¦‚ç‡ï¼Œä»æŠ›10æ¬¡å¾—åˆ°4æ¬¡æ˜¯æ­£é¢çš„ç»“æœæ¥çœ‹ï¼Œæ½œæ„è¯†ä¹Ÿä¼šè®¤ä¸ºæ˜¯40%ã€‚</p>
<h2 id="ä¾‹å­æ­£æ€åˆ†å¸ƒ">ä¾‹å­ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰</h2>
<p>ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡è¯æ˜æ­£æ€åˆ†å¸ƒçš„å‚æ•°<span class="math inline">\(\muï¼Œ\sigma^2\)</span> æ˜¯å¹³å‡å€¼å’Œæ–¹å·®ã€‚</p>
<p><span class="math inline">\(f\left(x ; \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}\)</span></p>
<p>æ‰‹ç®—æˆ–è€…ç½‘ä¸Šå¥½å¤šï¼Œå†æ‰“ä¸€élatexæœ‰ç‚¹å¤šã€‚</p>
<p>è¯æ˜æ— åå’Œæœ‰åçš„æˆ‘è¿˜ä¸æ‡‚</p>
<h1 id="å¤šå…ƒçº¿æ€§å›å½’">å¤šå…ƒçº¿æ€§å›å½’</h1>
<p><img src="/images/data7202_1st_3.jpg"></p>
<p>pæ˜¯coefficentçš„æ•°é‡ï¼Œå³<span class="math inline">\(\Beta\)</span>çš„æ•°é‡ï¼Œnæ˜¯æœ‰å¤šå°‘æ¡æ•°æ®ï¼Œå³<span class="math inline">\(X\)</span>çŸ©é˜µçš„è¡Œæ•°ã€‚</p>
<p>ä»¥åå†ä¸¾ä¾‹å­ï¼Œæˆ‘ç¿äº†ã€‚</p>
<h2 id="æ¨å¯¼">æ¨å¯¼</h2>
<p><span class="math inline">\(\boldsymbol{Y}=\boldsymbol{X} \boldsymbol{\beta}+\boldsymbol{\varepsilon}\)</span></p>
<p>æ ¹æ®æœ€å°äºŒä¹˜æ³•ï¼Œæå¤§ä¼¼ç„¶ä¼°è®¡æˆ–è€…error functionéƒ½èƒ½è®¡ç®—å‡º <span class="math inline">\(\boldsymbol{\beta}\)</span>ï¼š<span class="math inline">\(\left(X^{\top} X\right)^{-1} X^{\top} Y\)</span></p>
<p>æˆ‘ä»¬çš„é¢„æµ‹å€¼ä¸ºï¼š<span class="math inline">\(\widehat{Y}=X \widehat{\boldsymbol{\beta}} = X\left(X^{\top} X\right)^{-1} X^{\top} Y\)</span></p>
<p>ç§°<span class="math inline">\(X\left(X^{\top} X\right)^{-1} X^{\top}\)</span> ä¸º<strong>hat matrix</strong>ï¼Œæˆ–<strong>H</strong></p>
<h2 id="h-matrix">H matrix</h2>
<p><span class="math inline">\(H = X\left(X^{\top} X\right)^{-1} X^{\top}\)</span></p>
<p><span class="math inline">\(H\)</span> æ˜¯ä¸€ä¸ªå¯¹ç§°å’Œidempotentï¼ˆå¹‚ç­‰ï¼‰çŸ©é˜µ</p>
<p>ä¸€ä¸ªçŸ©é˜µAæ˜¯idempotentçš„æ„æ€ä¸º <span class="math inline">\(A^2 = A\)</span></p>
<p>è¯æ˜Hæ˜¯idempotentçš„ï¼š</p>
<p><span class="math inline">\(H^2 = X\left(X^{\top} X\right)^{-1} X^{\top} (X\left(X^{\top} X\right)^{-1} X^{\top}) = X(X^{\top} X^{-1} (X^{\top} X))X^{\top} X^{-1} X^{\top}\)</span></p>
<p>å› ä¸º <span class="math inline">\(X^{\top} X^{-1} (X^{\top} X) = I\)</span></p>
<p>å³ $ H^2 = H$</p>
<p>è¯æ˜<span class="math inline">\(H\)</span>æ˜¯å¯¹ç§°çš„ï¼š</p>
<p>è¿˜ä¸ä¼šï¼Œç½‘ä¸Šéƒ½è¯´æ˜¾ç„¶å¯çŸ¥ã€‚ã€‚ã€‚</p>
<p>##æ®‹å·®</p>
<p>æ®‹å·®residualä¸ºï¼š<span class="math inline">\(e = Y-\widehat{Y}=\left(1_{n}-H\right) Y\)</span></p>
<p>è¯æ˜ï¼š<span class="math inline">\(\operatorname{Var}(\mathrm{e})=\left(\mathrm{I}_{n}-\mathrm{H}\right)[\operatorname{Var}(\mathrm{Y})]\left(\mathrm{I}_{n}-\mathrm{H}\right)^{\top}=\sigma^{2}\left(\mathrm{I}_{n}-\mathrm{H}\right)\)</span></p>
<p>ä»¤<span class="math inline">\(\mathrm{I}_{n}-\mathrm{H} = A\)</span></p>
<p>$() = E((A(Y-))(A(Y-))^T) $</p>
<p><span class="math inline">\(= E(A(Y-\mu)(Y- \mu)^TA^T)\)</span></p>
<p><span class="math inline">\(= AE((Y-\mu)(Y- \mu)^T)A^T\)</span></p>
<p><span class="math inline">\(= A\operatorname{Var}(\mathrm{Y})A^T\)</span></p>
<p><span class="math inline">\(= (\mathrm{I}_{n}-\mathrm{H})\operatorname{Var}(\mathrm{Y})(\mathrm{I}_{n}-\mathrm{H})^T\)</span></p>
<p>å› ä¸º<span class="math inline">\((\mathrm{I}_{n}-\mathrm{H})(\mathrm{I}_{n}-\mathrm{H})^T = (\mathrm{I}_{n}-\mathrm{H})\)</span> ï¼ˆæ ¹æ®å¯¹ç§°ï¼Œè½¬ç½®ç­‰äºæœ¬èº«ã€‚å†æ ¹æ®å¹‚ç­‰ï¼Œå¹³æ–¹ç­‰äºæœ¬èº«ï¼‰</p>
<p><span class="math inline">\(= \sigma^2(\mathrm{I}_{n}-\mathrm{H})\)</span></p>
<p>å› æ­¤<span class="math inline">\(e_{i}=\sigma^{2}\left(1-h_{i i}\right)\)</span> ,<span class="math inline">\(h_{ii}\)</span> æ˜¯<span class="math inline">\(H\)</span> çš„å¯¹è§’å…ƒï¼Œä¹Ÿå«<strong>æ æ†å€¼ï¼ˆleverageï¼‰</strong></p>
<h2 id="leverageæ æ†å€¼">leverageæ æ†å€¼</h2>
<p><span class="math inline">\(h_{ii}\)</span> ä¸º<span class="math inline">\(H\)</span>çŸ©é˜µå¯¹è§’çº¿ä¸Šçš„å€¼ï¼Œå®ƒèƒ½å¾ˆå¤§çš„å½±å“é¢„æµ‹å€¼ï¼ŒåŸå› å¦‚å›¾ï¼š</p>
<p><img src="/images/data7202_1st_8.jpg"></p>
<p>ç”±äº<span class="math inline">\(\sum_{i=1}^{n} h_{i i}=p\)</span>ï¼ˆå¥½åƒæ˜¯æ ¹æ®traceï¼Œå¯¹ç§°å¹‚ç­‰çŸ©é˜µå…¬å¼æ¨å¯¼çš„ï¼Œæˆ‘ç¿äº†ï¼‰ï¼š</p>
<p>å› æ­¤å¦‚æœ<span class="math inline">\(h_{ii} &gt; 2p/n\)</span>, åˆ™ç§°è§‚æµ‹å€¼<span class="math inline">\(x_i\)</span> ä¸ºå¼‚å¸¸å€¼ã€‚</p>
<p><img src="/images/data7202_1st_6.jpg"></p>
<p>å¦‚æœStudentized residualçš„ç»å¯¹å€¼éå¸¸å¤§ï¼Œä¾‹å¦‚<span class="math inline">\(\left|r_{i}^{*}\right|&gt;2\)</span> ï¼Œä¹Ÿå¯ä»¥è¯´<span class="math inline">\(x_i\)</span>æ˜¯å¼‚å¸¸å€¼ã€‚</p>
<h2 id="model-assumption">Model assumption</h2>
<p>æ®‹å·®å¯¹äºmodel assumptionå¾ˆé‡è¦ï¼Œå®ƒå¿…é¡»æ»¡è¶³ä¸‹é¢å››æ¡æ€§è´¨ã€‚</p>
<h3 id="normalityqq-plot">Normality(QQ-PLOT)</h3>
<p>æ®‹å·®çš„åˆ†å¸ƒåº”è¯¥æ¥è¿‘æ­£æ€åˆ†å¸ƒï¼ˆå‡å€¼ä¸º0ï¼‰</p>
<p>å¯ä»¥ä½¿ç”¨qq-plotæ¥åˆ¤æ–­æ•°æ®é›†æ˜¯å¦æœä»æ­£æ€åˆ†å¸ƒï¼Œå½“qqplotçš„å›¾è¿‘ä¼¼ä¸ºä¸€æ¡45åº¦çš„ç›´çº¿æ—¶ï¼Œåˆ™è¯´æ˜è¯¥æ•°æ®æœä»æ­£æ€åˆ†å¸ƒã€‚</p>
<p><img src="/images/data7202_1st_3.jpg"></p>
<p>ä¸Šå›¾å·¦åŠéƒ¨åˆ†è¿‘ä¼¼æ­£æ€åˆ†å¸ƒï¼Œå› æ­¤qqplotä¸ºä¸€æ¡45åº¦ç›´çº¿ã€‚è€Œå³åŠåˆ†å¸ƒåˆ™ä¸æ˜¯ã€‚</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">qqnorm(x)</span><br><span class="line">qqline(x)</span><br></pre></td></tr></table></figure>
<h3 id="linearity">Linearity</h3>
<p>æ¨¡å‹åº”è¯¥æ•è·æ‰€æœ‰ç³»ç»Ÿæ•°æ®ä¸­å­˜åœ¨çš„æ–¹å·®ï¼Œåªç•™ä¸‹éšæœºå™ªå£°ã€‚</p>
<p>æˆ–è€…è¯´ä½œå‡ºçš„å›¾åƒè¿‘ä¼¼ä¸€æ¡ç›´çº¿ã€‚è¯´æ˜å¯ä»¥å’Œæˆ‘ä»¬å‡è®¾æ˜¯çº¿æ€§ç›¸å…³ä¸€è‡´ã€‚</p>
<p>###homoscedasticity(constant variance)</p>
<p>å‡è®¾æˆ‘ä»¬æœ‰å®¶åº­æ”¶å…¥å’Œå¥¢ä¾ˆå“æ”¯å‡ºçš„æ•°æ®ã€‚ä½¿ç”¨äºŒå…ƒå›å½’ï¼Œæˆ‘ä»¬ä½¿ç”¨å®¶åº­æ”¶å…¥æ¥é¢„æµ‹å¥¢ä¾ˆå“æ¶ˆè´¹ã€‚ä¸å‡ºæ‰€æ–™ï¼Œæ”¶å…¥ä¸æ”¯å‡ºä¹‹é—´æœ‰å¾ˆå¼ºçš„ç§¯æè”ç³»ã€‚é€šè¿‡æ£€æŸ¥æ®‹å·®ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªé—®é¢˜â€“æ®‹å·®å¯¹äºå®¶åº­æ”¶å…¥è¾ƒä½çš„å€¼å¾ˆå°ï¼ˆå‡ ä¹æ‰€æœ‰ä½æ”¶å…¥å®¶åº­åœ¨å¥¢ä¾ˆå“ä¸Šçš„èŠ±è´¹éƒ½ä¸å¤šï¼‰ï¼Œè€Œè¾ƒå¯Œè£•å®¶åº­çš„æ®‹å·®å¤§å°å´å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼ˆæœ‰äº›å®¶åº­åœ¨å¥¢ä¾ˆå“ä¸ŠèŠ±äº†å¾ˆå¤šé’±ï¼Œè€Œæœ‰äº›å®¶åº­çš„å¥¢ä¾ˆå“èŠ±é”€åˆ™æ¯”è¾ƒä¸­ç­‰ï¼‰ã€‚è¿™ç§æƒ…å†µè¡¨ç¤ºå¼‚æ–¹å·®æ€§ï¼Œå› ä¸ºè¯¯å·®çš„å¤§å°éšè‡ªå˜é‡çš„å€¼è€Œå˜åŒ–ã€‚</p>
<p>å¦‚å›¾æ‰€ç¤ºï¼š</p>
<p><img src="/images/data7202_1st_1.jpg"></p>
<p>æˆ‘ä»¬å‘ç°ä¸Šè¿°ä¾‹å­å°±å¯ä»¥å¯¹åº”â€œnonconstant varianceâ€å›¾ï¼Œå³æ–¹å·®å’Œæ®‹å·®ä¹‹é—´å­˜åœ¨å…³è”æ€§ã€‚</p>
<p>â€œcontant varianceâ€œ å›¾æ®‹å·®å¯¹äºè‡ªå˜é‡æˆ–è€…é¢„æµ‹å€¼éƒ½æ²¡æœ‰å…³è”æ€§ï¼Œè¯´æ˜å°±æ˜¯éšæœºè¯¯å·®ï¼Œæ˜¯æˆ‘ä»¬æƒ³è¦çš„ç»“æœã€‚</p>
<p>ç›´æ¥æµ‹è¯•ç¬¦ä¸ç¬¦åˆhomoscedasticityçš„ä»£ç ä¸º</p>
<p><img src="/images/data7202_1st_4.jpg"></p>
<p><strong>suggested power transformation</strong> çš„ç”¨å¤„ï¼š</p>
<p>suggested power transformationå€¼ä¸º<span class="math inline">\(p\)</span>, <span class="math inline">\(Y^{p}\)</span>æ›´èƒ½æ»¡è¶³homoscedasticityæ€§ã€‚ï¼ˆ<span class="math inline">\(Y\)</span> å°±æ˜¯çœŸå®å€¼ï¼‰</p>
<p>ä¾‹å­ï¼šsuggested power transformationå€¼ä¸º0.5ï¼Œé‚£ä¹ˆä½¿ç”¨ <span class="math inline">\(Y^{0.5}\)</span> æ¯”ä½¿ç”¨<span class="math inline">\(Y\)</span> æ›´èƒ½æ»¡è¶³homoscedasticityæ€§ï¼Œæ­¤æ—¶æ®‹å·®<span class="math inline">\(e = Y^{0.5}-\widehat{Y}\)</span></p>
<h3 id="independence">independence</h3>
<p><img src="/images/data7202_1st_2.jpg"></p>
<p>å¦‚æœæ®‹å·®æ ¹æ®æ—¶é—´å¹¶æ²¡æœ‰æ˜æ˜¾çº¿æ€§å…³ç³»æ—¶ï¼Œè¯´æ˜æ®‹å·®å…·æœ‰ç‹¬ç«‹æ€§ã€‚</p>
<p>æ—¶é—´çš„æ„æ€æ˜¯è‡ªå˜é‡è¾“å…¥çš„é¡ºåºã€‚</p>
<h1 id="å…·ä½“æ“ä½œä¾‹å­">å…·ä½“æ“ä½œä¾‹å­</h1>
<p>æ•°æ®ï¼šä¸­é£æ‚£è€…å‡ºé™¢ååº·å¤æƒ…å†µ</p>
<p>åº”å˜é‡ï¼ˆ<span class="math inline">\(Y\)</span>): åº·å¤å‡ºé™¢æ—¶çš„æ­¥è¡Œé€Ÿåº¦</p>
<p>æœ‰120ç»„æ•°æ®ï¼Œ17ä¸ªè‡ªå˜é‡å±æ€§ã€‚</p>
<p>ç›®çš„ï¼š</p>
<ol type="1">
<li>å°è¯•é¢„æµ‹å‡ºé™¢æ—¶çš„æ­¥è¡Œé€Ÿåº¦</li>
<li>æ‰¾å‡ºå“ªäº›è‡ªå˜é‡æœ€èƒ½å½±å“é¢„æµ‹å€¼</li>
</ol>
<p>å˜é‡çš„å«ä¹‰å°±ä¸ä»‹ç»äº†</p>
<h2 id="å˜é‡å¤„ç†">å˜é‡å¤„ç†</h2>
<p>å¯¹äºåˆ†ç±»å‹å˜é‡ï¼Œä¸€èˆ¬æˆ‘ä»¬å°†å…¶å¤„ç†ä¸ºdummy variablesï¼ˆå“‘å˜é‡ï¼‰</p>
<p>åªæœ‰2ä¸ªåˆ†ç±»æ—¶ï¼Œä¾‹å¦‚æ€§åˆ«ï¼šæˆ‘ä»¬å°†female = 1ï¼Œ male = 0</p>
<p>åˆ†ç±»æ•°é‡å¤§äº2æ—¶ï¼Œä¾‹å¦‚ä¾‹å­ä¸­çš„ï¼ˆside of strokeï¼‰å˜é‡å«æœ‰â€œleftâ€ï¼Œâ€œrightâ€ï¼Œâ€œotherâ€ä¸‰ä¸ªåˆ†ç±»</p>
<p>å°†å…¶å˜ä¸ºï¼šâ€œleftâ€ = (1 0 0), â€œrightâ€ = (0 1 0), â€œotherâ€ = (0 0 1)</p>
<p>å®é™…ä¸Šæˆ‘ä»¬å¯ä»¥å°†3ç±»çš„è¿›è¡Œä¼˜åŒ–ï¼Œä¾‹å¦‚ï¼šâ€œleftâ€ = (1 0), â€œrightâ€ = (0 1), â€œotherâ€ = (0 0)</p>
<p>æ—¢ä¸æ˜¯leftä¹Ÿä¸æ˜¯rightçš„è‡ªç„¶å°±æ˜¯å¦ä¸€ä¸ªåˆ†ç±»ã€‚</p>
<p>è¿™æ ·åšçš„ç›®çš„å¯ä»¥å‡å°‘ç©ºé—´ã€‚</p>
<h2 id="ç¬¬ä¸€æ­¥çº¿æ€§æ£€æŸ¥linearity">ç¬¬ä¸€æ­¥ï¼šçº¿æ€§æ£€æŸ¥ï¼ˆlinearityï¼‰</h2>
<p>æˆ‘ä»¬å¯¹æ‰€æœ‰å˜é‡ç”»ä¸€æ¬¡å•å˜é‡-å› å˜é‡æ•£ç‚¹å›¾ï¼Œè§‚å¯Ÿå…¶æ˜¯å¦å…·æœ‰çº¿æ€§ã€‚</p>
<p>å› ä¸ºå…·æœ‰çº¿æ€§æ‰èƒ½ç¬¦åˆæˆ‘ä»¬çš„å‡è®¾ï¼Œæˆ‘ä»¬çš„å…¬å¼ã€‚</p>
<h2 id="ç¬¬äºŒæ­¥ç›¸å…³æ€§">ç¬¬äºŒæ­¥ï¼šç›¸å…³æ€§</h2>
<p>ä¸ºä»€ä¹ˆå±æ€§å¯ä»¥å½±å“åˆ°å› å˜é‡ï¼ˆè¡Œèµ°é€Ÿåº¦ï¼‰å‘¢ï¼Ÿ</p>
<p>å®ƒå¯èƒ½æ˜¯ç›¸å…³çš„ï¼Œä½†æ˜¯ç›¸å…³å¹¶ä¸æ„å‘³ç€å› æœå…³ç³»ã€‚ è¿™äº›å˜é‡ä¸­æœ‰è®¸å¤šç›¸äº’å…³è”ï¼Œå› ä¸ºå…¶ä»–å› ç´ ï¼ˆä¾‹å¦‚ï¼Œè¿åŠ¨åŠŸèƒ½å—æŸçš„ç¨‹åº¦ï¼‰ä»¥ç›¸ä¼¼çš„æ–¹å¼å½±å“å…¶ä¸­ä¸€ä¸ªä»¥ä¸Šçš„å› ç´ ã€‚</p>
<p>æˆ‘ä»¬ä»ç„¶å¸Œæœ›çœ‹åˆ°ä¼¼ä¹ä¸å‡ºé™¢æ­¥è¡Œé€Ÿåº¦ç›´æ¥ç›¸å…³çš„è§£é‡Šå˜é‡ä¼šæˆä¸ºå¤šå…ƒå›å½’åˆ†æä¸­çš„æœ€ä½³é¢„æµ‹å˜é‡ã€‚</p>
<h2 id="ç¬¬ä¸‰æ­¥ç¼ºå¤±æ•°æ®">ç¬¬ä¸‰æ­¥ï¼šç¼ºå¤±æ•°æ®</h2>
<p>åœ¨120ç»„æ•°æ®ä¸­åªæœ‰85ç»„æ‹¥æœ‰å®Œæ•´çš„æ‰€æœ‰å˜é‡ã€‚</p>
<p>ç„¶è€Œå¤§å¤šæ•°çš„å¤šå…ƒçº¿æ€§å›å½’ä¼šç›´æ¥å»æ‰å«æœ‰æœªçŸ¥é‡çš„æ•°æ®ç»„ã€‚</p>
<p>è¿™å¤§å¤§ç¼©å°äº†æ ·æœ¬å¤§å°ã€‚é¼“åŠ±æˆ‘ä»¬å°½å¯èƒ½ä½¿ç”¨è¾ƒå°‘çš„è§£é‡Šå˜é‡ã€‚</p>
<h3 id="å¯¹è§£é‡Šå˜é‡çš„åˆå§‹å‰”é™¤">å¯¹è§£é‡Šå˜é‡çš„åˆå§‹å‰”é™¤</h3>
<p>å¯¹æ¯ä¸€ä¸ªè§£é‡Šå˜é‡éƒ½åšä¸€æ¬¡ç®€å•çº¿æ€§å›å½’ï¼Œä»…ä¿ç•™é‚£äº›æ˜¾è‘—å…³è”å“åº”å˜é‡çš„è§£é‡Šå˜é‡ã€‚</p>
<p>é€šè¿‡è¿™ä¸ªæ“ä½œï¼Œæˆ‘ä»¬åˆ é™¤äº†â€œFIM cognitiveâ€ï¼Œâ€œMAS 6-8â€œï¼Œâ€œageâ€ï¼Œâ€œgenderâ€ï¼Œâ€œside of strokeâ€ç­‰å˜é‡ã€‚</p>
<h2 id="ç¬¬å››æ­¥çº¿æ€§å›å½’">ç¬¬å››æ­¥ï¼šçº¿æ€§å›å½’</h2>
<p>å¯¹è§£é‡Šå˜é‡ &quot;FIM motor scoreâ€ å’Œâ€œ walking speed at admissionâ€åˆ†åˆ«åšç®€å•çº¿æ€§å›å½’ï¼Œå¦‚ä¸‹ï¼š</p>
<p><img src="/images/data7202_1st_7.jpg"></p>
<h3 id="p-value">p-value</h3>
<p>å½“p-valueå°äºç»™å®š<span class="math inline">\(\alpha\)</span>, ä¸€èˆ¬ä¸º0.005æˆ–0.001æ—¶ï¼Œåˆ™ç§°è¯¥å˜é‡æ˜¯å¦æœ‰æ„ä¹‰ã€‚ï¼ˆè¶Šå°è¶Šå¥½ï¼‰</p>
<p>å…·ä½“ä¸€ç‚¹ï¼š</p>
<p>é›¶å‡è®¾ï¼šå¯¹äº<span class="math inline">\(x_j\)</span>, <span class="math inline">\(\beta_j = 0\)</span> å«ä¹‰å°±æ˜¯<span class="math inline">\(x_j\)</span>è¿™ä¸ªè§£é‡Šå˜é‡å¯¹äºå“åº”å˜é‡æ¯«æ— å½±å“ã€‚</p>
<p>å¤‡æ‹©å‡è®¾ï¼š<span class="math inline">\(\beta_j \neq 0\)</span></p>
<p>å¦‚ä½•è®¡ç®—ï¼Ÿ</p>
<p>æˆ‘ä»¬è®¡ç®—<span class="math inline">\(\hat{\mathrm{y}}=\mathrm{b}_{0}+\mathrm{b}_{1} \mathrm{x}_{1}+\mathrm{b}_{2} \mathrm{x}_{2}+\ldots+\mathrm{b}_{\mathrm{d}} \mathrm{x}_{\mathrm{d}}\)</span> (æ’é™¤æ‰<span class="math inline">\(\mathrm{b}_{j}\)</span>)ï¼Œæ¯”è¾ƒè¿™ä¸ªå€¼äº§ç”Ÿçš„errorå’ŒåŒ…æ‹¬<span class="math inline">\(\mathrm{b}_{j}\)</span>æ—¶çš„errorã€‚</p>
<p>å½“å°äºæŸä¸ªé˜ˆå€¼ï¼ˆ<span class="math inline">\(\alpha\)</span>ï¼‰æ—¶ï¼Œåˆ™è¯´æ˜<span class="math inline">\(x_j\)</span>æ˜¯ä¸€ä¸ªé‡è¦çš„è§£é‡Šå˜é‡ã€‚</p>
<p>å¤šå…ƒçº¿æ€§å›å½’ä¸­ä¸€ä¸ªè§£é‡Šå˜é‡çš„på€¼é€šå¸¸ä¼šæ¯”ç®€å•çº¿æ€§å›å½’ä¸­æ­¤å˜é‡çš„på€¼é«˜ï¼ˆè¯´æ˜æ›´å°‘çš„é‡è¦æ€§ï¼‰</p>
<p>ç”šè‡³å¯èƒ½ä»â€œé‡è¦â€è¢«åç§»åˆ°â€œä¸é‡è¦â€</p>
<p>ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ</p>
<p>å› ä¸ºé€‰æ‹©å…¶ä»–çš„å˜é‡çš„æ—¶å€™å°±å¯èƒ½å·²ç»å¸®åŠ©åˆ°å“åº”å˜é‡äº†ï¼Œæ‰€ä»¥ä»…ä»…å¢åŠ ä¸€ä¸ªå¹¶ä¸èƒ½å¾ˆæœ‰æ•ˆçš„å¸®åŠ©ã€‚</p>
<p>å› æ­¤ï¼šä»…æœ‰ä¸€äº›è§£é‡Šå˜é‡æ˜¯é‡è¦çš„ã€‚</p>
<p>æˆ‘ä»¬å¯ä»¥æ ¹æ®p-valueå€¼è¿›è¡Œæ’åºï¼Œè®²è¾ƒå¤§çš„p-valueå¯¹åº”çš„è§£é‡Šå˜é‡å‰”é™¤ã€‚</p>
<h3 id="coefficient">Coefficient</h3>
<p>â€œCoef.â€œå³æ˜¯<span class="math inline">\(\beta\)</span>çš„å€¼ï¼Œè¶Šå¤§è¯´æ˜å¯¹ç»“æœå½±å“è¶Šå¤§ã€‚ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰</p>
<p>###R-squared</p>
<p>https://zhuanlan.zhihu.com/p/47180789</p>
<h4 id="sstosse">SSTO,SSE</h4>
<p><span class="math inline">\(SSTO = \sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}\)</span></p>
<p><span class="math inline">\(y_i\)</span>æ˜¯çœŸå®å€¼ï¼Œ<span class="math inline">\(\bar{y}\)</span>æ˜¯æ ·æœ¬å‡å€¼ã€‚</p>
<p><span class="math inline">\(\mathrm{SSE}=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}\)</span></p>
<p><span class="math inline">\(\hat{y}_{i}\)</span>æ˜¯<span class="math inline">\(y\)</span>çš„é¢„æµ‹å€¼ã€‚</p>
<p><img src="/images/data7202_1st_11.jpg"></p>
<p>ä»å›¾ä¸­çœ‹å‡ºï¼š</p>
<p>æ¯ä¸ªæ•£ç‚¹å°±æ˜¯<span class="math inline">\(y\)</span>ã€‚</p>
<p>ç»¿çº¿å°±æ˜¯<span class="math inline">\(\bar{y}\)</span>,å› ä¸ºæ˜¯å¸¸æ•°ï¼Œæ‰€ä»¥æ˜¯ä¸€æ¡æ°´å¹³çš„ç›´çº¿ã€‚</p>
<p>çº¢çº¿æ˜¯<span class="math inline">\(\hat{y}_{i}\)</span>,æ˜¯ç”±çº¿æ€§å›å½’å¾—å‡ºçš„ç›´çº¿ã€‚</p>
<p>æ‰€ä»¥SSTOæ˜¯çœŸå®æƒ…å†µå¯¹äºå¹³å‡å€¼çš„ç¦»æ•£ç¨‹åº¦ï¼Œè€ŒSSEæ˜¯çœŸå®æƒ…å†µå¯¹äºéšæœºè¯¯å·®çš„ç¦»æ•£æƒ…å†µï¼ˆå¯èƒ½æ˜¯ä¸ªç—…å¥ï¼‰</p>
<p>ç”±è¿™ä¸ªä¸¤ä¸ªå€¼æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªé‡è¦çš„åˆ¤æ–­å€¼ï¼š</p>
<p><span class="math inline">\(R^{2}=\frac{\mathrm{SSTO}-\mathrm{SSE}}{\mathrm{SSTO}} = 1 - \frac{\mathrm{SSE}}{\mathrm{SSTO}}\)</span></p>
<p>å½“<span class="math inline">\(R^2\)</span>è¶Šæ¥è¿‘1ï¼Œè¯´æ˜å›å½’çº¿æ›´åŠ çš„ç²¾ç¡®ã€‚</p>
<p>ç”±äºè¿™ä¸ªæ˜¯æœ‰åä¼°è®¡ï¼ŒæŠŠå®ƒå˜ä¸ºæ— ååçš„<span class="math inline">\(R^2\)</span>å°±æ˜¯â€œAdj R_squaredâ€ï¼Œé“ç†ä»¥åå†ææ‡‚ã€‚</p>
<p>â€œAdj R_squaredâ€ = 0.8ï¼Œåˆ™è¯´æ˜äº†è§£é‡Šå˜é‡è§£é‡Šäº†80%çš„å“åº”å˜é‡çš„æ–¹å·®ã€‚ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰</p>
<h2 id="å¤šå…ƒçº¿æ€§å›å½’çš„æ¡ä»¶">å¤šå…ƒçº¿æ€§å›å½’çš„æ¡ä»¶</h2>
<ol type="1">
<li>æ¯ä¸€ä¸ªè§£é‡Šå˜é‡å’Œå“åº”å˜é‡çš„å…³ç³»éƒ½é€¼è¿‘ä¸€æ¡ç›´çº¿ï¼ˆlinearityï¼‰</li>
<li>æ²¡æœ‰é«˜å½±å“çš„å¼‚å¸¸å€¼</li>
<li>æ®‹å·®é€¼è¿‘æ­£æ€åˆ†å¸ƒ</li>
<li>æ®‹å·®å…·æœ‰constant varianveï¼ˆç”¨è¿‡ residuals vs fitted valueså›¾ï¼‰</li>
<li>æ•°æ®å€¼ç‹¬ç«‹</li>
<li>è§£é‡Šå˜é‡çš„ä¸ªæ•°på—åˆ°æ ·æœ¬å¤§å°nçš„çº¦æŸï¼Œé€šå¸¸æˆ‘ä»¬æ ¹æ®ç»éªŒæ³•åˆ™ï¼š<span class="math inline">\(p \leq n/3\)</span></li>
</ol>
<h2 id="å¤šå…ƒçº¿æ€§å›å½’å›¾">å¤šå…ƒçº¿æ€§å›å½’å›¾</h2>
<p><img src="/images/data7202_1st_9.jpg"></p>
<p>é€šå¸¸æˆ‘ä»¬ä½¿ç”¨p-valueè¿›è¡Œæ’åºï¼ˆè¶Šå°è¶Šé‡è¦ï¼‰ï¼Œå¦‚æœp-valueå€¼ç›¸åŒï¼Œåˆ™ä½¿ç”¨t-testç»“æœçš„ç»å¯¹å€¼ï¼ˆè¶Šå¤§è¶Šé‡è¦ï¼‰</p>
<p><img src="/images/data7202_1st_10.jpg"></p>
<p>æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹æ‰€æœ‰å˜é‡ä¹‹é—´çš„correlationã€‚æˆ‘ä»¬å‘ç°æœ‰äº›å˜é‡ä¹‹é—´ä¹Ÿæœ‰å¾ˆå¼ºçš„å…³è”æ€§ï¼Œè¿™å¹¶ä¸å¥½ï¼Œå¤§å¤šæ•°çš„åšæ³•æ˜¯å»æ‰å®ƒä»¬ã€‚</p>
<h2 id="æ€»ç»“">æ€»ç»“</h2>
<ol type="1">
<li>å¦‚æœéœ€è¦ï¼Œå¯ä»¥é€šè¿‡å›¾å½¢ï¼Œç›¸å…³æ€§è®¡ç®—ä»¥åŠç®€å•çš„çº¿æ€§å›å½’ä¸å“åº”è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æï¼Œä»¥æ‰¾å‡ºæœ‰å¸Œæœ›çš„å˜é‡ã€‚ä½†æ˜¯ï¼Œè¿™ä¸èƒ½å®Œå…¨é¢„æœŸå¤šå…ƒçº¿æ€§å›å½’çš„ç»“æœã€‚</li>
<li>ä¼°è®¡çš„æ–œç‡ç³»æ•°å¯ä»¥è§£é‡Šä¸ºä¸è§£é‡Šå˜é‡çš„å•ä½å¢åŠ ç›¸å…³çš„å“åº”å˜é‡çš„å¹³å‡å˜åŒ–ï¼Œè€Œå…¶ä»–è§£é‡Šå˜é‡åˆ™ä¿æŒä¸å˜ã€‚åº”è¯¥æŠ¥å‘Šå…¶ç½®ä¿¡åŒºé—´ã€‚</li>
<li>æ‰¾åˆ°å…³è”åï¼Œå› æœå…³ç³»é€šå¸¸ä»ç„¶ä¸æ¸…æ¥šï¼Œä½†å¯èƒ½å­˜åœ¨äºå˜é‡å¯¹ä¹‹é—´ã€‚è¿™å¯ä»¥æ¿€åŠ±æ—¨åœ¨æ”¹å–„ååº”å˜é‡ç»“æœçš„åç»­å¹²é¢„ç ”ç©¶ã€‚</li>
<li>åŸºäºå¤šä¸ªè§£é‡Šå˜é‡ï¼Œå¯ä»¥ä½¿ç”¨å¤šå…ƒçº¿æ€§å›å½’ä¸ºæ„Ÿå…´è¶£çš„å“åº”å˜é‡å¼€å‘é¢„æµ‹æ–¹ç¨‹ã€‚</li>
<li>å®ƒè¿˜å¯ä»¥é‡åŒ–å“ªäº›è§£é‡Šå˜é‡ä¸å“åº”å˜é‡æ˜¾ç€ç›¸å…³ã€‚</li>
<li>åˆ†ç±»å˜é‡å¯ä»¥å˜ä¸ºå“‘å˜é‡æ”¾å…¥æ¨¡å‹ä¸­</li>
<li>å¤šé‡çº¿æ€§å›å½’çš„å‡è®¾ä¸ç®€å•çº¿æ€§å›å½’çš„å‡è®¾ç›¸åŒï¼Œä½†å¯¹å¯èƒ½çš„è§£é‡Šå˜é‡çš„æ•°é‡æœ‰é¢å¤–çš„é™åˆ¶ï¼Œå»ºè®®æœ€å¤šä¸ºn / 3ï¼Œå…¶ä¸­nä¸ºè§‚å¯Ÿæ•°ã€‚</li>
</ol>
]]></content>
      <categories>
        <category>DATA7202</category>
      </categories>
      <tags>
        <tag>MLE</tag>
        <tag>stats</tag>
        <tag>biased estimate</tag>
      </tags>
  </entry>
</search>
